{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4f99077ebb2266ab",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "5686b8e03ec69ef3",
   "metadata": {
    "collapsed": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Getting Started with Modal\n",
    "author: Chris Levy\n",
    "date: '2024-09-03'\n",
    "date-modified: '2024-09-03'\n",
    "image: imgs/llm_loop.png\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: show\n",
    "    page-layout: full\n",
    "include-in-header:\n",
    "  - text: |\n",
    "      <style>\n",
    "      .cell-output-stdout code {\n",
    "        word-break: break-wor !important;\n",
    "        white-space: pre-wrap !important;\n",
    "      }\n",
    "      </style>\n",
    "bibliography: ../../bibliography.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315cf57d",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "\n",
    "In my professional life, I wear many hats: researcher/AI engineer/AI application developer/Software engineer/etc. Pick whatever label you like. \n",
    "The code I write eventually ends up on a production environment, supported by sophisticated DevOps infrastructure. This system \n",
    "leverages a suite of tools such as Kubernetes, Rancher, Karpenter, Helm charts, Argo CD, GitHub Actions, \n",
    "and of course AWS. I'm fortunate to work alongside an exceptional DevOps team that keeps this complex machinery running smoothly. \n",
    "While I'm not deeply involved in the nitty-gritty of DevOps and infrastructure, I'm certainly exposed to it.\n",
    "I have a solid high level understanding of the underlying infrastructure that powers my code, including job scheduling, worker management, \n",
    "API interactions, and resource allocation for CPU and memory. I firmly believe that understanding the operational \n",
    "environment, at least to some level, is crucial for effective development.\n",
    "\n",
    "\n",
    "On the other hand, I also crave the simplicity of building and tinkering without infrastructure concerns, especially in my free time. \n",
    "I shouldn't need a DevOps team for personal projects. Ideally, I'd work directly with Python code using just my IDE and terminal. \n",
    "I'd rather avoid writing another YAML file or learning yet another framework. The last thing I want is to worry about spinning up instances, \n",
    "managing IAM roles, installing CUDA drivers, or juggling multiple microservices and containers. What I seek is a streamlined development experience \n",
    "that lets me focus on creativity and problem-solving, not infrastructure management.\n",
    "\n",
    "\n",
    "\n",
    "This is where Modal enters the picture. I'm genuinely excited about [Modal](https://modal.com/) and consider it \n",
    "the most impressive platform I've encountered for running code without infrastructure concerns. Modal is a serverless \n",
    "platform designed for Data/ML/AI teams that seamlessly bridges the gap between local development and cloud execution. \n",
    "The primary interface is a Python SDK, where decorators are used to quickly move function execution into the cloud.\n",
    "You write your code as if it were running locally, and Modal effortlessly deploys and runs it in the cloud. This \n",
    "approach offers the best of both worlds: the simplicity of local development with the power and scalability of \n",
    "cloud computing. \n",
    "\n",
    "\n",
    "Modal didn't simply create a wrapper on top of Kubernetes or Docker.\n",
    "While I won't even pretend to understand the engineering behind it,\n",
    "it's clearly their secret sauce. From what I've read and heard,\n",
    "they've built their own systems from scratch in Rust, including a container runtime, custom file system, custom image builder, and custom job scheduler.\n",
    "This allows for launching containers in the cloud within seconds. \n",
    "\n",
    "\n",
    "I should note that I have only used Modal for personal projects and tinkering around with various ideas. \n",
    "However, I anticipate incorporating it more into my professional work, particularly for\n",
    "research projects and proofs of concept. Looking ahead, I can envision leveraging Modal directly in our production environment as well. \n",
    "It seems particularly well-suited for deploying complex AI models that require specific container configurations \n",
    "and GPU resources, especially in scenarios with unpredictable or spiky traffic patterns.\n",
    "\n",
    "\n",
    "\n",
    "If you want to learn more about the history of Modal or keep up with the latest news, I recommend the following resources:\n",
    "\n",
    "- [Modal Website](https://modal.com/)\n",
    "- [Modal X Account](https://x.com/modal_labs)\n",
    "- [Modal Slack Account](https://modal.com/slack) (They are so helpful and responsive on Slack)\n",
    "- [Charles Frye X Account](https://x.com/charles_irl) (AI Engineer at Modal)\n",
    "- [Erik Bernhardsson X Account](https://x.com/bernhardsson) (CEO at Modal)\n",
    "- [1 to 100: Modal Labs](https://www.youtube.com/watch?v=MGVeavVJiWw) (Interview with Erik Bernhardsson)\n",
    "- [Why you should join Modal](https://whyyoushouldjoin.substack.com/p/modal) (Article)\n",
    "- [What I have been working on: Modal](https://erikbern.com/2022/12/07/what-ive-been-working-on-modal.html) (Older article with relevant background)\n",
    "\n",
    "## Why am I writing this Post?\n",
    "\n",
    "\n",
    "I could simply direct you to the Modal Documentation, which is exceptionally comprehensive and well-crafted. \n",
    "In fact, it's so good that I doubt I could do it justice in a single post. However, I'm currently investing time \n",
    "in learning Modal, and what better way to solidify my understanding than by writing about it? Even if it means \n",
    "repeating some of the information in the documentation, it will still be a valuable exercise. Moreover, I'm eager \n",
    "to spread the word about this game-changing platform that I believe is still flying under the radar for many developers. \n",
    "By sharing my experiences and insights, I hope to contribute to the growing community of Modal enthusiasts.\n",
    "\n",
    "\n",
    "# Setting Up Modal\n",
    "\n",
    "- [getting started documentation](https://modal.com/docs/guide)\n",
    "\n",
    "```\n",
    "# create an account at modal.com\n",
    "pip install modal\n",
    "modal setup\n",
    "```\n",
    "\n",
    "ðŸš€âœ¨ That is like zero friction! âœ¨ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611eabff",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Hello Modal\n",
    "\n",
    "Okay letâ€™s write our first function and run it in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de305b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def import_python_as_markdown(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    return f\"```python\\n{content}\\n```\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9198f461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import modal\n",
       "\n",
       "app = modal.App(\"hello-modal\")\n",
       "\n",
       "\n",
       "@app.function()\n",
       "def f(i):\n",
       "    print(f\"hello modal! {i} + {i} is {i+i}\\n\")\n",
       "    return i\n",
       "\n",
       "\n",
       "@app.local_entrypoint()\n",
       "def main():\n",
       "    print(\"This is running locally\")\n",
       "    print(f.local(1))\n",
       "\n",
       "    print(\"This is running remotely on Modal\")\n",
       "    print(f.remote(2))\n",
       "\n",
       "    print(\"This is running in parallel and remotely on Modal\")\n",
       "    total = 0\n",
       "    for ret in f.map(range(2500)):\n",
       "        total += ret\n",
       "    print(total)\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "file_path = 'hello_modal.py'\n",
    "markdown_content = import_python_as_markdown(file_path)\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f75ab0",
   "metadata": {},
   "source": [
    "We decorated our function with the primary logic and then decorated the entry point.\n",
    "We can call the function locally, remotely, in parallel and remotely on Modal.\n",
    "Here is a video showing the output when running the code.\n",
    "We run it with this command: \n",
    "\n",
    "\n",
    "`modal run hello_modal.py`\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=-BgAiGW4o5c >}}\n",
    "\n",
    "Take a moment to let that sink in! We can run the code on a remote server and see the output and print statements locally. \n",
    "Imagine trying to do that with a traditional server where you have to log in and manually copy the logs. \n",
    "This is a simple function, but the ability to run it remotely on Modal and get the output locally is quite impressive. \n",
    "Modal handles spinning up containers and managing everything else seamlessly.\n",
    "\n",
    "## Shell into your container\n",
    "\n",
    "We will see in later examples how to customize the environment of the container.\n",
    "But even with this simple example we can shell into the default container and poke around.\n",
    "There are numerous ways to [develop and debug your application with Modal](https://modal.com/docs/guide/developing-debugging#developing-and-debugging).\n",
    "\n",
    "Here we will use the `modal shell` command to quickly create a container and shell into it.\n",
    "\n",
    "```\n",
    "modal shell hello_modal.py::f\n",
    "```\n",
    "\n",
    "This video shows how easy it is to shell into the container.\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=5yw29jQEn3E >}}\n",
    "\n",
    "By shelling into the container you get direct access to that isolated environment.\n",
    "You can inspect the file system, test the installation of additional dependencies, and generally poke around to ensure\n",
    "your application is configured correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa264cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9594b62b",
   "metadata": {},
   "source": [
    "\n",
    "# Image Generation with Flux Models from Black Forest Labs\n",
    "\n",
    "Let's dive into our first \"real\" and exciting example. If you haven't heard already, the new Flux image generation models from [Black Forest Labs](https://blackforestlabs.ai/) are truly impressive. One of the easiest ways to try them out is through [Replicate](https://replicate.com/stability-ai/sdxl).\n",
    "\n",
    "What's particularly appealing about the two smaller Flux models is that their weights are open-source and available for download. Running these models using the transformers and diffusers libraries is relatively straightforward. You can find an example in the model card [here](https://huggingface.co/black-forest-labs/FLUX.1-schnell#diffusers) - it only takes a handful of lines of code!\n",
    "\n",
    "However, there's a catch: you need access to a GPU environment with CUDA installed. This can be a  barrier for developers, including myself, who don't have access to a local GPU. This is where Modal can really shine, providing easy access to GPU resources in the cloud within an isolated environment. You can mess around within the isolated environment without worrying about messing up your local machine. \n",
    "\n",
    "Here is some code to create a simple endpoint hosted through Modal that allows you to generate images.\n",
    "You can read the Modal documentation for all the details but I will call out some of the important features here.\n",
    "\n",
    "- We defined a specific container with `Image.debian_slim(python_version=\"3.11\").run_commands(.....`\n",
    "    - I did not know what to put here initially. I just started with a blank slate and then shelled into the container and tried running snippets of code until I figured out what I needed. It's an iterative process where you build up your container with whatever dependencies you need.\n",
    "- We use the [Modal class syntax](https://modal.com/docs/guide/lifecycle-functions). \n",
    "    - `@enter` - Called when a new container is started. Useful for loading weights into memory for example.\n",
    "    - `@build` - Code that runs as a part of the container image build process. Useful for downloading weights. \n",
    "    - In the case of Hugging Face diffusers models, the weights only have to be downloaded once and future containers will only need to load the weights into memory. This means the initial build proccess takes longer but subsequent builds are much faster. Especially since Modal has done all the heavy lifting and enginnering to make containers load very fast.\n",
    "- `@modal.web_endpoint(method=\"POST\", docs=True)` is used to create a web server using FastAPI under the hood. See [here](https://modal.com/docs/guide/webhooks) for more information on creating web endpoints.\n",
    "- Modal has multiple ways to deal with secrets and environment variables. See [here](https://modal.com/docs/guide/secrets) for more information. Here I am making use of `Secret.from_dotenv()` to load the Hugging Face token from a .env file.\n",
    "- `gpu=\"A100\"` - [GPU acceleration!](https://modal.com/docs/guide/gpu#gpu-acceleration). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0998be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import modal\n",
       "from modal import Image, build, enter\n",
       "import os\n",
       "from dotenv import load_dotenv\n",
       "\n",
       "load_dotenv()\n",
       "app = modal.App(\"black-forest-labs-flux\")\n",
       "\n",
       "image = Image.debian_slim(python_version=\"3.11\").run_commands(\n",
       "    \"apt-get update && apt-get install -y git\",\n",
       "    \"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\",\n",
       "    \"pip install transformers\",\n",
       "    \"pip install accelerate\",\n",
       "    \"pip install sentencepiece\",\n",
       "    \"pip install git+https://github.com/huggingface/diffusers.git\",\n",
       "    \"pip install python-dotenv\",\n",
       "    f'huggingface-cli login --token {os.environ[\"HUGGING_FACE_ACCESS_TOKEN\"]}',\n",
       ")\n",
       "\n",
       "\n",
       "@app.cls(image=image, secrets=[modal.Secret.from_dotenv()], gpu=\"A100\", cpu=4, timeout=600, container_idle_timeout=300)\n",
       "class Model:\n",
       "    @build()\n",
       "    @enter()\n",
       "    def setup(self):\n",
       "        import torch\n",
       "        from diffusers import FluxPipeline\n",
       "        from transformers.utils import move_cache\n",
       "\n",
       "        # black-forest-labs/FLUX.1-schnell\n",
       "        # black-forest-labs/FLUX.1-dev\n",
       "        self.model = \"black-forest-labs/FLUX.1-schnell\"\n",
       "        self.pipe = FluxPipeline.from_pretrained(self.model, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
       "        move_cache()\n",
       "\n",
       "    @modal.web_endpoint(method=\"POST\", docs=True)\n",
       "    def f(self, data: dict):\n",
       "        import torch\n",
       "        import random\n",
       "        from io import BytesIO\n",
       "        import base64\n",
       "\n",
       "        prompts = data[\"prompts\"]\n",
       "        fnames = data[\"fnames\"]\n",
       "        num_inference_steps = data.get(\"num_inference_steps\", 4)\n",
       "        seed = data.get(\"seed\", random.randint(1, 2**63 - 1))\n",
       "        guidance_scale = data.get(\"guidance_scale\", 3.5)\n",
       "\n",
       "        results = []\n",
       "        for prompt, fname in zip(prompts, fnames):\n",
       "            image = self.pipe(\n",
       "                prompt,\n",
       "                output_type=\"pil\",\n",
       "                num_inference_steps=num_inference_steps,\n",
       "                generator=torch.Generator(\"cpu\").manual_seed(seed),\n",
       "                guidance_scale=guidance_scale,\n",
       "            ).images[0]\n",
       "\n",
       "            # Convert PIL image to bytes\n",
       "            buffered = BytesIO()\n",
       "            image.save(buffered, format=\"PNG\")\n",
       "            img_str = base64.b64encode(buffered.getvalue()).decode()\n",
       "\n",
       "            results.append(\n",
       "                {\n",
       "                    \"filename\": f\"{fname}_guidance_scale_{guidance_scale}_num_inference_steps_{num_inference_steps}_seed_{seed}_model_{self.model.replace('/', '_')}.png\",\n",
       "                    \"image\": img_str,\n",
       "                }\n",
       "            )\n",
       "\n",
       "        return results\n",
       "\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "markdown_content = import_python_as_markdown('flux.py')\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f93fcf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You can serve it for testing with this command:\n",
    "\n",
    "```\n",
    "modal serve flux.py\n",
    "```\n",
    "\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=692hHe6Irjg >}}\n",
    "\n",
    "I sped up the video but here are some takeaways:\n",
    "\n",
    "- The app starts immediately with 0 containers so the cost is zero. Containers are only spun up when the first request comes in.\n",
    "- I had already built the container image so this time around the model weights were already within the container image. On the first request to the endpoint the container spins up and the model weights are loaded into memory. The container boots up in about 25 seconds.\n",
    "- Once the weights are loaded into memory, subsequent requests are much faster.\n",
    "- `container_idle_timeout` is set to 300 seconds. This means the container will be terminated after 300 seconds of inactivity so it scales down to 0 containers. But the endpoint application is still running and can scale back up when the next request comes in.\n",
    "\n",
    "\n",
    "Here is some inference code so we can hit the endpoint and download the images locally. This code will work as long as the endpoint is running.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4ab9da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def main():\n",
       "    import requests\n",
       "    import base64\n",
       "    import os\n",
       "\n",
       "    os.makedirs(\"images\", exist_ok=True)\n",
       "    # Your API endpoint URL\n",
       "    API_URL = \"https://drchrislevy--black-forest-labs-flux-model-f-dev.modal.run\"  # Replace with your actual Modal app URL\n",
       "\n",
       "    # Sample data\n",
       "    data = {\n",
       "        \"prompts\": [\n",
       "            \"A serene mountain landscape at sunset\",\n",
       "            \"A futuristic cityscape with flying cars\",\n",
       "            \"An underwater scene with colorful coral reefs\",\n",
       "            \"A steampunk-inspired clockwork dragon\",\n",
       "            \"A bioluminescent forest at midnight\",\n",
       "            \"An ancient library filled with floating books\",\n",
       "            \"A surreal Salvador Dali-inspired melting cityscape\",\n",
       "            \"A cyberpunk street market in neon-lit rain\",\n",
       "            \"A whimsical tea party on a giant mushroom\",\n",
       "            \"An intergalactic spaceport with alien travelers\",\n",
       "        ],\n",
       "        \"fnames\": [\n",
       "            \"mountain_sunset\",\n",
       "            \"future_city\",\n",
       "            \"underwater_coral\",\n",
       "            \"steampunk_dragon\",\n",
       "            \"bioluminescent_forest\",\n",
       "            \"floating_library\",\n",
       "            \"melting_cityscape\",\n",
       "            \"cyberpunk_market\",\n",
       "            \"mushroom_teaparty\",\n",
       "            \"alien_spaceport\",\n",
       "        ],\n",
       "        \"num_inference_steps\": 4,\n",
       "        \"guidance_scale\": 7,\n",
       "    }\n",
       "\n",
       "    # Make the API request\n",
       "    response = requests.post(API_URL, json=data)\n",
       "\n",
       "    if response.status_code == 200:\n",
       "        results = response.json()\n",
       "\n",
       "        for result in results:\n",
       "            filename = result[\"filename\"]\n",
       "            img_data = base64.b64decode(result[\"image\"])\n",
       "\n",
       "            with open(os.path.join(\"images\", filename), \"wb\") as f:\n",
       "                f.write(img_data)\n",
       "            print(f\"Saved: {filename}\")\n",
       "    else:\n",
       "        print(f\"Error: {response.status_code}\")\n",
       "        print(response.text)\n",
       "\n",
       "    print(\"All images have been downloaded to the 'images/' folder.\")\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "markdown_content = import_python_as_markdown('flux_inference.py')\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c3919e",
   "metadata": {},
   "source": [
    "Here is a video of running the inference code, inspecting the Modal application dashboard, and viewing downloaded images on my local machine.\n",
    "The video is also sped up.\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=MI91tqjT9z4 >}}\n",
    "\n",
    "\n",
    "One very awesome thing about Modal is that it scales automatically with the number of requests.\n",
    "Containers are spun up and down dynamically based on the number of requests. You can tweak parameters to control the behavior of the scaling, and you can refer to the [documentation](https://modal.com/docs/guide/concurrent-inputs) for the details. Let's illustrate this with running the large Flux model, `\"black-forest-labs/FLUX.1-dev\"`.\n",
    "\n",
    "Im going to change the request payload to\n",
    "\n",
    "```python\n",
    "# Sample data\n",
    "data = {\n",
    "    \"prompts\": [\n",
    "        \"Futuristic spaceship wreckage overgrown with lush forest vegetation\",\n",
    "        \"Pristine tropical island with crystal-clear blue waters and white sandy beaches\",\n",
    "        \"Abandoned, overgrown streets of post-apocalyptic Boston from The Last of Us\",\n",
    "    ],\n",
    "    \"fnames\": [\"sci_fi_forest_ship\", \"tropical_island_paradise\", \"last_of_us_boston\"],\n",
    "    \"num_inference_steps\": 50,\n",
    "    \"guidance_scale\": 3.5,\n",
    "}\n",
    "```\n",
    "\n",
    "In the video I'm going to kick off 10 requests to the endpoint in 10 shells at the same time.\n",
    "You will see the containers spin up and then back down automatically. For this demo\n",
    "I also changed `container_idle_timeout` to 10 seconds so the containers are terminated quickly.\n",
    "The video is sped up at 5X the speed.\n",
    "\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=K9vDW8J440k >}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332343e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa5c8285",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "````\n",
    "allow_concurrent_inputs=1,\n",
    "concurrency_limit=2,\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
