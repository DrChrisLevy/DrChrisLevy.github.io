{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4f99077ebb2266ab",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "5686b8e03ec69ef3",
   "metadata": {
    "collapsed": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Getting Started with Modal\n",
    "author: Chris Levy\n",
    "date: '2024-09-03'\n",
    "date-modified: '2024-09-03'\n",
    "image: imgs/modal.png\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: show\n",
    "    page-layout: full\n",
    "include-in-header:\n",
    "  - text: |\n",
    "      <style>\n",
    "      .cell-output-stdout code {\n",
    "        word-break: break-wor !important;\n",
    "        white-space: pre-wrap !important;\n",
    "      }\n",
    "      </style>\n",
    "bibliography: ../../bibliography.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315cf57d",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "\n",
    "In my professional life, I wear many hats: researcher/AI engineer/AI application developer/Software engineer/etc. Pick whatever label you like. \n",
    "The code I write eventually ends up on a production environment, supported by sophisticated DevOps infrastructure. This system \n",
    "leverages a suite of tools such as Kubernetes, Rancher, Karpenter, Helm charts, Argo CD, GitHub Actions, \n",
    "and of course AWS. I'm fortunate to work alongside an exceptional DevOps team that keeps this complex machinery running smoothly. \n",
    "While I'm not deeply involved in the nitty-gritty of DevOps and infrastructure, I'm certainly exposed to it.\n",
    "I have a solid high level understanding of the underlying infrastructure that powers my code, including job scheduling, worker management, \n",
    "API interactions, and resource allocation for CPU and memory. I firmly believe that understanding the operational \n",
    "environment, at least to some level, is crucial for effective development.\n",
    "\n",
    "\n",
    "On the other hand, I also crave the simplicity of building and tinkering without infrastructure concerns, especially in my free time. \n",
    "I shouldn't need a DevOps team for personal projects. Ideally, I'd work directly with Python code using just my IDE and terminal. \n",
    "I'd rather avoid writing another YAML file or learning yet another framework. The last thing I want is to worry about spinning up instances, \n",
    "managing IAM roles, installing CUDA drivers, or juggling multiple microservices and containers. What I seek is a streamlined development experience \n",
    "that lets me focus on creativity and problem-solving, not infrastructure management.\n",
    "\n",
    "\n",
    "\n",
    "This is where Modal enters the picture. I'm genuinely excited about [Modal](https://modal.com/) and consider it \n",
    "the most impressive platform I've encountered for running code without infrastructure concerns. Modal is a serverless \n",
    "platform designed for Data/ML/AI teams that seamlessly bridges the gap between local development and cloud execution. \n",
    "The primary interface is a Python SDK, where decorators are used to quickly move function execution into the cloud.\n",
    "You write your code as if it were running locally, and Modal effortlessly deploys and runs it in the cloud. This \n",
    "approach offers the best of both worlds: the simplicity of local development with the power and scalability of \n",
    "cloud computing. \n",
    "\n",
    "\n",
    "Modal didn't simply create a wrapper on top of Kubernetes or Docker.\n",
    "While I won't even pretend to understand the engineering behind it,\n",
    "it's clearly their secret sauce. From what I've read and heard,\n",
    "they've built their own systems from scratch in Rust, including a container runtime, custom file system, custom image builder, and custom job scheduler.\n",
    "This allows for launching containers in the cloud within seconds. \n",
    "\n",
    "For many AI applications, GPUs are a necessity. This can be a  barrier for developers, including myself, who don't have access to a local GPU. This is where Modal can really shine, providing easy access to GPU resources in the cloud within an isolated environment. You can mess around within the isolated environment without worrying about messing up your local machine. \n",
    "\n",
    "Of course there are many great options out there for spinning up GPU instances in the cloud. Some of the other platforms I enjoy are [Jarvis Labs](https://jarvislabs.ai/), [Lambda Labs](https://cloud.lambdalabs.com/), and [RunPod](https://www.runpod.io/). I have tried all of these and I like them. I have even wrote previously about using some of these services [here](https://drchrislevy.github.io/posts/intro_fine_tune/intro_fine_tune.html) and [here](https://drchrislevy.github.io/posts/fine_tune_jarvis/fine_tune_jarvis.html). Modal is offering something different. But to be honest, it's the developer experience that Modal is hooking me on. The lower cold start times and giving me the feeling of developing locally is just so nice. \n",
    "\n",
    "I should note that I have only used Modal for personal projects and tinkering around with various ideas. \n",
    "However, I anticipate incorporating it more into my professional work, particularly for\n",
    "research projects and proofs of concept. Looking ahead, I can envision leveraging Modal directly in our production environment as well. \n",
    "It seems particularly well-suited for deploying complex AI models that require specific container configurations \n",
    "and GPU resources, especially in scenarios with unpredictable or spiky traffic patterns.\n",
    "\n",
    "\n",
    "\n",
    "If you want to learn more about the history of Modal or keep up with the latest news, I recommend the following resources:\n",
    "\n",
    "- [Modal Website](https://modal.com/)\n",
    "- [Modal X Account](https://x.com/modal_labs)\n",
    "- [Modal Slack Account](https://modal.com/slack) (They are so helpful and responsive on Slack)\n",
    "- [Charles Frye X Account](https://x.com/charles_irl) (AI Engineer at Modal)\n",
    "- [Erik Bernhardsson X Account](https://x.com/bernhardsson) (CEO at Modal)\n",
    "- [1 to 100: Modal Labs](https://www.youtube.com/watch?v=MGVeavVJiWw) (Interview with Erik Bernhardsson)\n",
    "- [Why you should join Modal](https://whyyoushouldjoin.substack.com/p/modal) (Article)\n",
    "- [What I have been working on: Modal](https://erikbern.com/2022/12/07/what-ive-been-working-on-modal.html) (Older article with relevant background)\n",
    "\n",
    "## Why am I writing this Post?\n",
    "\n",
    "\n",
    "I could simply direct you to the Modal Documentation, which is exceptionally comprehensive and well-crafted. \n",
    "In fact, it's so good that I doubt I could do it justice in a single post. However, I'm currently investing time \n",
    "in learning Modal, and what better way to solidify my understanding than by writing about it? Even if it means \n",
    "repeating some of the information in the documentation, it will still be a valuable exercise. Moreover, I'm eager \n",
    "to spread the word about this game-changing platform that I believe is still flying under the radar for many developers. \n",
    "By sharing my experiences and insights, I hope to contribute to the growing community of Modal enthusiasts.\n",
    "\n",
    "\n",
    "# Setting Up Modal\n",
    "\n",
    "- [getting started documentation](https://modal.com/docs/guide)\n",
    "\n",
    "```\n",
    "# create an account at modal.com\n",
    "pip install modal\n",
    "modal setup\n",
    "```\n",
    "\n",
    "ðŸš€âœ¨ That is like zero friction! âœ¨ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611eabff",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Hello Modal\n",
    "\n",
    "Okay letâ€™s write our first function and run it in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2de305b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def import_python_as_markdown(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    return f\"```python\\n{content}\\n```\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9198f461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import modal\n",
       "\n",
       "app = modal.App(\"hello-modal\")\n",
       "\n",
       "\n",
       "@app.function()\n",
       "def f(i):\n",
       "    print(f\"hello modal! {i} + {i} is {i+i}\\n\")\n",
       "    return i\n",
       "\n",
       "\n",
       "@app.local_entrypoint()\n",
       "def main():\n",
       "    print(\"This is running locally\")\n",
       "    print(f.local(1))\n",
       "\n",
       "    print(\"This is running remotely on Modal\")\n",
       "    print(f.remote(2))\n",
       "\n",
       "    print(\"This is running in parallel and remotely on Modal\")\n",
       "    total = 0\n",
       "    for ret in f.map(range(2500)):\n",
       "        total += ret\n",
       "    print(total)\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "file_path = 'hello_modal.py'\n",
    "markdown_content = import_python_as_markdown(file_path)\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f75ab0",
   "metadata": {},
   "source": [
    "We decorated our function with the primary logic and then decorated the entry point.\n",
    "We can call the function locally, remotely, in parallel and remotely on Modal.\n",
    "Here is a video showing the output when running the code.\n",
    "We run it with this command: \n",
    "\n",
    "\n",
    "`modal run hello_modal.py`\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=-BgAiGW4o5c >}}\n",
    "\n",
    "Take a moment to let that sink in! We can run the code on a remote server and see the output and print statements locally. \n",
    "Imagine trying to do that with a traditional server where you have to log in and manually copy the logs. \n",
    "This is a simple function, but the ability to run it remotely on Modal and get the output locally is quite impressive. \n",
    "Modal handles spinning up containers and managing everything else seamlessly.\n",
    "\n",
    "## Shell into your container\n",
    "\n",
    "We will see in later examples how to customize the environment of the container.\n",
    "But even with this simple example we can shell into the default container and poke around.\n",
    "There are numerous ways to [develop and debug your application with Modal](https://modal.com/docs/guide/developing-debugging#developing-and-debugging).\n",
    "\n",
    "Here we will use the `modal shell` command to quickly create a container and shell into it.\n",
    "\n",
    "```\n",
    "modal shell hello_modal.py::f\n",
    "```\n",
    "\n",
    "This video shows how easy it is to shell into the container.\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=5yw29jQEn3E >}}\n",
    "\n",
    "By shelling into the container you get direct access to that isolated environment.\n",
    "You can inspect the file system, test the installation of additional dependencies, and generally poke around to ensure\n",
    "your application is configured correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa264cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9594b62b",
   "metadata": {},
   "source": [
    "\n",
    "# Image Generation with Flux Models from Black Forest Labs\n",
    "\n",
    "Let's dive into our first \"real\" and exciting example. If you haven't heard already, the new Flux image generation models from [Black Forest Labs](https://blackforestlabs.ai/) are truly impressive. One of the easiest ways to try them out is through [Replicate](https://replicate.com/stability-ai/sdxl).\n",
    "\n",
    "What's particularly appealing about the two smaller Flux models is that their weights are open and available for download. Running these models using the transformers and diffusers libraries is relatively straightforward. You can find an example in the model card [here](https://huggingface.co/black-forest-labs/FLUX.1-schnell#diffusers) - it only takes a handful of lines of code!\n",
    "\n",
    "Here is some code to create a simple endpoint hosted through Modal that allows you to generate images.\n",
    "You can read the Modal documentation for all the details but I will call out some of the important features here.\n",
    "\n",
    "- We defined a specific container with `Image.debian_slim(python_version=\"3.11\").run_commands(.....`\n",
    "    - I did not know what to put here initially. I just started with a blank slate and then shelled into the container and tried running snippets of code until I figured out what I needed. It's an iterative process where you build up your container with whatever dependencies you need.\n",
    "- We use the [Modal class syntax](https://modal.com/docs/guide/lifecycle-functions). \n",
    "    - `@enter` - Called when a new container is started. Useful for loading weights into memory for example.\n",
    "    - `@build` - Code that runs as a part of the container image build process. Useful for downloading weights. \n",
    "    - In the case of Hugging Face diffusers models, the weights only have to be downloaded once and future containers will only need to load the weights into memory. This means the initial build process takes longer but subsequent builds are much faster. Especially since Modal has done all the heavy lifting and engineering to make containers load very fast.\n",
    "- `@modal.web_endpoint(method=\"POST\", docs=True)` is used to create a web server using FastAPI under the hood. See [here](https://modal.com/docs/guide/webhooks) for more information on creating web endpoints.\n",
    "- Modal has multiple ways to deal with secrets and environment variables. See [here](https://modal.com/docs/guide/secrets) for more information. Here I am making use of `Secret.from_dotenv()` to load the Hugging Face token from a .env file.\n",
    "- `gpu=\"A100\"` - [GPU acceleration!](https://modal.com/docs/guide/gpu#gpu-acceleration). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0998be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import modal\n",
       "from modal import Image, build, enter\n",
       "import os\n",
       "from dotenv import load_dotenv\n",
       "\n",
       "load_dotenv()\n",
       "app = modal.App(\"black-forest-labs-flux\")\n",
       "\n",
       "image = Image.debian_slim(python_version=\"3.11\").run_commands(\n",
       "    \"apt-get update && apt-get install -y git\",\n",
       "    \"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\",\n",
       "    \"pip install transformers\",\n",
       "    \"pip install accelerate\",\n",
       "    \"pip install sentencepiece\",\n",
       "    \"pip install git+https://github.com/huggingface/diffusers.git\",\n",
       "    \"pip install python-dotenv\",\n",
       "    f'huggingface-cli login --token {os.environ[\"HUGGING_FACE_ACCESS_TOKEN\"]}',\n",
       ")\n",
       "\n",
       "\n",
       "@app.cls(image=image, secrets=[modal.Secret.from_dotenv()], gpu=\"A100\", cpu=4, timeout=600, container_idle_timeout=300)\n",
       "class Model:\n",
       "    @build()\n",
       "    @enter()\n",
       "    def setup(self):\n",
       "        import torch\n",
       "        from diffusers import FluxPipeline\n",
       "        from transformers.utils import move_cache\n",
       "\n",
       "        # black-forest-labs/FLUX.1-schnell\n",
       "        # black-forest-labs/FLUX.1-dev\n",
       "        self.model = \"black-forest-labs/FLUX.1-schnell\"\n",
       "        self.pipe = FluxPipeline.from_pretrained(self.model, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
       "        move_cache()\n",
       "\n",
       "    @modal.web_endpoint(method=\"POST\", docs=True)\n",
       "    def f(self, data: dict):\n",
       "        import torch\n",
       "        import random\n",
       "        from io import BytesIO\n",
       "        import base64\n",
       "\n",
       "        prompts = data[\"prompts\"]\n",
       "        fnames = data[\"fnames\"]\n",
       "        num_inference_steps = data.get(\"num_inference_steps\", 4)\n",
       "        seed = data.get(\"seed\", random.randint(1, 2**63 - 1))\n",
       "        guidance_scale = data.get(\"guidance_scale\", 3.5)\n",
       "\n",
       "        results = []\n",
       "        for prompt, fname in zip(prompts, fnames):\n",
       "            image = self.pipe(\n",
       "                prompt,\n",
       "                output_type=\"pil\",\n",
       "                num_inference_steps=num_inference_steps,\n",
       "                generator=torch.Generator(\"cpu\").manual_seed(seed),\n",
       "                guidance_scale=guidance_scale,\n",
       "            ).images[0]\n",
       "\n",
       "            # Convert PIL image to bytes\n",
       "            buffered = BytesIO()\n",
       "            image.save(buffered, format=\"PNG\")\n",
       "            img_str = base64.b64encode(buffered.getvalue()).decode()\n",
       "\n",
       "            results.append(\n",
       "                {\n",
       "                    \"filename\": f\"{fname}_guidance_scale_{guidance_scale}_num_inference_steps_{num_inference_steps}_seed_{seed}_model_{self.model.replace('/', '_')}.png\",\n",
       "                    \"image\": img_str,\n",
       "                }\n",
       "            )\n",
       "\n",
       "        return results\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "markdown_content = import_python_as_markdown('flux.py')\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f93fcf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You can serve it for testing with this command:\n",
    "\n",
    "```\n",
    "modal serve flux.py\n",
    "```\n",
    "\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=692hHe6Irjg >}}\n",
    "\n",
    "I sped up the video but here are some takeaways:\n",
    "\n",
    "- The app starts immediately with 0 containers so the cost is zero. Containers are only spun up when the first request comes in.\n",
    "- I had already built the container image so this time around the model weights were already within the container image. On the first request to the endpoint the container spins up and the model weights are loaded into memory. The container boots up in about 25 seconds.\n",
    "- Once the weights are loaded into memory, subsequent requests are much faster.\n",
    "- `container_idle_timeout` is set to 300 seconds. This means the container will be terminated after 300 seconds of inactivity so it scales down to 0 containers. But the endpoint application is still running and can scale back up when the next request comes in.\n",
    "\n",
    "\n",
    "Here is some inference code so we can hit the endpoint and download the images locally. This code will work as long as the endpoint is running.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4ab9da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def main():\n",
       "    import requests\n",
       "    import base64\n",
       "    import os\n",
       "\n",
       "    os.makedirs(\"images\", exist_ok=True)\n",
       "    # Your API endpoint URL\n",
       "    API_URL = \"https://drchrislevy--black-forest-labs-flux-model-f-dev.modal.run\"  # Replace with your actual Modal app URL\n",
       "\n",
       "    # Sample data\n",
       "    data = {\n",
       "        \"prompts\": [\n",
       "            \"A pristine tropical island paradise with crystal-clear turquoise waters lapping at white sandy shores. Palm trees sway gently in the breeze along the coastline. In the foreground, the words 'Welcome to Modal' are elegantly written in the smooth wet sand, with small seashells decorating the letters. The sun is setting in the background, painting the sky with vibrant hues of orange, pink, and purple. A few scattered clouds reflect the warm sunset colors.\",\n",
       "        ],\n",
       "        \"fnames\": [\n",
       "            \"modal_island\"\n",
       "        ],\n",
       "        \"num_inference_steps\": 4,\n",
       "        \"guidance_scale\": 5,\n",
       "    }\n",
       "\n",
       "    # Make the API request\n",
       "    response = requests.post(API_URL, json=data)\n",
       "\n",
       "    if response.status_code == 200:\n",
       "        results = response.json()\n",
       "\n",
       "        for result in results:\n",
       "            filename = result[\"filename\"]\n",
       "            img_data = base64.b64decode(result[\"image\"])\n",
       "\n",
       "            with open(os.path.join(\"images\", filename), \"wb\") as f:\n",
       "                f.write(img_data)\n",
       "            print(f\"Saved: {filename}\")\n",
       "    else:\n",
       "        print(f\"Error: {response.status_code}\")\n",
       "        print(response.text)\n",
       "\n",
       "    print(\"All images have been downloaded to the 'images/' folder.\")\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "markdown_content = import_python_as_markdown('flux_inference.py')\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c3919e",
   "metadata": {},
   "source": [
    "Here is a video of running the inference code, inspecting the Modal application dashboard, and viewing downloaded images on my local machine.\n",
    "The video is also sped up.\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=MI91tqjT9z4 >}}\n",
    "\n",
    "\n",
    "One very awesome thing about Modal is that it scales automatically with the number of requests.\n",
    "Containers are spun up and down dynamically based on the number of requests. You can tweak parameters to control the behavior of the scaling, and you can refer to the [documentation](https://modal.com/docs/guide/concurrent-inputs) for the details. Let's illustrate this with running the larger Flux model, `\"black-forest-labs/FLUX.1-dev\"`.\n",
    "\n",
    "Im going to change the request payload to\n",
    "\n",
    "```python\n",
    "# Sample data\n",
    "data = {\n",
    "    \"prompts\": [\n",
    "        \"Futuristic spaceship wreckage overgrown with lush forest vegetation\",\n",
    "        \"Pristine tropical island with crystal-clear blue waters and white sandy beaches\",\n",
    "        \"Abandoned, overgrown streets of post-apocalyptic Boston from The Last of Us\",\n",
    "    ],\n",
    "    \"fnames\": [\"sci_fi_forest_ship\", \"tropical_island_paradise\", \"last_of_us_boston\"],\n",
    "    \"num_inference_steps\": 50,\n",
    "    \"guidance_scale\": 3.5,\n",
    "}\n",
    "```\n",
    "\n",
    "In the video I'm going to kick off 10 requests to the endpoint in 10 shells at the same time.\n",
    "You will see the containers spin up and then back down automatically. For this demo\n",
    "I also changed `container_idle_timeout` to 10 seconds so the containers are terminated quickly.\n",
    "The video is sped up at 5X the speed.\n",
    "\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=K9vDW8J440k >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332343e4",
   "metadata": {},
   "source": [
    "# Qwen2-VL: Vision Language Model\n",
    "\n",
    "Qwen2-VL is a recent vision language model from the Qwen team. Here is a [post](https://qwenlm.github.io/blog/qwen2-vl/) about it, and also the the Hugging Face [model card](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct) for the 7B parameter model. I wanted to experiment with this model and make use of flash attention during inference. \n",
    "\n",
    "First, lets get into setting up the container with the CUDA environment. There are different [ways](https://modal.com/docs/guide/cuda) of using\n",
    "CUDA on Modal. Since Modal supports the underlying CUDA stack, it's often very easy to get started by simply using `pip` to install your libraries. Here is an example straight out of their [documentation](https://modal.com/docs/guide/cuda#install-gpu-accelerated-torch-and-transformers-with-pip_install):\n",
    "\n",
    "```\n",
    "image = modal.Image.debian_slim().pip_install(\"torch\")\n",
    "\n",
    "\n",
    "@app.function(gpu=\"any\", image=image)\n",
    "def run_torch():\n",
    "    import torch\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    print(f\"It is {has_cuda} that torch can access CUDA\")\n",
    "    return has_cuda\n",
    "```\n",
    "\n",
    "Easy!\n",
    "\n",
    "Other use cases may require a more involved installation setup. One such example is using flash attention. Luckily, Modal also [documented](https://modal.com/docs/guide/cuda#for-more-complex-setups-use-an-officially-supported-cuda-image) this common use case and how to go about setting up the environment. So that is what I started with here. I shelled into the container and ran bits of code while debugging the environment until I got it working. You can see the full code for the container image below. One little trick is you can use `.run_commands(` and `.pip_install` multiple times within the code that builds the container image. This is convenient to cache the parts of the container installation that are working\n",
    "while you are still debugging. That way if you are experimenting say with the final line, `.run_commands(\"pip install flash-attn --no-build-isolation\")`, you don't have to rerun the entire container build every time you make a small change.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b3cb56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import modal\n",
       "from modal import build, enter\n",
       "import os\n",
       "from dotenv import load_dotenv\n",
       "\n",
       "load_dotenv()\n",
       "app = modal.App(\"qwen2_vl_78_Instruct\")\n",
       "\n",
       "cuda_version = \"12.4.0\"  # should be no greater than host CUDA version\n",
       "flavor = \"devel\"  #  includes full CUDA toolkit\n",
       "operating_sys = \"ubuntu22.04\"\n",
       "tag = f\"{cuda_version}-{flavor}-{operating_sys}\"\n",
       "image = (\n",
       "    modal.Image.from_registry(f\"nvidia/cuda:{tag}\", add_python=\"3.11\")\n",
       "    .apt_install(\"git\")\n",
       "    .pip_install(\n",
       "        \"ninja\",  # required to build flash-attn\n",
       "        \"packaging\",  # required to build flash-attn\n",
       "        \"wheel\",  # required to build flash-attn\n",
       "    )\n",
       "    .run_commands(\n",
       "        \"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\",\n",
       "        \"pip install git+https://github.com/huggingface/transformers\",\n",
       "        \"pip install accelerate\",\n",
       "        \"pip install qwen-vl-utils\",\n",
       "        \"pip install python-dotenv\",\n",
       "        f'huggingface-cli login --token {os.environ[\"HUGGING_FACE_ACCESS_TOKEN\"]}',\n",
       "    )\n",
       "    .run_commands(\"pip install flash-attn --no-build-isolation\")\n",
       ")\n",
       "\n",
       "\n",
       "@app.cls(image=image, secrets=[modal.Secret.from_dotenv()], gpu=\"a100\", cpu=4, timeout=600, container_idle_timeout=300)\n",
       "class Model:\n",
       "    @build()\n",
       "    @enter()\n",
       "    def setup(self):\n",
       "        from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, TextStreamer\n",
       "        import torch\n",
       "\n",
       "        # We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
       "\n",
       "        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
       "            \"Qwen/Qwen2-VL-7B-Instruct\",\n",
       "            torch_dtype=torch.bfloat16,\n",
       "            attn_implementation=\"flash_attention_2\",\n",
       "            vision_config={\"torch_dtype\": torch.bfloat16},\n",
       "            device_map=\"auto\",\n",
       "        )\n",
       "        # default processor\n",
       "        self.processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n",
       "\n",
       "        # The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
       "        # min_pixels = 256*28*28\n",
       "        # max_pixels = 1280*28*28\n",
       "        # processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
       "\n",
       "        self.streamer = TextStreamer(self.processor, skip_prompt=True, skip_special_tokens=True)\n",
       "\n",
       "    @modal.method()\n",
       "    def f(self, messages_list, max_new_tokens=256, show_stream=False):\n",
       "        from qwen_vl_utils import process_vision_info\n",
       "\n",
       "        def messages_inference(messages):\n",
       "            # Preparation for inference\n",
       "            text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
       "            image_inputs, video_inputs = process_vision_info(messages)\n",
       "            inputs = self.processor(\n",
       "                text=[text],\n",
       "                images=image_inputs,\n",
       "                videos=video_inputs,\n",
       "                padding=True,\n",
       "                return_tensors=\"pt\",\n",
       "            )\n",
       "            inputs = inputs.to(\"cuda\")\n",
       "\n",
       "            # Inference: Generation of the output\n",
       "            if show_stream:\n",
       "                print(\"\\n\\n-----------------------------------------------------------\\n\\n\")\n",
       "                generated_ids = self.model.generate(**inputs, max_new_tokens=max_new_tokens, streamer=self.streamer)\n",
       "            else:\n",
       "                generated_ids = self.model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
       "            generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
       "            output_text = self.processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
       "            return output_text\n",
       "\n",
       "        return [messages_inference(messages) for messages in messages_list]\n",
       "\n",
       "\n",
       "@app.local_entrypoint()\n",
       "def main():\n",
       "    s3_bucket = os.environ[\"S3_BUCKET\"]  # where my images are hosted\n",
       "    s3_prefix = os.environ[\"S3_PREFIX\"]  # where my images are hosted\n",
       "    model = Model()\n",
       "\n",
       "    messages_list = [\n",
       "        [\n",
       "            {\n",
       "                \"role\": \"user\",\n",
       "                \"content\": [\n",
       "                    {\n",
       "                        \"type\": \"image\",\n",
       "                        \"image\": f\"https://{s3_bucket}.s3.amazonaws.com{s3_prefix}tropical_island_paradise_guidance_scale_3.5_num_inference_steps_50_seed_5013302010029533033_model_black-forest-labs_FLUX.1-dev.png\",\n",
       "                    },\n",
       "                    {\"type\": \"text\", \"text\": \"Give a short description of this image.\"},\n",
       "                ],\n",
       "            }\n",
       "        ],\n",
       "        [\n",
       "            {\n",
       "                \"role\": \"user\",\n",
       "                \"content\": [\n",
       "                    {\n",
       "                        \"type\": \"image\",\n",
       "                        \"image\": f\"https://{s3_bucket}.s3.amazonaws.com{s3_prefix}sci_fi_forest_ship_guidance_scale_3.5_num_inference_steps_50_seed_6510529542810937186_model_black-forest-labs_FLUX.1-dev.png\",\n",
       "                    },\n",
       "                    {\"type\": \"text\", \"text\": \"Give a long detailed description of this image.\"},\n",
       "                ],\n",
       "            }\n",
       "        ],\n",
       "        [\n",
       "            {\n",
       "                \"role\": \"user\",\n",
       "                \"content\": [\n",
       "                    {\n",
       "                        \"type\": \"image\",\n",
       "                        \"image\": f\"https://{s3_bucket}.s3.amazonaws.com{s3_prefix}tropical_island_paradise_guidance_scale_3.5_num_inference_steps_50_seed_5013302010029533033_model_black-forest-labs_FLUX.1-dev.png\",\n",
       "                    },\n",
       "                    {\n",
       "                        \"type\": \"image\",\n",
       "                        \"image\": f\"https://{s3_bucket}.s3.amazonaws.com{s3_prefix}sci_fi_forest_ship_guidance_scale_3.5_num_inference_steps_50_seed_6510529542810937186_model_black-forest-labs_FLUX.1-dev.png\",\n",
       "                    },\n",
       "                    {\"type\": \"text\", \"text\": \"For each image explain whether you think it is real or AI generated. Explain your reasoning.\"},\n",
       "                ],\n",
       "            }\n",
       "        ],\n",
       "    ]\n",
       "\n",
       "    print(\"Testing Single Inference with show_stream=True\")\n",
       "    print(model.f.remote(messages_list[1:2], max_new_tokens=1000, show_stream=True))\n",
       "\n",
       "    print(\"Testing Multiple Container with show_stream=False\")\n",
       "    for res in model.f.starmap([(messages_list[:1], 1000, False), (messages_list[1:2], 1000, False), (messages_list[2:3], 1000, False)]):\n",
       "        print(res)\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "markdown_content = import_python_as_markdown('qwen2_vl_78_Instruct.py')\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b077c9",
   "metadata": {},
   "source": [
    "The function `f` above takes a list of `messages` and loops over each one sequentially and handles the inference. \n",
    "There is also the option to pass in a `stream=True` parameter to get a streaming response as the tokens are generated.\n",
    "\n",
    "For the test images I am using some images that I generated with Flux earlier. Here are two such images\n",
    "and the descriptions generated from Qwen2-VL, using the code above. For the images on the left I asked for\n",
    "a longer detailed description and for the images on the right I asked for a short description.\n",
    "\n",
    "::: {layout-ncol=2}\n",
    "![This image depicts a futuristic, floating structure nestled within a lush, verdant landscape. The structure appears to be an abandoned or repurposed aircraft, possibly a large jet or a military transport plane, given its size and shape. The aircraft has been partially integrated into the natural environment, with vines and other greenery growing over its surface, suggesting a long period of disuse or abandonment. The aircraft is suspended in mid-air, supported by unseen technology or infrastructure that is not visible in the image. The surrounding area is dense with greenery, including trees, bushes, and other vegetation, creating a sense of harmony between the man-made structure and the natural world. The landscape below the aircraft features a river or stream, flanked by rocky cliffs and dense foliage, adding to the serene and almost otherworldly atmosphere of the scene. The overall color palette is dominated by greens and browns, with the aircraft's rusted metal adding a touch of industrial decay. The lighting is soft and diffused, likely from an overcast sky, which enhances the tranquil and somewhat mystical ambiance of the image. The scene evokes a sense of isolation and abandonment, yet also a connection to nature, as if the aircraft has been reclaimed by the environment.](imgs/sci_fi_forest_ship_guidance_scale_3.5_num_inference_steps_50_seed_6510529542810937186_model_black-forest-labs_FLUX.1-dev.png)\n",
    "\n",
    "![The image depicts a small, tropical island with a white sandy beach surrounded by crystal-clear turquoise waters. The island is covered with lush greenery\n",
    "and palm trees, and there is a small structure on the beach. The sky is mostly clear with a few scattered clouds.](imgs/tropical_island_paradise_guidance_scale_3.5_num_inference_steps_50_seed_5013302010029533033_model_black-forest-labs_FLUX.1-dev.png)\n",
    ":::\n",
    "\n",
    "And here is the output from the model on where I passed both images into the conversation and asked whether or not they were AI generated.\n",
    "\n",
    "\n",
    "*The first image appears to be a real photograph of a tropical island with clear blue waters and a small sandy beach. The\n",
    "natural lighting, shadows, and details in the water and sky suggest that this is a real image.\\n\\nThe second image, on the other hand, depicts a futuristic,\n",
    "floating structure covered in vegetation, suspended over a lush, green landscape with a river. The level of detail, the blending of natural and artificial\n",
    "elements, and the overall surreal nature of the scene suggest that this is likely an AI-generated image. The combination of a realistic natural environment\n",
    "with an advanced, almost sci-fi structure is not something typically captured in real photographs.*\n",
    "\n",
    "Here is a video of running the above inference on the container at 5X speed. I first stream one image and then use [starmap](https://modal.com/docs/guide/scale#starmap) to spin up 3 containers in parallel to handle 3 different requests.\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=KEcGqcFLidI >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f520b",
   "metadata": {},
   "source": [
    "# Multimodal RAG with ColPali\n",
    "\n",
    "CoPali is an innovative document retrieval model that uses vision language models to efficiently index and retrieve documents based on their visual features.\n",
    "It allows querying images, paragraphs and tables from any document with no pre-processing whatsoever. Instead of chunking up the document pages into embeddings,\n",
    "it instead uses the visual of the entire page/image. I am quite new to CoPali and need to do a deep dive into it in the near future. For now I am simply going to fumble my\n",
    "way through getting some inference working. I highly recommend the following resources for learning more:\n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/2407.01449v1)\n",
    "- [Original ColPali Repo](https://github.com/illuin-tech/colpali)\n",
    "- Recent [Talk](https://docs.google.com/presentation/d/1Zczs5Sk3FsCO06ZLDznqkOOhbTe96PwJa4_7FwyMBrA/edit#slide=id.p) by [Benjamin ClaviÃ©](https://x.com/bclavie): *RAG is more than dense embeddings*\n",
    "    - Many more links/resources at the end of these slides.\n",
    "- [Hugging Face Model Cards](https://huggingface.co/vidore/colpali-v1.2)\n",
    "    - There seem to be updates so always look for the latest model.\n",
    "- Benjamin ClaviÃ© wrote a [demo notebook](https://github.com/AnswerDotAI/byaldi/blob/main/examples/chat_with_your_pdf.ipynb) here within the AnswerDotAI repo which uses CoPali and Claude 3.5 Sonnet.\n",
    "- [Merve Noyan](https://x.com/mervenoyann) wrote a notebook [here](https://github.com/merveenoyan/smol-vision/blob/main/ColPali_%2B_Qwen2_VL.ipynb) using CoPali and Qwen2-VL.\n",
    "- Notebook by [Jo  Bergum](https://x.com/jobergum) on scaling ColPALI with Vespa: [Scaling ColPALI (VLM) Retrieval](https://github.com/vespa-engine/pyvespa/blob/master/docs/sphinx/source/examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f470e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be1633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa5c8285",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "````\n",
    "allow_concurrent_inputs=1,\n",
    "concurrency_limit=2,\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
