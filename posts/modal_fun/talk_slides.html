<!DOCTYPE html>
<html lang="en"><head>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/tabby.min.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.3.450">

  <meta name="author" content="Chris Levy">
  <meta name="dcterms.date" content="2024-09-24">
  <title>Chris Levy - Modal: A Serverless platform for data/AI/ML teams</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="../../site_libs/revealjs/dist/theme/quarto.css">
  <link href="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title"><a href="https://modal.com/">Modal</a>: A Serverless platform for data/AI/ML teams</h1>
  <p class="subtitle">Cloud execution with the simplicity of local development</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Chris Levy 
</div>
</div>
</div>

  <p class="date">2024-09-24</p>
</section>
<section>
<section id="intro" class="title-slide slide level1 center">
<h1>Intro</h1>

</section>
<section id="getting-started-with-modal" class="slide level2">
<h2>Getting Started with Modal</h2>
<ul>
<li><a href="https://modal.com/docs/guide">guide</a></li>
</ul>
<pre><code># create an account at modal.com
pip install modal
modal setup</code></pre>
</section>
<section id="hello-modal" class="slide level2 smaller">
<h2>Hello Modal</h2>
<div class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">import</span> modal</span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="im">import</span> time</span>
<span id="cb2-3"><a href="#cb2-3"></a></span>
<span id="cb2-4"><a href="#cb2-4"></a>app <span class="op">=</span> modal.App(<span class="st">"hello-modal"</span>)</span>
<span id="cb2-5"><a href="#cb2-5"></a></span>
<span id="cb2-6"><a href="#cb2-6"></a></span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="at">@app.function</span>()</span>
<span id="cb2-8"><a href="#cb2-8"></a><span class="kw">def</span> f(i):</span>
<span id="cb2-9"><a href="#cb2-9"></a>    time.sleep(<span class="dv">1</span>)</span>
<span id="cb2-10"><a href="#cb2-10"></a>    <span class="bu">print</span>(<span class="ss">f"hello modal! </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> + </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> is </span><span class="sc">{</span>i<span class="op">+</span>i<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb2-11"><a href="#cb2-11"></a>    <span class="cf">return</span> i</span>
<span id="cb2-12"><a href="#cb2-12"></a></span>
<span id="cb2-13"><a href="#cb2-13"></a></span>
<span id="cb2-14"><a href="#cb2-14"></a><span class="at">@app.local_entrypoint</span>()</span>
<span id="cb2-15"><a href="#cb2-15"></a><span class="kw">def</span> main():</span>
<span id="cb2-16"><a href="#cb2-16"></a>    <span class="bu">print</span>(<span class="st">"This is running locally"</span>)</span>
<span id="cb2-17"><a href="#cb2-17"></a>    <span class="bu">print</span>(f.local(<span class="dv">1</span>))</span>
<span id="cb2-18"><a href="#cb2-18"></a></span>
<span id="cb2-19"><a href="#cb2-19"></a>    <span class="bu">print</span>(<span class="st">"This is running remotely on Modal"</span>)</span>
<span id="cb2-20"><a href="#cb2-20"></a>    <span class="bu">print</span>(f.remote(<span class="dv">2</span>))</span>
<span id="cb2-21"><a href="#cb2-21"></a></span>
<span id="cb2-22"><a href="#cb2-22"></a>    <span class="bu">print</span>(<span class="st">"This is running in parallel and remotely on Modal"</span>)</span>
<span id="cb2-23"><a href="#cb2-23"></a>    total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-24"><a href="#cb2-24"></a>    <span class="cf">for</span> ret <span class="kw">in</span> f.<span class="bu">map</span>(<span class="bu">range</span>(<span class="dv">500</span>)):</span>
<span id="cb2-25"><a href="#cb2-25"></a>        total <span class="op">+=</span> ret</span>
<span id="cb2-26"><a href="#cb2-26"></a>    <span class="bu">print</span>(total)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<pre><code>modal run hello_modal.py</code></pre>
<ul>
<li>show use of <code>allow_concurrent_inputs=100, concurrency_limit=1</code></li>
</ul>
</section>
<section id="debugging-is-easy" class="slide level2">
<h2>Debugging Is Easy</h2>
<ul>
<li>Instantly create a <strong>new</strong> container and shell into it</li>
</ul>
<pre><code>modal shell hello_modal.py::f</code></pre>
<ul>
<li>list containers and open interactive shell</li>
</ul>
<pre><code>modal container list
modal container exec &lt;container_id&gt; /bin/bash</code></pre>
<ul>
<li>drop into an <code>ipython</code> REPL at any point in the code</li>
</ul>
<pre><code>modal.interact()
import IPython
IPython.embed()</code></pre>
<pre><code>modal run -i hello_modal.py</code></pre>
</section>
<section id="more-about-modal" class="slide level2 smaller">
<h2>More About Modal</h2>
<ul>
<li>Not a wrapper on top of Kubernetes or Docker.</li>
<li>They’ve built their own systems from scratch in Rust, including a container runtime, custom file system, custom image builder, and custom job scheduler.</li>
<li>Write Python code and execute it in the cloud in seconds</li>
<li>Deploy autoscaling inference endpoints on CPUs and/or GPUs (A100s, A10Gs, T4s, L4s, H100s)</li>
<li>Run large-scale batch jobs on thousands of containers</li>
<li>Turn your function into a cron job, or serve it as an web endpoint, with one line of code</li>
<li>Define images, hardware and persistent storage intuitively in Python</li>
</ul>
</section>
<section id="more-about-modal-1" class="slide level2 smaller">
<h2>More About Modal</h2>
<ul>
<li><p><a href="https://modal.com/blog/serverless-http">blog</a></p></li>
<li><p>Serverless function platforms have constraints. A lot of them, too!</p>
<ul>
<li>Functions on AWS Lambda are limited to 15-minute runs and 50 MB images. As of 2024, they can only use 3 CPUs (6 threads) and 10 GB of memory. Response bandwidth is 16 Mbps.</li>
<li>Google Cloud Run is a bit better, with 4 CPUs and 32 GB of memory, plus 75 Mbps bandwidth.</li>
<li>Cloudflare Workers are the most restricted. Their images can only be 10 MB in size and have 6 HTTP connections. Execution is limited to 30 seconds of CPU time, 128 MB of memory.</li>
</ul></li>
<li><p>But modern compute workloads can be much more demanding: AI inference, training neural networks, rendering graphics, simulating physics, running data pipelines, and so on.</p></li>
<li><p>Modal containers can each use up to 64 CPUs, 336 GB of memory, and 8 Nvidia H100 GPUs. And they may need to download up to hundreds of gigabytes of model weights and image data on container startup.</p></li>
<li><p>Spin up and shut down quickly, since having any idle time is expensive. Scale to zero and bill by the second.</p></li>
<li><p>Modal containers are potentially long-running and compute-heavy, with big inputs and outputs. This is the opposite of what “serverless” is usually good at.</p></li>
</ul>
</section></section>
<section>
<section id="live-demo-1-image-generation" class="title-slide slide level1 center">
<h1>Live Demo 1: Image Generation</h1>
<pre><code>modal deploy flux.py
python image_gen_app.py</code></pre>
</section>
<section id="image-generation-modal-app-code" class="slide level2 smaller">
<h2>Image Generation Modal App Code</h2>
<div class="cell" data-execution_count="4">
<div class="cell-output cell-output-display">
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="im">import</span> modal</span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="im">from</span> modal <span class="im">import</span> Image, build, enter</span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="im">import</span> os</span>
<span id="cb9-4"><a href="#cb9-4"></a><span class="im">from</span> dotenv <span class="im">import</span> load_dotenv</span>
<span id="cb9-5"><a href="#cb9-5"></a></span>
<span id="cb9-6"><a href="#cb9-6"></a>load_dotenv()</span>
<span id="cb9-7"><a href="#cb9-7"></a>app <span class="op">=</span> modal.App(<span class="st">"black-forest-labs-flux"</span>)</span>
<span id="cb9-8"><a href="#cb9-8"></a></span>
<span id="cb9-9"><a href="#cb9-9"></a>image <span class="op">=</span> Image.debian_slim(python_version<span class="op">=</span><span class="st">"3.11"</span>).run_commands(</span>
<span id="cb9-10"><a href="#cb9-10"></a>    <span class="st">"apt-get update &amp;&amp; apt-get install -y git"</span>,</span>
<span id="cb9-11"><a href="#cb9-11"></a>    <span class="st">"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"</span>,</span>
<span id="cb9-12"><a href="#cb9-12"></a>    <span class="st">"pip install transformers"</span>,</span>
<span id="cb9-13"><a href="#cb9-13"></a>    <span class="st">"pip install accelerate"</span>,</span>
<span id="cb9-14"><a href="#cb9-14"></a>    <span class="st">"pip install sentencepiece"</span>,</span>
<span id="cb9-15"><a href="#cb9-15"></a>    <span class="st">"pip install git+https://github.com/huggingface/diffusers.git"</span>,</span>
<span id="cb9-16"><a href="#cb9-16"></a>    <span class="st">"pip install python-dotenv"</span>,</span>
<span id="cb9-17"><a href="#cb9-17"></a>    <span class="ss">f'huggingface-cli login --token </span><span class="sc">{</span>os<span class="sc">.</span>environ[<span class="st">"HUGGING_FACE_ACCESS_TOKEN"</span>]<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb9-18"><a href="#cb9-18"></a>)</span>
<span id="cb9-19"><a href="#cb9-19"></a></span>
<span id="cb9-20"><a href="#cb9-20"></a></span>
<span id="cb9-21"><a href="#cb9-21"></a><span class="at">@app.cls</span>(image<span class="op">=</span>image, secrets<span class="op">=</span>[modal.Secret.from_dotenv()], gpu<span class="op">=</span><span class="st">"A100"</span>, cpu<span class="op">=</span><span class="dv">4</span>, timeout<span class="op">=</span><span class="dv">600</span>, container_idle_timeout<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb9-22"><a href="#cb9-22"></a><span class="kw">class</span> Model:</span>
<span id="cb9-23"><a href="#cb9-23"></a>    <span class="at">@build</span>()</span>
<span id="cb9-24"><a href="#cb9-24"></a>    <span class="at">@enter</span>()</span>
<span id="cb9-25"><a href="#cb9-25"></a>    <span class="kw">def</span> setup(<span class="va">self</span>):</span>
<span id="cb9-26"><a href="#cb9-26"></a>        <span class="im">import</span> torch</span>
<span id="cb9-27"><a href="#cb9-27"></a>        <span class="im">from</span> diffusers <span class="im">import</span> FluxPipeline</span>
<span id="cb9-28"><a href="#cb9-28"></a>        <span class="im">from</span> transformers.utils <span class="im">import</span> move_cache</span>
<span id="cb9-29"><a href="#cb9-29"></a></span>
<span id="cb9-30"><a href="#cb9-30"></a>        <span class="co"># black-forest-labs/FLUX.1-schnell</span></span>
<span id="cb9-31"><a href="#cb9-31"></a>        <span class="co"># black-forest-labs/FLUX.1-dev</span></span>
<span id="cb9-32"><a href="#cb9-32"></a>        <span class="va">self</span>.model <span class="op">=</span> <span class="st">"black-forest-labs/FLUX.1-schnell"</span></span>
<span id="cb9-33"><a href="#cb9-33"></a>        <span class="va">self</span>.pipe <span class="op">=</span> FluxPipeline.from_pretrained(<span class="va">self</span>.model, torch_dtype<span class="op">=</span>torch.bfloat16).to(<span class="st">"cuda"</span>)</span>
<span id="cb9-34"><a href="#cb9-34"></a>        move_cache()</span>
<span id="cb9-35"><a href="#cb9-35"></a></span>
<span id="cb9-36"><a href="#cb9-36"></a>    <span class="at">@modal.web_endpoint</span>(method<span class="op">=</span><span class="st">"POST"</span>, docs<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-37"><a href="#cb9-37"></a>    <span class="kw">def</span> f(<span class="va">self</span>, data: <span class="bu">dict</span>):</span>
<span id="cb9-38"><a href="#cb9-38"></a>        <span class="im">import</span> torch</span>
<span id="cb9-39"><a href="#cb9-39"></a>        <span class="im">import</span> random</span>
<span id="cb9-40"><a href="#cb9-40"></a>        <span class="im">from</span> io <span class="im">import</span> BytesIO</span>
<span id="cb9-41"><a href="#cb9-41"></a>        <span class="im">import</span> base64</span>
<span id="cb9-42"><a href="#cb9-42"></a></span>
<span id="cb9-43"><a href="#cb9-43"></a>        prompts <span class="op">=</span> data[<span class="st">"prompts"</span>]</span>
<span id="cb9-44"><a href="#cb9-44"></a>        fnames <span class="op">=</span> data[<span class="st">"fnames"</span>]</span>
<span id="cb9-45"><a href="#cb9-45"></a>        num_inference_steps <span class="op">=</span> data.get(<span class="st">"num_inference_steps"</span>, <span class="dv">4</span>)</span>
<span id="cb9-46"><a href="#cb9-46"></a>        seed <span class="op">=</span> data.get(<span class="st">"seed"</span>, random.randint(<span class="dv">1</span>, <span class="dv">2</span><span class="op">**</span><span class="dv">63</span> <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb9-47"><a href="#cb9-47"></a>        guidance_scale <span class="op">=</span> data.get(<span class="st">"guidance_scale"</span>, <span class="fl">3.5</span>)</span>
<span id="cb9-48"><a href="#cb9-48"></a></span>
<span id="cb9-49"><a href="#cb9-49"></a>        results <span class="op">=</span> []</span>
<span id="cb9-50"><a href="#cb9-50"></a>        <span class="cf">for</span> prompt, fname <span class="kw">in</span> <span class="bu">zip</span>(prompts, fnames):</span>
<span id="cb9-51"><a href="#cb9-51"></a>            image <span class="op">=</span> <span class="va">self</span>.pipe(</span>
<span id="cb9-52"><a href="#cb9-52"></a>                prompt,</span>
<span id="cb9-53"><a href="#cb9-53"></a>                output_type<span class="op">=</span><span class="st">"pil"</span>,</span>
<span id="cb9-54"><a href="#cb9-54"></a>                num_inference_steps<span class="op">=</span>num_inference_steps,</span>
<span id="cb9-55"><a href="#cb9-55"></a>                generator<span class="op">=</span>torch.Generator(<span class="st">"cpu"</span>).manual_seed(seed),</span>
<span id="cb9-56"><a href="#cb9-56"></a>                guidance_scale<span class="op">=</span>guidance_scale,</span>
<span id="cb9-57"><a href="#cb9-57"></a>            ).images[<span class="dv">0</span>]</span>
<span id="cb9-58"><a href="#cb9-58"></a></span>
<span id="cb9-59"><a href="#cb9-59"></a>            <span class="co"># Convert PIL image to bytes</span></span>
<span id="cb9-60"><a href="#cb9-60"></a>            buffered <span class="op">=</span> BytesIO()</span>
<span id="cb9-61"><a href="#cb9-61"></a>            image.save(buffered, <span class="bu">format</span><span class="op">=</span><span class="st">"PNG"</span>)</span>
<span id="cb9-62"><a href="#cb9-62"></a>            img_str <span class="op">=</span> base64.b64encode(buffered.getvalue()).decode()</span>
<span id="cb9-63"><a href="#cb9-63"></a></span>
<span id="cb9-64"><a href="#cb9-64"></a>            results.append(</span>
<span id="cb9-65"><a href="#cb9-65"></a>                {</span>
<span id="cb9-66"><a href="#cb9-66"></a>                    <span class="st">"filename"</span>: <span class="ss">f"</span><span class="sc">{</span>fname<span class="sc">}</span><span class="ss">_guidance_scale_</span><span class="sc">{</span>guidance_scale<span class="sc">}</span><span class="ss">_num_inference_steps_</span><span class="sc">{</span>num_inference_steps<span class="sc">}</span><span class="ss">_seed_</span><span class="sc">{</span>seed<span class="sc">}</span><span class="ss">_model_</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>model<span class="sc">.</span>replace(<span class="st">'/'</span>, <span class="st">'_'</span>)<span class="sc">}</span><span class="ss">.png"</span>,</span>
<span id="cb9-67"><a href="#cb9-67"></a>                    <span class="st">"image"</span>: img_str,</span>
<span id="cb9-68"><a href="#cb9-68"></a>                }</span>
<span id="cb9-69"><a href="#cb9-69"></a>            )</span>
<span id="cb9-70"><a href="#cb9-70"></a></span>
<span id="cb9-71"><a href="#cb9-71"></a>        <span class="cf">return</span> results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<ul>
<li>show: <code>modal container list</code></li>
</ul>
</section></section>
<section>
<section id="live-demo-2-vision-language-model" class="title-slide slide level1 center">
<h1>Live Demo 2: Vision Language Model</h1>

</section>
<section id="vision-language-models-are-becoming-the-new-norm" class="slide level2 smaller">
<h2>Vision Language Models Are Becoming the New Norm</h2>
<ul>
<li><a href="https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct">Qwen/Qwen2-VL-7B-Instruct</a></li>
<li><a href="https://huggingface.co/mistralai/Pixtral-12B-2409">mistralai/Pixtral-12B-2409</a></li>
<li><a href="https://x.com/DrJimFan/status/1836437271073243450">a family of frontier-class multimodal LLMs just released from NVIDIA</a></li>
</ul>
<p>We will be using Qwen2-VL-7B-Instruct for this demo.</p>
<p>We will deploy this model not as a web endpoint, but as a function that can be called from any other python context, including from within other Modal containers.</p>
</section>
<section id="image-example-1" class="slide level2 smaller">
<h2>Image Example 1</h2>
<pre><code>modal run qwen2_vl_78_Instruct.py</code></pre>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img data-src="imgs/tropical_island_paradise_guidance_scale_3.5_num_inference_steps_50_seed_5013302010029533033_model_black-forest-labs_FLUX.1-dev.png"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img data-src="imgs/sci_fi_forest_ship_guidance_scale_3.5_num_inference_steps_50_seed_6510529542810937186_model_black-forest-labs_FLUX.1-dev.png"></p>
</div>
</div>
</div>
</section>
<section id="image-example-1-1" class="slide level2 smaller">
<h2>Image Example 1</h2>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/tropical_island_paradise_guidance_scale_3.5_num_inference_steps_50_seed_5013302010029533033_model_black-forest-labs_FLUX.1-dev.png"></p>
<figcaption>The image depicts a small, tropical island with a white sandy beach surrounded by crystal-clear turquoise waters. The island is covered with lush greenery and palm trees, and there is a small structure on the beach. The sky is mostly clear with a few scattered clouds.</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="imgs/sci_fi_forest_ship_guidance_scale_3.5_num_inference_steps_50_seed_6510529542810937186_model_black-forest-labs_FLUX.1-dev.png"></p>
<figcaption>This image depicts a futuristic, floating structure nestled within a lush, verdant landscape. The structure appears to be an abandoned or repurposed aircraft, possibly a large jet or a military transport plane, given its size and shape. The aircraft has been partially integrated into the natural environment, with vines and other greenery growing over its surface, suggesting a long period of disuse or abandonment.aircraft is suspended in mid-air, supported by unseen technology or infrastructure that is not visible in the image. The surrounding area is dense with greenery, including trees, bushes, and other vegetation, creating a sense of harmony between the man-made structure and the natural world. The landscape below the aircraft features a river or stream, flanked by rocky cliffs and dense foliage, adding to the serene and almost otherworldly atmosphere of the scene.overall color palette is dominated by greens and browns, with the aircraft’s rusted metal adding a touch of industrial decay. The lighting is soft and diffused, likely from an overcast sky, which enhances the tranquil and somewhat mystical ambiance of the image. The scene evokes a sense of isolation and abandonment, yet also a connection to nature, as if the aircraft has been reclaimed by the environment</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p><em>The first image appears to be a real photograph of a tropical island with clear blue waters and a small sandy beach. The natural lighting, shadows, and details in the water and sky suggest that this is a real image.second image, on the other hand, depicts a futuristic, floating structure covered in vegetation, suspended over a lush, green landscape with a river. The level of detail, the blending of natural and artificial elements, and the overall surreal nature of the scene suggest that this is likely an AI-generated image. The combination of a realistic natural environment with an advanced, almost sci-fi structure is not something typically captured in real photographs.</em></p>
</section>
<section id="qwen2-vl-7b-instruct-app-code" class="slide level2 smaller">
<h2>Qwen2-VL-7B-Instruct App Code</h2>
<div class="cell" data-execution_count="5">
<div class="cell-output cell-output-display">
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="im">import</span> modal</span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="im">from</span> modal <span class="im">import</span> build, enter</span>
<span id="cb11-3"><a href="#cb11-3"></a><span class="im">import</span> os</span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="im">from</span> dotenv <span class="im">import</span> load_dotenv</span>
<span id="cb11-5"><a href="#cb11-5"></a></span>
<span id="cb11-6"><a href="#cb11-6"></a>load_dotenv()</span>
<span id="cb11-7"><a href="#cb11-7"></a>app <span class="op">=</span> modal.App(<span class="st">"qwen2_vl_78_Instruct"</span>)</span>
<span id="cb11-8"><a href="#cb11-8"></a></span>
<span id="cb11-9"><a href="#cb11-9"></a>cuda_version <span class="op">=</span> <span class="st">"12.4.0"</span>  <span class="co"># should be no greater than host CUDA version</span></span>
<span id="cb11-10"><a href="#cb11-10"></a>flavor <span class="op">=</span> <span class="st">"devel"</span>  <span class="co">#  includes full CUDA toolkit</span></span>
<span id="cb11-11"><a href="#cb11-11"></a>operating_sys <span class="op">=</span> <span class="st">"ubuntu22.04"</span></span>
<span id="cb11-12"><a href="#cb11-12"></a>tag <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>cuda_version<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>flavor<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>operating_sys<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb11-13"><a href="#cb11-13"></a>image <span class="op">=</span> (</span>
<span id="cb11-14"><a href="#cb11-14"></a>    modal.Image.from_registry(<span class="ss">f"nvidia/cuda:</span><span class="sc">{</span>tag<span class="sc">}</span><span class="ss">"</span>, add_python<span class="op">=</span><span class="st">"3.11"</span>)</span>
<span id="cb11-15"><a href="#cb11-15"></a>    .apt_install(<span class="st">"git"</span>)</span>
<span id="cb11-16"><a href="#cb11-16"></a>    .pip_install(</span>
<span id="cb11-17"><a href="#cb11-17"></a>        <span class="st">"ninja"</span>,  <span class="co"># required to build flash-attn</span></span>
<span id="cb11-18"><a href="#cb11-18"></a>        <span class="st">"packaging"</span>,  <span class="co"># required to build flash-attn</span></span>
<span id="cb11-19"><a href="#cb11-19"></a>        <span class="st">"wheel"</span>,  <span class="co"># required to build flash-attn</span></span>
<span id="cb11-20"><a href="#cb11-20"></a>    )</span>
<span id="cb11-21"><a href="#cb11-21"></a>    .run_commands(</span>
<span id="cb11-22"><a href="#cb11-22"></a>        <span class="st">"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"</span>,</span>
<span id="cb11-23"><a href="#cb11-23"></a>        <span class="st">"pip install git+https://github.com/huggingface/transformers"</span>,</span>
<span id="cb11-24"><a href="#cb11-24"></a>        <span class="st">"pip install accelerate"</span>,</span>
<span id="cb11-25"><a href="#cb11-25"></a>        <span class="st">"pip install qwen-vl-utils"</span>,</span>
<span id="cb11-26"><a href="#cb11-26"></a>        <span class="st">"pip install python-dotenv"</span>,</span>
<span id="cb11-27"><a href="#cb11-27"></a>        <span class="ss">f'huggingface-cli login --token </span><span class="sc">{</span>os<span class="sc">.</span>environ[<span class="st">"HUGGING_FACE_ACCESS_TOKEN"</span>]<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb11-28"><a href="#cb11-28"></a>    )</span>
<span id="cb11-29"><a href="#cb11-29"></a>    .run_commands(<span class="st">"pip install flash-attn --no-build-isolation"</span>)</span>
<span id="cb11-30"><a href="#cb11-30"></a>)</span>
<span id="cb11-31"><a href="#cb11-31"></a></span>
<span id="cb11-32"><a href="#cb11-32"></a></span>
<span id="cb11-33"><a href="#cb11-33"></a><span class="at">@app.cls</span>(image<span class="op">=</span>image, secrets<span class="op">=</span>[modal.Secret.from_dotenv()], gpu<span class="op">=</span>modal.gpu.A100(count<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span><span class="st">"80GB"</span>), cpu<span class="op">=</span><span class="dv">4</span>, timeout<span class="op">=</span><span class="dv">600</span>, container_idle_timeout<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb11-34"><a href="#cb11-34"></a><span class="kw">class</span> Model:</span>
<span id="cb11-35"><a href="#cb11-35"></a>    <span class="at">@build</span>()</span>
<span id="cb11-36"><a href="#cb11-36"></a>    <span class="at">@enter</span>()</span>
<span id="cb11-37"><a href="#cb11-37"></a>    <span class="kw">def</span> setup(<span class="va">self</span>):</span>
<span id="cb11-38"><a href="#cb11-38"></a>        <span class="im">from</span> transformers <span class="im">import</span> Qwen2VLForConditionalGeneration, AutoProcessor, TextStreamer</span>
<span id="cb11-39"><a href="#cb11-39"></a>        <span class="im">import</span> torch</span>
<span id="cb11-40"><a href="#cb11-40"></a></span>
<span id="cb11-41"><a href="#cb11-41"></a>        <span class="co"># We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.</span></span>
<span id="cb11-42"><a href="#cb11-42"></a></span>
<span id="cb11-43"><a href="#cb11-43"></a>        <span class="va">self</span>.model <span class="op">=</span> Qwen2VLForConditionalGeneration.from_pretrained(</span>
<span id="cb11-44"><a href="#cb11-44"></a>            <span class="st">"Qwen/Qwen2-VL-7B-Instruct"</span>,</span>
<span id="cb11-45"><a href="#cb11-45"></a>            torch_dtype<span class="op">=</span>torch.bfloat16,</span>
<span id="cb11-46"><a href="#cb11-46"></a>            attn_implementation<span class="op">=</span><span class="st">"flash_attention_2"</span>,</span>
<span id="cb11-47"><a href="#cb11-47"></a>            vision_config<span class="op">=</span>{<span class="st">"torch_dtype"</span>: torch.bfloat16},</span>
<span id="cb11-48"><a href="#cb11-48"></a>            device_map<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb11-49"><a href="#cb11-49"></a>        )</span>
<span id="cb11-50"><a href="#cb11-50"></a>        <span class="co"># default processor</span></span>
<span id="cb11-51"><a href="#cb11-51"></a>        <span class="va">self</span>.processor <span class="op">=</span> AutoProcessor.from_pretrained(<span class="st">"Qwen/Qwen2-VL-7B-Instruct"</span>)</span>
<span id="cb11-52"><a href="#cb11-52"></a></span>
<span id="cb11-53"><a href="#cb11-53"></a>        <span class="co"># The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.</span></span>
<span id="cb11-54"><a href="#cb11-54"></a>        <span class="co"># min_pixels = 256*28*28</span></span>
<span id="cb11-55"><a href="#cb11-55"></a>        <span class="co"># max_pixels = 1280*28*28</span></span>
<span id="cb11-56"><a href="#cb11-56"></a>        <span class="co"># processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-7B-Instruct", min_pixels=min_pixels, max_pixels=max_pixels)</span></span>
<span id="cb11-57"><a href="#cb11-57"></a></span>
<span id="cb11-58"><a href="#cb11-58"></a>        <span class="va">self</span>.streamer <span class="op">=</span> TextStreamer(<span class="va">self</span>.processor, skip_prompt<span class="op">=</span><span class="va">True</span>, skip_special_tokens<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-59"><a href="#cb11-59"></a></span>
<span id="cb11-60"><a href="#cb11-60"></a>    <span class="at">@modal.method</span>()</span>
<span id="cb11-61"><a href="#cb11-61"></a>    <span class="kw">def</span> f(<span class="va">self</span>, messages_list, max_new_tokens<span class="op">=</span><span class="dv">512</span>, show_stream<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb11-62"><a href="#cb11-62"></a>        <span class="im">from</span> qwen_vl_utils <span class="im">import</span> process_vision_info</span>
<span id="cb11-63"><a href="#cb11-63"></a></span>
<span id="cb11-64"><a href="#cb11-64"></a>        <span class="kw">def</span> messages_inference(messages):</span>
<span id="cb11-65"><a href="#cb11-65"></a>            <span class="co"># Preparation for inference</span></span>
<span id="cb11-66"><a href="#cb11-66"></a>            text <span class="op">=</span> <span class="va">self</span>.processor.apply_chat_template(messages, tokenize<span class="op">=</span><span class="va">False</span>, add_generation_prompt<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-67"><a href="#cb11-67"></a>            image_inputs, video_inputs <span class="op">=</span> process_vision_info(messages)</span>
<span id="cb11-68"><a href="#cb11-68"></a>            inputs <span class="op">=</span> <span class="va">self</span>.processor(</span>
<span id="cb11-69"><a href="#cb11-69"></a>                text<span class="op">=</span>[text],</span>
<span id="cb11-70"><a href="#cb11-70"></a>                images<span class="op">=</span>image_inputs,</span>
<span id="cb11-71"><a href="#cb11-71"></a>                videos<span class="op">=</span>video_inputs,</span>
<span id="cb11-72"><a href="#cb11-72"></a>                padding<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb11-73"><a href="#cb11-73"></a>                return_tensors<span class="op">=</span><span class="st">"pt"</span>,</span>
<span id="cb11-74"><a href="#cb11-74"></a>            )</span>
<span id="cb11-75"><a href="#cb11-75"></a>            inputs <span class="op">=</span> inputs.to(<span class="st">"cuda"</span>)</span>
<span id="cb11-76"><a href="#cb11-76"></a></span>
<span id="cb11-77"><a href="#cb11-77"></a>            <span class="co"># Inference: Generation of the output</span></span>
<span id="cb11-78"><a href="#cb11-78"></a>            <span class="cf">if</span> show_stream:</span>
<span id="cb11-79"><a href="#cb11-79"></a>                <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n\n</span><span class="st">-----------------------------------------------------------</span><span class="ch">\n\n</span><span class="st">"</span>)</span>
<span id="cb11-80"><a href="#cb11-80"></a>                generated_ids <span class="op">=</span> <span class="va">self</span>.model.generate(<span class="op">**</span>inputs, max_new_tokens<span class="op">=</span>max_new_tokens, streamer<span class="op">=</span><span class="va">self</span>.streamer)</span>
<span id="cb11-81"><a href="#cb11-81"></a>            <span class="cf">else</span>:</span>
<span id="cb11-82"><a href="#cb11-82"></a>                generated_ids <span class="op">=</span> <span class="va">self</span>.model.generate(<span class="op">**</span>inputs, max_new_tokens<span class="op">=</span>max_new_tokens)</span>
<span id="cb11-83"><a href="#cb11-83"></a>            generated_ids_trimmed <span class="op">=</span> [out_ids[<span class="bu">len</span>(in_ids) :] <span class="cf">for</span> in_ids, out_ids <span class="kw">in</span> <span class="bu">zip</span>(inputs.input_ids, generated_ids)]</span>
<span id="cb11-84"><a href="#cb11-84"></a>            output_text <span class="op">=</span> <span class="va">self</span>.processor.batch_decode(generated_ids_trimmed, skip_special_tokens<span class="op">=</span><span class="va">True</span>, clean_up_tokenization_spaces<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb11-85"><a href="#cb11-85"></a>            <span class="cf">return</span> output_text</span>
<span id="cb11-86"><a href="#cb11-86"></a></span>
<span id="cb11-87"><a href="#cb11-87"></a>        <span class="cf">return</span> [messages_inference(messages) <span class="cf">for</span> messages <span class="kw">in</span> messages_list]</span>
<span id="cb11-88"><a href="#cb11-88"></a></span>
<span id="cb11-89"><a href="#cb11-89"></a></span>
<span id="cb11-90"><a href="#cb11-90"></a><span class="at">@app.local_entrypoint</span>()</span>
<span id="cb11-91"><a href="#cb11-91"></a><span class="kw">def</span> main():</span>
<span id="cb11-92"><a href="#cb11-92"></a>    s3_bucket <span class="op">=</span> os.environ[<span class="st">"S3_BUCKET"</span>]  <span class="co"># where my images are hosted</span></span>
<span id="cb11-93"><a href="#cb11-93"></a>    s3_prefix <span class="op">=</span> os.environ[<span class="st">"S3_PREFIX"</span>]  <span class="co"># where my images are hosted</span></span>
<span id="cb11-94"><a href="#cb11-94"></a>    model <span class="op">=</span> Model()</span>
<span id="cb11-95"><a href="#cb11-95"></a></span>
<span id="cb11-96"><a href="#cb11-96"></a>    messages_list <span class="op">=</span> [</span>
<span id="cb11-97"><a href="#cb11-97"></a>        [</span>
<span id="cb11-98"><a href="#cb11-98"></a>            {</span>
<span id="cb11-99"><a href="#cb11-99"></a>                <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb11-100"><a href="#cb11-100"></a>                <span class="st">"content"</span>: [</span>
<span id="cb11-101"><a href="#cb11-101"></a>                    {</span>
<span id="cb11-102"><a href="#cb11-102"></a>                        <span class="st">"type"</span>: <span class="st">"image"</span>,</span>
<span id="cb11-103"><a href="#cb11-103"></a>                        <span class="st">"image"</span>: <span class="ss">f"https://</span><span class="sc">{</span>s3_bucket<span class="sc">}</span><span class="ss">.s3.amazonaws.com</span><span class="sc">{</span>s3_prefix<span class="sc">}</span><span class="ss">tropical_island_paradise_guidance_scale_3.5_num_inference_steps_50_seed_5013302010029533033_model_black-forest-labs_FLUX.1-dev.png"</span>,</span>
<span id="cb11-104"><a href="#cb11-104"></a>                    },</span>
<span id="cb11-105"><a href="#cb11-105"></a>                    {<span class="st">"type"</span>: <span class="st">"text"</span>, <span class="st">"text"</span>: <span class="st">"Give a short description of this image."</span>},</span>
<span id="cb11-106"><a href="#cb11-106"></a>                ],</span>
<span id="cb11-107"><a href="#cb11-107"></a>            }</span>
<span id="cb11-108"><a href="#cb11-108"></a>        ],</span>
<span id="cb11-109"><a href="#cb11-109"></a>        [</span>
<span id="cb11-110"><a href="#cb11-110"></a>            {</span>
<span id="cb11-111"><a href="#cb11-111"></a>                <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb11-112"><a href="#cb11-112"></a>                <span class="st">"content"</span>: [</span>
<span id="cb11-113"><a href="#cb11-113"></a>                    {</span>
<span id="cb11-114"><a href="#cb11-114"></a>                        <span class="st">"type"</span>: <span class="st">"image"</span>,</span>
<span id="cb11-115"><a href="#cb11-115"></a>                        <span class="st">"image"</span>: <span class="ss">f"https://</span><span class="sc">{</span>s3_bucket<span class="sc">}</span><span class="ss">.s3.amazonaws.com</span><span class="sc">{</span>s3_prefix<span class="sc">}</span><span class="ss">sci_fi_forest_ship_guidance_scale_3.5_num_inference_steps_50_seed_6510529542810937186_model_black-forest-labs_FLUX.1-dev.png"</span>,</span>
<span id="cb11-116"><a href="#cb11-116"></a>                    },</span>
<span id="cb11-117"><a href="#cb11-117"></a>                    {<span class="st">"type"</span>: <span class="st">"text"</span>, <span class="st">"text"</span>: <span class="st">"Give a long detailed description of this image."</span>},</span>
<span id="cb11-118"><a href="#cb11-118"></a>                ],</span>
<span id="cb11-119"><a href="#cb11-119"></a>            }</span>
<span id="cb11-120"><a href="#cb11-120"></a>        ],</span>
<span id="cb11-121"><a href="#cb11-121"></a>        [</span>
<span id="cb11-122"><a href="#cb11-122"></a>            {</span>
<span id="cb11-123"><a href="#cb11-123"></a>                <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb11-124"><a href="#cb11-124"></a>                <span class="st">"content"</span>: [</span>
<span id="cb11-125"><a href="#cb11-125"></a>                    {</span>
<span id="cb11-126"><a href="#cb11-126"></a>                        <span class="st">"type"</span>: <span class="st">"image"</span>,</span>
<span id="cb11-127"><a href="#cb11-127"></a>                        <span class="st">"image"</span>: <span class="ss">f"https://</span><span class="sc">{</span>s3_bucket<span class="sc">}</span><span class="ss">.s3.amazonaws.com</span><span class="sc">{</span>s3_prefix<span class="sc">}</span><span class="ss">tropical_island_paradise_guidance_scale_3.5_num_inference_steps_50_seed_5013302010029533033_model_black-forest-labs_FLUX.1-dev.png"</span>,</span>
<span id="cb11-128"><a href="#cb11-128"></a>                    },</span>
<span id="cb11-129"><a href="#cb11-129"></a>                    {</span>
<span id="cb11-130"><a href="#cb11-130"></a>                        <span class="st">"type"</span>: <span class="st">"image"</span>,</span>
<span id="cb11-131"><a href="#cb11-131"></a>                        <span class="st">"image"</span>: <span class="ss">f"https://</span><span class="sc">{</span>s3_bucket<span class="sc">}</span><span class="ss">.s3.amazonaws.com</span><span class="sc">{</span>s3_prefix<span class="sc">}</span><span class="ss">sci_fi_forest_ship_guidance_scale_3.5_num_inference_steps_50_seed_6510529542810937186_model_black-forest-labs_FLUX.1-dev.png"</span>,</span>
<span id="cb11-132"><a href="#cb11-132"></a>                    },</span>
<span id="cb11-133"><a href="#cb11-133"></a>                    {<span class="st">"type"</span>: <span class="st">"text"</span>, <span class="st">"text"</span>: <span class="st">"For each image explain whether you think it is real or AI generated. Explain your reasoning."</span>},</span>
<span id="cb11-134"><a href="#cb11-134"></a>                ],</span>
<span id="cb11-135"><a href="#cb11-135"></a>            }</span>
<span id="cb11-136"><a href="#cb11-136"></a>        ],</span>
<span id="cb11-137"><a href="#cb11-137"></a>    ]</span>
<span id="cb11-138"><a href="#cb11-138"></a></span>
<span id="cb11-139"><a href="#cb11-139"></a>    <span class="bu">print</span>(<span class="st">"Testing Multiple Container with show_stream=False"</span>)</span>
<span id="cb11-140"><a href="#cb11-140"></a>    <span class="cf">for</span> res <span class="kw">in</span> model.f.starmap([(messages_list[:<span class="dv">1</span>], <span class="dv">1000</span>, <span class="va">False</span>), (messages_list[<span class="dv">1</span>:<span class="dv">2</span>], <span class="dv">1000</span>, <span class="va">False</span>), (messages_list[<span class="dv">2</span>:<span class="dv">3</span>], <span class="dv">1000</span>, <span class="va">False</span>)]):</span>
<span id="cb11-141"><a href="#cb11-141"></a>        <span class="cf">for</span> r <span class="kw">in</span> res:</span>
<span id="cb11-142"><a href="#cb11-142"></a>            <span class="bu">print</span>(r)</span>
<span id="cb11-143"><a href="#cb11-143"></a>            <span class="bu">print</span>(<span class="st">"</span><span class="ch">\n\n</span><span class="st">-----------------------------------------------------------</span><span class="ch">\n\n</span><span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</section>
<section id="image-example-2" class="slide level2 smaller">
<h2>Image Example 2</h2>
<p>List the top 5 countries in Europe with the highest GDP. <img data-src="images/gdp.webp"></p>
</section>
<section id="image-example-2-cont." class="slide level2 smaller">
<h2>Image Example 2 Cont.</h2>
<pre><code>modal deploy qwen2_vl_78_Instruct.py</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="im">import</span> dotenv</span>
<span id="cb13-2"><a href="#cb13-2"></a>dotenv.load_dotenv()</span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="im">import</span> os</span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="co"># where my images are</span></span>
<span id="cb13-5"><a href="#cb13-5"></a>s3_bucket <span class="op">=</span> os.environ[<span class="st">"S3_BUCKET"</span>] </span>
<span id="cb13-6"><a href="#cb13-6"></a>s3_prefix <span class="op">=</span> os.environ[<span class="st">"S3_PREFIX"</span>]</span>
<span id="cb13-7"><a href="#cb13-7"></a></span>
<span id="cb13-8"><a href="#cb13-8"></a><span class="im">import</span> modal</span>
<span id="cb13-9"><a href="#cb13-9"></a>f <span class="op">=</span> modal.Function.lookup(<span class="st">"qwen2_vl_78_Instruct"</span>, <span class="st">"Model.f"</span>)</span>
<span id="cb13-10"><a href="#cb13-10"></a>messages_list <span class="op">=</span> [</span>
<span id="cb13-11"><a href="#cb13-11"></a>        [</span>
<span id="cb13-12"><a href="#cb13-12"></a>            {</span>
<span id="cb13-13"><a href="#cb13-13"></a>                <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb13-14"><a href="#cb13-14"></a>                <span class="st">"content"</span>: [</span>
<span id="cb13-15"><a href="#cb13-15"></a>                    {</span>
<span id="cb13-16"><a href="#cb13-16"></a>                        <span class="st">"type"</span>: <span class="st">"image"</span>,</span>
<span id="cb13-17"><a href="#cb13-17"></a>                        <span class="st">"image"</span>: <span class="ss">f"https://</span><span class="sc">{</span>s3_bucket<span class="sc">}</span><span class="ss">.s3.amazonaws.com</span><span class="sc">{</span>s3_prefix<span class="sc">}</span><span class="ss">gdp.webp"</span>,</span>
<span id="cb13-18"><a href="#cb13-18"></a>                    },</span>
<span id="cb13-19"><a href="#cb13-19"></a>                    {<span class="st">"type"</span>: <span class="st">"text"</span>, <span class="st">"text"</span>: <span class="st">"List the top 5 countries in Europe with the highest GDP"</span>},</span>
<span id="cb13-20"><a href="#cb13-20"></a>                ],</span>
<span id="cb13-21"><a href="#cb13-21"></a>            }</span>
<span id="cb13-22"><a href="#cb13-22"></a>        ]</span>
<span id="cb13-23"><a href="#cb13-23"></a>]</span>
<span id="cb13-24"><a href="#cb13-24"></a>f.remote(messages_list, max_new_tokens<span class="op">=</span><span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="image-example-3" class="slide level2 smaller">
<h2>Image Example 3</h2>

<img data-src="imgs/dashboard_sample.png" class="r-stretch"></section>
<section id="image-example-3-cont." class="slide level2 smaller">
<h2>Image Example 3 Cont.</h2>
<div class="sourceCode" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="im">import</span> dotenv</span>
<span id="cb14-2"><a href="#cb14-2"></a><span class="im">import</span> os</span>
<span id="cb14-3"><a href="#cb14-3"></a>dotenv.load_dotenv()</span>
<span id="cb14-4"><a href="#cb14-4"></a><span class="co"># where my images are</span></span>
<span id="cb14-5"><a href="#cb14-5"></a>s3_bucket <span class="op">=</span> os.environ[<span class="st">"S3_BUCKET"</span>]</span>
<span id="cb14-6"><a href="#cb14-6"></a>s3_prefix <span class="op">=</span> os.environ[<span class="st">"S3_PREFIX"</span>]</span>
<span id="cb14-7"><a href="#cb14-7"></a></span>
<span id="cb14-8"><a href="#cb14-8"></a><span class="im">import</span> modal</span>
<span id="cb14-9"><a href="#cb14-9"></a></span>
<span id="cb14-10"><a href="#cb14-10"></a>f <span class="op">=</span> modal.Function.lookup(<span class="st">"qwen2_vl_78_Instruct"</span>, <span class="st">"Model.f"</span>)</span>
<span id="cb14-11"><a href="#cb14-11"></a>messages_list <span class="op">=</span> [</span>
<span id="cb14-12"><a href="#cb14-12"></a>    [</span>
<span id="cb14-13"><a href="#cb14-13"></a>        {</span>
<span id="cb14-14"><a href="#cb14-14"></a>            <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb14-15"><a href="#cb14-15"></a>            <span class="st">"content"</span>: [</span>
<span id="cb14-16"><a href="#cb14-16"></a>                {</span>
<span id="cb14-17"><a href="#cb14-17"></a>                    <span class="st">"type"</span>: <span class="st">"image"</span>,</span>
<span id="cb14-18"><a href="#cb14-18"></a>                    <span class="st">"image"</span>: <span class="ss">f"https://</span><span class="sc">{</span>s3_bucket<span class="sc">}</span><span class="ss">.s3.amazonaws.com</span><span class="sc">{</span>s3_prefix<span class="sc">}</span><span class="ss">dashboard_sample.png"</span>,</span>
<span id="cb14-19"><a href="#cb14-19"></a>                },</span>
<span id="cb14-20"><a href="#cb14-20"></a>                {<span class="st">"type"</span>: <span class="st">"text"</span>, <span class="st">"text"</span>: <span class="st">"Which brand had the largest drop in the number of posts?"</span>},</span>
<span id="cb14-21"><a href="#cb14-21"></a>            ],</span>
<span id="cb14-22"><a href="#cb14-22"></a>        }</span>
<span id="cb14-23"><a href="#cb14-23"></a>    ],</span>
<span id="cb14-24"><a href="#cb14-24"></a>    [</span>
<span id="cb14-25"><a href="#cb14-25"></a>        {</span>
<span id="cb14-26"><a href="#cb14-26"></a>            <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb14-27"><a href="#cb14-27"></a>            <span class="st">"content"</span>: [</span>
<span id="cb14-28"><a href="#cb14-28"></a>                {</span>
<span id="cb14-29"><a href="#cb14-29"></a>                    <span class="st">"type"</span>: <span class="st">"image"</span>,</span>
<span id="cb14-30"><a href="#cb14-30"></a>                    <span class="st">"image"</span>: <span class="ss">f"https://</span><span class="sc">{</span>s3_bucket<span class="sc">}</span><span class="ss">.s3.amazonaws.com</span><span class="sc">{</span>s3_prefix<span class="sc">}</span><span class="ss">dashboard_sample.png"</span>,</span>
<span id="cb14-31"><a href="#cb14-31"></a>                },</span>
<span id="cb14-32"><a href="#cb14-32"></a>                {<span class="st">"type"</span>: <span class="st">"text"</span>, <span class="st">"text"</span>: <span class="st">"Which day did NBA have the most posts?"</span>},</span>
<span id="cb14-33"><a href="#cb14-33"></a>            ],</span>
<span id="cb14-34"><a href="#cb14-34"></a>        }</span>
<span id="cb14-35"><a href="#cb14-35"></a>    ],</span>
<span id="cb14-36"><a href="#cb14-36"></a>    [</span>
<span id="cb14-37"><a href="#cb14-37"></a>        {</span>
<span id="cb14-38"><a href="#cb14-38"></a>            <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb14-39"><a href="#cb14-39"></a>            <span class="st">"content"</span>: [</span>
<span id="cb14-40"><a href="#cb14-40"></a>                {</span>
<span id="cb14-41"><a href="#cb14-41"></a>                    <span class="st">"type"</span>: <span class="st">"image"</span>,</span>
<span id="cb14-42"><a href="#cb14-42"></a>                    <span class="st">"image"</span>: <span class="ss">f"https://</span><span class="sc">{</span>s3_bucket<span class="sc">}</span><span class="ss">.s3.amazonaws.com</span><span class="sc">{</span>s3_prefix<span class="sc">}</span><span class="ss">dashboard_sample.png"</span>,</span>
<span id="cb14-43"><a href="#cb14-43"></a>                },</span>
<span id="cb14-44"><a href="#cb14-44"></a>                {<span class="st">"type"</span>: <span class="st">"text"</span>, <span class="st">"text"</span>: <span class="st">"Which brand is posting the least?"</span>},</span>
<span id="cb14-45"><a href="#cb14-45"></a>            ],</span>
<span id="cb14-46"><a href="#cb14-46"></a>        }</span>
<span id="cb14-47"><a href="#cb14-47"></a>    ],</span>
<span id="cb14-48"><a href="#cb14-48"></a>    [</span>
<span id="cb14-49"><a href="#cb14-49"></a>        {</span>
<span id="cb14-50"><a href="#cb14-50"></a>            <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb14-51"><a href="#cb14-51"></a>            <span class="st">"content"</span>: [</span>
<span id="cb14-52"><a href="#cb14-52"></a>                {</span>
<span id="cb14-53"><a href="#cb14-53"></a>                    <span class="st">"type"</span>: <span class="st">"image"</span>,</span>
<span id="cb14-54"><a href="#cb14-54"></a>                    <span class="st">"image"</span>: <span class="ss">f"https://</span><span class="sc">{</span>s3_bucket<span class="sc">}</span><span class="ss">.s3.amazonaws.com</span><span class="sc">{</span>s3_prefix<span class="sc">}</span><span class="ss">dashboard_sample.png"</span>,</span>
<span id="cb14-55"><a href="#cb14-55"></a>                },</span>
<span id="cb14-56"><a href="#cb14-56"></a>                {<span class="st">"type"</span>: <span class="st">"text"</span>, <span class="st">"text"</span>: <span class="st">"Which brand posted more? Revolve or Spotify?"</span>},</span>
<span id="cb14-57"><a href="#cb14-57"></a>            ],</span>
<span id="cb14-58"><a href="#cb14-58"></a>        }</span>
<span id="cb14-59"><a href="#cb14-59"></a>    ],</span>
<span id="cb14-60"><a href="#cb14-60"></a>]</span>
<span id="cb14-61"><a href="#cb14-61"></a><span class="cf">for</span> res <span class="kw">in</span> f.starmap([(messages_list[:<span class="dv">2</span>],), (messages_list[<span class="dv">2</span>:],)]):</span>
<span id="cb14-62"><a href="#cb14-62"></a>    <span class="cf">for</span> r <span class="kw">in</span> res:</span>
<span id="cb14-63"><a href="#cb14-63"></a>        <span class="bu">print</span>(r)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section></section>
<section>
<section id="live-demo-3-multimodal-rag-with-colpali" class="title-slide slide level1 center">
<h1>Live Demo 3: Multimodal RAG with ColPali</h1>

</section>
<section id="problem-description" class="slide level2">
<h2>Problem Description</h2>
<ul>
<li>You have a collection of documents (PDF/images) with text and visual content.</li>
<li>You want to be able to ask questions about the content and retrieve relevant information.</li>
<li>Traditional RAG with dense embeddings and chunking of text is limited.</li>
<li>You need a way to combine vision and language modalities for retrieval.</li>
</ul>
</section>
<section id="high-level-idea" class="slide level2">
<h2>High Level Idea</h2>

<img data-src="imgs/colpali2.png" class="r-stretch"></section>
<section id="high-level-idea-1" class="slide level2">
<h2>High Level Idea</h2>

<img data-src="imgs/colpali1.png" class="r-stretch"></section>
<section id="high-level-idea-2" class="slide level2">
<h2>High Level Idea</h2>
<ul>
<li>show the demo pdf</li>
<li>Use ColPali model to process each page of the PDF.</li>
<li>A query question comes in</li>
<li>Process the query with the ColPali model</li>
<li>Calculate the most 3 similar (to the query) pages/images of the PDF which most likely contain the answer</li>
<li>Pass off the 3 pages/images as context to our vision Language Model - Qwen2-VL-7B-Instruct</li>
<li>Ask the vision language model to answer the query</li>
<li>These two models (ColPali and Qwen2-VL-7B-Instruct) are running on separate Modal containers</li>
</ul>
</section>
<section id="colpali-app-code" class="slide level2 smaller">
<h2>ColPali App Code</h2>
<pre><code>modal deploy colpali.py</code></pre>
<div class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<div class="sourceCode" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="im">import</span> modal</span>
<span id="cb16-2"><a href="#cb16-2"></a><span class="im">from</span> modal <span class="im">import</span> build, enter</span>
<span id="cb16-3"><a href="#cb16-3"></a><span class="im">import</span> os</span>
<span id="cb16-4"><a href="#cb16-4"></a><span class="im">from</span> dotenv <span class="im">import</span> load_dotenv</span>
<span id="cb16-5"><a href="#cb16-5"></a></span>
<span id="cb16-6"><a href="#cb16-6"></a>load_dotenv()</span>
<span id="cb16-7"><a href="#cb16-7"></a>app <span class="op">=</span> modal.App(<span class="st">"colpali"</span>)</span>
<span id="cb16-8"><a href="#cb16-8"></a></span>
<span id="cb16-9"><a href="#cb16-9"></a>cuda_version <span class="op">=</span> <span class="st">"12.4.0"</span>  <span class="co"># should be no greater than host CUDA version</span></span>
<span id="cb16-10"><a href="#cb16-10"></a>flavor <span class="op">=</span> <span class="st">"devel"</span>  <span class="co">#  includes full CUDA toolkit</span></span>
<span id="cb16-11"><a href="#cb16-11"></a>operating_sys <span class="op">=</span> <span class="st">"ubuntu22.04"</span></span>
<span id="cb16-12"><a href="#cb16-12"></a>tag <span class="op">=</span> <span class="ss">f"</span><span class="sc">{</span>cuda_version<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>flavor<span class="sc">}</span><span class="ss">-</span><span class="sc">{</span>operating_sys<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb16-13"><a href="#cb16-13"></a>image <span class="op">=</span> (</span>
<span id="cb16-14"><a href="#cb16-14"></a>    modal.Image.from_registry(<span class="ss">f"nvidia/cuda:</span><span class="sc">{</span>tag<span class="sc">}</span><span class="ss">"</span>, add_python<span class="op">=</span><span class="st">"3.11"</span>)</span>
<span id="cb16-15"><a href="#cb16-15"></a>    .apt_install(<span class="st">"git"</span>, <span class="st">"poppler-utils"</span>)</span>
<span id="cb16-16"><a href="#cb16-16"></a>    .pip_install(</span>
<span id="cb16-17"><a href="#cb16-17"></a>        <span class="st">"ninja"</span>,  <span class="co"># required to build flash-attn</span></span>
<span id="cb16-18"><a href="#cb16-18"></a>        <span class="st">"packaging"</span>,  <span class="co"># required to build flash-attn</span></span>
<span id="cb16-19"><a href="#cb16-19"></a>        <span class="st">"wheel"</span>,  <span class="co"># required to build flash-attn</span></span>
<span id="cb16-20"><a href="#cb16-20"></a>    )</span>
<span id="cb16-21"><a href="#cb16-21"></a>    .run_commands(</span>
<span id="cb16-22"><a href="#cb16-22"></a>        <span class="st">"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"</span>,</span>
<span id="cb16-23"><a href="#cb16-23"></a>        <span class="st">"pip install git+https://github.com/huggingface/transformers"</span>,</span>
<span id="cb16-24"><a href="#cb16-24"></a>        <span class="st">"pip install accelerate"</span>,</span>
<span id="cb16-25"><a href="#cb16-25"></a>        <span class="st">"pip install git+https://github.com/illuin-tech/colpali.git"</span>,</span>
<span id="cb16-26"><a href="#cb16-26"></a>        <span class="st">"pip install requests pdf2image PyPDF2"</span>,</span>
<span id="cb16-27"><a href="#cb16-27"></a>        <span class="st">"pip install python-dotenv"</span>,</span>
<span id="cb16-28"><a href="#cb16-28"></a>        <span class="ss">f'huggingface-cli login --token </span><span class="sc">{</span>os<span class="sc">.</span>environ[<span class="st">"HUGGING_FACE_ACCESS_TOKEN"</span>]<span class="sc">}</span><span class="ss">'</span>,</span>
<span id="cb16-29"><a href="#cb16-29"></a>    )</span>
<span id="cb16-30"><a href="#cb16-30"></a>    .run_commands(<span class="st">"pip install flash-attn --no-build-isolation"</span>)</span>
<span id="cb16-31"><a href="#cb16-31"></a>)</span>
<span id="cb16-32"><a href="#cb16-32"></a></span>
<span id="cb16-33"><a href="#cb16-33"></a></span>
<span id="cb16-34"><a href="#cb16-34"></a><span class="at">@app.cls</span>(image<span class="op">=</span>image, secrets<span class="op">=</span>[modal.Secret.from_dotenv()], gpu<span class="op">=</span>modal.gpu.A100(count<span class="op">=</span><span class="dv">1</span>, size<span class="op">=</span><span class="st">"80GB"</span>), cpu<span class="op">=</span><span class="dv">4</span>, timeout<span class="op">=</span><span class="dv">600</span>, container_idle_timeout<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb16-35"><a href="#cb16-35"></a><span class="kw">class</span> Model:</span>
<span id="cb16-36"><a href="#cb16-36"></a>    <span class="at">@build</span>()</span>
<span id="cb16-37"><a href="#cb16-37"></a>    <span class="at">@enter</span>()</span>
<span id="cb16-38"><a href="#cb16-38"></a>    <span class="kw">def</span> setup(<span class="va">self</span>):</span>
<span id="cb16-39"><a href="#cb16-39"></a>        <span class="im">from</span> typing <span class="im">import</span> cast</span>
<span id="cb16-40"><a href="#cb16-40"></a></span>
<span id="cb16-41"><a href="#cb16-41"></a>        <span class="im">import</span> torch</span>
<span id="cb16-42"><a href="#cb16-42"></a></span>
<span id="cb16-43"><a href="#cb16-43"></a>        <span class="im">from</span> colpali_engine.models <span class="im">import</span> ColPali</span>
<span id="cb16-44"><a href="#cb16-44"></a>        <span class="im">from</span> colpali_engine.models.paligemma.colpali.processing_colpali <span class="im">import</span> ColPaliProcessor</span>
<span id="cb16-45"><a href="#cb16-45"></a>        <span class="im">from</span> colpali_engine.utils.processing_utils <span class="im">import</span> BaseVisualRetrieverProcessor</span>
<span id="cb16-46"><a href="#cb16-46"></a></span>
<span id="cb16-47"><a href="#cb16-47"></a>        <span class="co"># Define adapter name</span></span>
<span id="cb16-48"><a href="#cb16-48"></a>        base_model_name <span class="op">=</span> <span class="st">"vidore/colpaligemma-3b-pt-448-base"</span></span>
<span id="cb16-49"><a href="#cb16-49"></a>        adapter_name <span class="op">=</span> <span class="st">"vidore/colpali-v1.2"</span></span>
<span id="cb16-50"><a href="#cb16-50"></a></span>
<span id="cb16-51"><a href="#cb16-51"></a>        <span class="co"># Load model</span></span>
<span id="cb16-52"><a href="#cb16-52"></a>        <span class="va">self</span>.model <span class="op">=</span> ColPali.from_pretrained(</span>
<span id="cb16-53"><a href="#cb16-53"></a>            base_model_name,</span>
<span id="cb16-54"><a href="#cb16-54"></a>            torch_dtype<span class="op">=</span>torch.bfloat16,</span>
<span id="cb16-55"><a href="#cb16-55"></a>            device_map<span class="op">=</span><span class="st">"cuda"</span>,</span>
<span id="cb16-56"><a href="#cb16-56"></a>        ).<span class="bu">eval</span>()</span>
<span id="cb16-57"><a href="#cb16-57"></a>        <span class="va">self</span>.model.load_adapter(adapter_name)</span>
<span id="cb16-58"><a href="#cb16-58"></a>        <span class="va">self</span>.processor <span class="op">=</span> cast(ColPaliProcessor, ColPaliProcessor.from_pretrained(<span class="st">"google/paligemma-3b-mix-448"</span>))</span>
<span id="cb16-59"><a href="#cb16-59"></a></span>
<span id="cb16-60"><a href="#cb16-60"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="bu">isinstance</span>(<span class="va">self</span>.processor, BaseVisualRetrieverProcessor):</span>
<span id="cb16-61"><a href="#cb16-61"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Processor should be a BaseVisualRetrieverProcessor"</span>)</span>
<span id="cb16-62"><a href="#cb16-62"></a></span>
<span id="cb16-63"><a href="#cb16-63"></a>    <span class="kw">def</span> pdf_to_images(<span class="va">self</span>, pdf_url):</span>
<span id="cb16-64"><a href="#cb16-64"></a>        <span class="co"># if pdf_url in PDF_IMAGES:</span></span>
<span id="cb16-65"><a href="#cb16-65"></a>        <span class="co">#     return PDF_IMAGES[pdf_url]</span></span>
<span id="cb16-66"><a href="#cb16-66"></a>        <span class="co"># Function to download and convert PDF url to images</span></span>
<span id="cb16-67"><a href="#cb16-67"></a>        <span class="im">import</span> requests</span>
<span id="cb16-68"><a href="#cb16-68"></a>        <span class="im">from</span> io <span class="im">import</span> BytesIO</span>
<span id="cb16-69"><a href="#cb16-69"></a>        <span class="im">from</span> pdf2image <span class="im">import</span> convert_from_bytes</span>
<span id="cb16-70"><a href="#cb16-70"></a></span>
<span id="cb16-71"><a href="#cb16-71"></a>        <span class="co"># Step 1: Download the PDF from the provided URL</span></span>
<span id="cb16-72"><a href="#cb16-72"></a>        response <span class="op">=</span> requests.get(pdf_url)</span>
<span id="cb16-73"><a href="#cb16-73"></a>        <span class="cf">if</span> response.status_code <span class="op">!=</span> <span class="dv">200</span>:</span>
<span id="cb16-74"><a href="#cb16-74"></a>            <span class="cf">raise</span> <span class="pp">Exception</span>(<span class="ss">f"Failed to download PDF from </span><span class="sc">{</span>pdf_url<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-75"><a href="#cb16-75"></a></span>
<span id="cb16-76"><a href="#cb16-76"></a>        <span class="co"># Step 2: Convert the PDF into images (in-memory)</span></span>
<span id="cb16-77"><a href="#cb16-77"></a>        pdf_bytes <span class="op">=</span> BytesIO(response.content)</span>
<span id="cb16-78"><a href="#cb16-78"></a>        images <span class="op">=</span> convert_from_bytes(pdf_bytes.read())</span>
<span id="cb16-79"><a href="#cb16-79"></a></span>
<span id="cb16-80"><a href="#cb16-80"></a>        <span class="co"># Step 3: Return the list of PIL images</span></span>
<span id="cb16-81"><a href="#cb16-81"></a>        <span class="cf">return</span> images</span>
<span id="cb16-82"><a href="#cb16-82"></a></span>
<span id="cb16-83"><a href="#cb16-83"></a>    <span class="kw">def</span> pil_image_to_data_url(<span class="va">self</span>, pil_image):</span>
<span id="cb16-84"><a href="#cb16-84"></a>        <span class="im">import</span> base64</span>
<span id="cb16-85"><a href="#cb16-85"></a>        <span class="im">from</span> io <span class="im">import</span> BytesIO</span>
<span id="cb16-86"><a href="#cb16-86"></a></span>
<span id="cb16-87"><a href="#cb16-87"></a>        <span class="co"># Convert PIL Image to bytes</span></span>
<span id="cb16-88"><a href="#cb16-88"></a>        buffered <span class="op">=</span> BytesIO()</span>
<span id="cb16-89"><a href="#cb16-89"></a>        pil_image.save(buffered, <span class="bu">format</span><span class="op">=</span><span class="st">"PNG"</span>)</span>
<span id="cb16-90"><a href="#cb16-90"></a></span>
<span id="cb16-91"><a href="#cb16-91"></a>        <span class="co"># Encode to base64</span></span>
<span id="cb16-92"><a href="#cb16-92"></a>        img_str <span class="op">=</span> base64.b64encode(buffered.getvalue()).decode()</span>
<span id="cb16-93"><a href="#cb16-93"></a></span>
<span id="cb16-94"><a href="#cb16-94"></a>        <span class="co"># Format as data URL</span></span>
<span id="cb16-95"><a href="#cb16-95"></a>        <span class="cf">return</span> <span class="ss">f"data:image/png;base64,</span><span class="sc">{</span>img_str<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb16-96"><a href="#cb16-96"></a></span>
<span id="cb16-97"><a href="#cb16-97"></a>    <span class="kw">def</span> answer_questions_with_image_context(<span class="va">self</span>, images, queries, idxs_top_k):</span>
<span id="cb16-98"><a href="#cb16-98"></a>        messages_list <span class="op">=</span> []</span>
<span id="cb16-99"><a href="#cb16-99"></a></span>
<span id="cb16-100"><a href="#cb16-100"></a>        <span class="cf">for</span> i, idxs <span class="kw">in</span> <span class="bu">enumerate</span>(idxs_top_k):</span>
<span id="cb16-101"><a href="#cb16-101"></a>            query <span class="op">=</span> queries[i]</span>
<span id="cb16-102"><a href="#cb16-102"></a>            content <span class="op">=</span> [{<span class="st">"type"</span>: <span class="st">"image"</span>, <span class="st">"image"</span>: <span class="va">self</span>.pil_image_to_data_url(images[idx])} <span class="cf">for</span> idx <span class="kw">in</span> idxs]</span>
<span id="cb16-103"><a href="#cb16-103"></a>            content.append({<span class="st">"type"</span>: <span class="st">"text"</span>, <span class="st">"text"</span>: <span class="ss">f"Using the provided image(s) as context, answer the following question.</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>query<span class="sc">}</span><span class="ss">"</span>})</span>
<span id="cb16-104"><a href="#cb16-104"></a>            messages_list.append([{<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: content}])</span>
<span id="cb16-105"><a href="#cb16-105"></a></span>
<span id="cb16-106"><a href="#cb16-106"></a>        f <span class="op">=</span> modal.Function.lookup(<span class="st">"qwen2_vl_78_Instruct"</span>, <span class="st">"Model.f"</span>)</span>
<span id="cb16-107"><a href="#cb16-107"></a>        <span class="cf">return</span> f.remote(messages_list)</span>
<span id="cb16-108"><a href="#cb16-108"></a></span>
<span id="cb16-109"><a href="#cb16-109"></a>    <span class="at">@modal.method</span>()</span>
<span id="cb16-110"><a href="#cb16-110"></a>    <span class="kw">def</span> f(<span class="va">self</span>, pdf_url: <span class="bu">str</span>, queries: <span class="bu">list</span>[<span class="bu">str</span>], top_k<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb16-111"><a href="#cb16-111"></a>        <span class="im">from</span> typing <span class="im">import</span> List</span>
<span id="cb16-112"><a href="#cb16-112"></a>        <span class="im">import</span> torch</span>
<span id="cb16-113"><a href="#cb16-113"></a>        <span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb16-114"><a href="#cb16-114"></a>        <span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb16-115"><a href="#cb16-115"></a></span>
<span id="cb16-116"><a href="#cb16-116"></a>        <span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-117"><a href="#cb16-117"></a></span>
<span id="cb16-118"><a href="#cb16-118"></a>        <span class="im">from</span> colpali_engine.utils.torch_utils <span class="im">import</span> ListDataset</span>
<span id="cb16-119"><a href="#cb16-119"></a></span>
<span id="cb16-120"><a href="#cb16-120"></a>        images <span class="op">=</span> <span class="va">self</span>.pdf_to_images(pdf_url)</span>
<span id="cb16-121"><a href="#cb16-121"></a>        batch_size <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb16-122"><a href="#cb16-122"></a>        <span class="co"># Run inference - docs</span></span>
<span id="cb16-123"><a href="#cb16-123"></a>        dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb16-124"><a href="#cb16-124"></a>            dataset<span class="op">=</span>ListDataset[<span class="bu">str</span>](images),</span>
<span id="cb16-125"><a href="#cb16-125"></a>            batch_size<span class="op">=</span>batch_size,</span>
<span id="cb16-126"><a href="#cb16-126"></a>            shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb16-127"><a href="#cb16-127"></a>            collate_fn<span class="op">=</span><span class="kw">lambda</span> x: <span class="va">self</span>.processor.process_images(x),</span>
<span id="cb16-128"><a href="#cb16-128"></a>        )</span>
<span id="cb16-129"><a href="#cb16-129"></a>        ds: List[torch.Tensor] <span class="op">=</span> []</span>
<span id="cb16-130"><a href="#cb16-130"></a>        <span class="cf">for</span> batch_doc <span class="kw">in</span> tqdm(dataloader):</span>
<span id="cb16-131"><a href="#cb16-131"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb16-132"><a href="#cb16-132"></a>                batch_doc <span class="op">=</span> {k: v.to(<span class="va">self</span>.model.device) <span class="cf">for</span> k, v <span class="kw">in</span> batch_doc.items()}</span>
<span id="cb16-133"><a href="#cb16-133"></a>                embeddings_doc <span class="op">=</span> <span class="va">self</span>.model(<span class="op">**</span>batch_doc)</span>
<span id="cb16-134"><a href="#cb16-134"></a>            ds.extend(<span class="bu">list</span>(torch.unbind(embeddings_doc.to(<span class="st">"cpu"</span>))))</span>
<span id="cb16-135"><a href="#cb16-135"></a></span>
<span id="cb16-136"><a href="#cb16-136"></a>        <span class="co"># Run inference - queries</span></span>
<span id="cb16-137"><a href="#cb16-137"></a>        dataloader <span class="op">=</span> DataLoader(</span>
<span id="cb16-138"><a href="#cb16-138"></a>            dataset<span class="op">=</span>ListDataset[<span class="bu">str</span>](queries),</span>
<span id="cb16-139"><a href="#cb16-139"></a>            batch_size<span class="op">=</span>batch_size,</span>
<span id="cb16-140"><a href="#cb16-140"></a>            shuffle<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb16-141"><a href="#cb16-141"></a>            collate_fn<span class="op">=</span><span class="kw">lambda</span> x: <span class="va">self</span>.processor.process_queries(x),</span>
<span id="cb16-142"><a href="#cb16-142"></a>        )</span>
<span id="cb16-143"><a href="#cb16-143"></a>        qs: List[torch.Tensor] <span class="op">=</span> []</span>
<span id="cb16-144"><a href="#cb16-144"></a>        <span class="cf">for</span> batch_query <span class="kw">in</span> dataloader:</span>
<span id="cb16-145"><a href="#cb16-145"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb16-146"><a href="#cb16-146"></a>                batch_query <span class="op">=</span> {k: v.to(<span class="va">self</span>.model.device) <span class="cf">for</span> k, v <span class="kw">in</span> batch_query.items()}</span>
<span id="cb16-147"><a href="#cb16-147"></a>                embeddings_query <span class="op">=</span> <span class="va">self</span>.model(<span class="op">**</span>batch_query)</span>
<span id="cb16-148"><a href="#cb16-148"></a>            qs.extend(<span class="bu">list</span>(torch.unbind(embeddings_query.to(<span class="st">"cpu"</span>))))</span>
<span id="cb16-149"><a href="#cb16-149"></a></span>
<span id="cb16-150"><a href="#cb16-150"></a>        <span class="co"># Run scoring</span></span>
<span id="cb16-151"><a href="#cb16-151"></a>        scores <span class="op">=</span> <span class="va">self</span>.processor.score(qs, ds).cpu().numpy()</span>
<span id="cb16-152"><a href="#cb16-152"></a>        <span class="co"># The top k indices for each query</span></span>
<span id="cb16-153"><a href="#cb16-153"></a>        idxs_top_k <span class="op">=</span> np.argsort(scores, axis<span class="op">=-</span><span class="dv">1</span>)[:, <span class="op">-</span>top_k:][:, ::<span class="op">-</span><span class="dv">1</span>].tolist()</span>
<span id="cb16-154"><a href="#cb16-154"></a></span>
<span id="cb16-155"><a href="#cb16-155"></a>        results <span class="op">=</span> []</span>
<span id="cb16-156"><a href="#cb16-156"></a>        answers <span class="op">=</span> <span class="va">self</span>.answer_questions_with_image_context(images, queries, idxs_top_k)</span>
<span id="cb16-157"><a href="#cb16-157"></a>        <span class="cf">for</span> question, idxs, answer <span class="kw">in</span> <span class="bu">zip</span>(queries, idxs_top_k, answers):</span>
<span id="cb16-158"><a href="#cb16-158"></a>            <span class="bu">print</span>(<span class="ss">f"QUESTION: </span><span class="sc">{</span>question<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-159"><a href="#cb16-159"></a>            <span class="bu">print</span>(<span class="ss">f"PDF PAGES USED FOR CONTEXT: </span><span class="sc">{</span>idxs<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-160"><a href="#cb16-160"></a>            <span class="bu">print</span>(<span class="ss">f"ANSWER: </span><span class="sc">{</span>answer<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">"</span>)</span>
<span id="cb16-161"><a href="#cb16-161"></a>            results.append({<span class="st">"question"</span>: question, <span class="st">"answer"</span>: answer, <span class="st">"pages"</span>: idxs})</span>
<span id="cb16-162"><a href="#cb16-162"></a>        <span class="cf">return</span> results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</section>
<section id="colpali-inference-demo" class="slide level2">
<h2>ColPali Inference Demo</h2>
<div class="sourceCode" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="co"># modal deploy qwen2_vl_78_Instruct.py</span></span>
<span id="cb17-2"><a href="#cb17-2"></a><span class="co"># modal deploy colpali.py</span></span>
<span id="cb17-3"><a href="#cb17-3"></a><span class="im">import</span> os</span>
<span id="cb17-4"><a href="#cb17-4"></a><span class="im">import</span> dotenv</span>
<span id="cb17-5"><a href="#cb17-5"></a>dotenv.load_dotenv()</span>
<span id="cb17-6"><a href="#cb17-6"></a><span class="im">import</span> modal</span>
<span id="cb17-7"><a href="#cb17-7"></a></span>
<span id="cb17-8"><a href="#cb17-8"></a>f <span class="op">=</span> modal.Function.lookup(<span class="st">"colpali"</span>, <span class="st">"Model.f"</span>)</span>
<span id="cb17-9"><a href="#cb17-9"></a></span>
<span id="cb17-10"><a href="#cb17-10"></a>f.remote(</span>
<span id="cb17-11"><a href="#cb17-11"></a>    <span class="st">"https://arxiv.org/pdf/1706.03762"</span>,  <span class="co"># Self Attention Paper: Attention is all you need</span></span>
<span id="cb17-12"><a href="#cb17-12"></a>    [</span>
<span id="cb17-13"><a href="#cb17-13"></a>        <span class="st">"Who are the authors of the paper?"</span>,</span>
<span id="cb17-14"><a href="#cb17-14"></a>        <span class="st">"What is the model architecture for the transformer?"</span>,</span>
<span id="cb17-15"><a href="#cb17-15"></a>        <span class="st">"What is the equation for Scaled Dot-Product Attention?"</span>,</span>
<span id="cb17-16"><a href="#cb17-16"></a>        <span class="st">"What Optimizer was used for training?"</span>,</span>
<span id="cb17-17"><a href="#cb17-17"></a>        <span class="st">"What was the value used for label smoothing?"</span>,</span>
<span id="cb17-18"><a href="#cb17-18"></a>    ],</span>
<span id="cb17-19"><a href="#cb17-19"></a>)</span>
<span id="cb17-20"><a href="#cb17-20"></a></span>
<span id="cb17-21"><a href="#cb17-21"></a>f.remote(</span>
<span id="cb17-22"><a href="#cb17-22"></a>    <span class="st">"https://arxiv.org/pdf/2407.01449"</span>,  <span class="co"># ColPali: Efficient Document Retrieval with Vision Language Models</span></span>
<span id="cb17-23"><a href="#cb17-23"></a>    [</span>
<span id="cb17-24"><a href="#cb17-24"></a>        <span class="st">"What was the size of the training dataset?"</span>,</span>
<span id="cb17-25"><a href="#cb17-25"></a>        <span class="st">"Can you summarize the abstract for me please?"</span>,</span>
<span id="cb17-26"><a href="#cb17-26"></a>        <span class="st">"What is the main contribution of this paper?"</span>,</span>
<span id="cb17-27"><a href="#cb17-27"></a>    ],</span>
<span id="cb17-28"><a href="#cb17-28"></a>)</span>
<span id="cb17-29"><a href="#cb17-29"></a></span>
<span id="cb17-30"><a href="#cb17-30"></a>s3_bucket <span class="op">=</span> os.environ[<span class="st">"S3_BUCKET"</span>]  <span class="co"># where my images are hosted</span></span>
<span id="cb17-31"><a href="#cb17-31"></a>s3_prefix <span class="op">=</span> os.environ[<span class="st">"S3_PREFIX"</span>]  <span class="co"># where my images are hosted</span></span>
<span id="cb17-32"><a href="#cb17-32"></a>f.remote(</span>
<span id="cb17-33"><a href="#cb17-33"></a>    <span class="ss">f"https://</span><span class="sc">{</span>s3_bucket<span class="sc">}</span><span class="ss">.s3.amazonaws.com/</span><span class="sc">{</span>s3_prefix<span class="sc">}</span><span class="ss">merged.pdf"</span>,</span>
<span id="cb17-34"><a href="#cb17-34"></a>    [</span>
<span id="cb17-35"><a href="#cb17-35"></a>        <span class="st">"How is the average engagement rate calculated on LinkedIn?"</span>,</span>
<span id="cb17-36"><a href="#cb17-36"></a>        <span class="st">"How is total engagements calculated on Pinterest?"</span>,</span>
<span id="cb17-37"><a href="#cb17-37"></a>        <span class="st">"How is total engagements calculated on Instagram?"</span>,</span>
<span id="cb17-38"><a href="#cb17-38"></a>        <span class="st">"What is the entertainment score and how is it calculated?"</span>,</span>
<span id="cb17-39"><a href="#cb17-39"></a>        <span class="st">"What was the change in total followers on Instagram for Nike?"</span>,</span>
<span id="cb17-40"><a href="#cb17-40"></a>        <span class="st">"What day was there a spike in avg engagement rate for Spotify on Instagram?"</span>,</span>
<span id="cb17-41"><a href="#cb17-41"></a>        <span class="st">"What differences can you call out between the top and lowest performing posts for NBA on Instagram in terms of visual content?"</span>,</span>
<span id="cb17-42"><a href="#cb17-42"></a>        <span class="st">"What was the top performing post by Nike on Instagram about on March 25?"</span>,</span>
<span id="cb17-43"><a href="#cb17-43"></a>        <span class="st">"what was the top performing post for BMW?"</span>,</span>
<span id="cb17-44"><a href="#cb17-44"></a>        <span class="st">"What was the lady eating in the top post for All Recipes on Pinterest? Where was it bought from?"</span>,</span>
<span id="cb17-45"><a href="#cb17-45"></a>    ],</span>
<span id="cb17-46"><a href="#cb17-46"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="colpali-inference-demo-1" class="slide level2">
<h2>ColPali Inference Demo</h2>
<div class="sourceCode" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a>[{<span class="st">'question'</span>: <span class="st">'How is the average engagement rate calculated on LinkedIn?'</span>,</span>
<span id="cb18-2"><a href="#cb18-2"></a>  <span class="st">'answer'</span>: [<span class="st">'The average engagement rate on LinkedIn is calculated as (Clicks + Reactions + Comments + Reposts) / Impressions.'</span>],</span>
<span id="cb18-3"><a href="#cb18-3"></a>  <span class="st">'pages'</span>: [<span class="dv">49</span>, <span class="dv">41</span>, <span class="dv">4</span>]},</span>
<span id="cb18-4"><a href="#cb18-4"></a> {<span class="st">'question'</span>: <span class="st">'How is total engagements calculated on Pinterest?'</span>,</span>
<span id="cb18-5"><a href="#cb18-5"></a>  <span class="st">'answer'</span>: [<span class="st">'Total engagements on Pinterest is calculated as the sum of outbound clicks, saves, and pin clicks.'</span>],</span>
<span id="cb18-6"><a href="#cb18-6"></a>  <span class="st">'pages'</span>: [<span class="dv">39</span>, <span class="dv">21</span>, <span class="dv">20</span>]},</span>
<span id="cb18-7"><a href="#cb18-7"></a> {<span class="st">'question'</span>: <span class="st">'How is total engagements calculated on Instagram?'</span>,</span>
<span id="cb18-8"><a href="#cb18-8"></a>  <span class="st">'answer'</span>: [<span class="st">'Total engagements on Instagram is calculated as the sum of likes, saves, comments, and shares minus the sum of dislikes, unlikes, and deleted comments.'</span>],</span>
<span id="cb18-9"><a href="#cb18-9"></a>  <span class="st">'pages'</span>: [<span class="dv">20</span>, <span class="dv">21</span>, <span class="dv">48</span>]},</span>
<span id="cb18-10"><a href="#cb18-10"></a> {<span class="st">'question'</span>: <span class="st">'What is the entertainment score and how is it calculated?'</span>,</span>
<span id="cb18-11"><a href="#cb18-11"></a>  <span class="st">'answer'</span>: [<span class="st">'The entertainment score is a measure of how entertained your audience was by your organic and promoted TikTok videos. It is scored on a scale from 0 to 10 and generated 48 hours after publishing. The calculation for entertainment score is proprietary, but it is based on the number of engagements your video receives.'</span>],</span>
<span id="cb18-12"><a href="#cb18-12"></a>  <span class="st">'pages'</span>: [<span class="dv">27</span>, <span class="dv">45</span>, <span class="dv">56</span>]},</span>
<span id="cb18-13"><a href="#cb18-13"></a> {<span class="st">'question'</span>: <span class="st">'What was the change in total followers on Instagram for Nike?'</span>,</span>
<span id="cb18-14"><a href="#cb18-14"></a>  <span class="st">'answer'</span>: [<span class="st">'The change in total followers on Instagram for Nike was an increase of 0.03%.'</span>],</span>
<span id="cb18-15"><a href="#cb18-15"></a>  <span class="st">'pages'</span>: [<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>]},</span>
<span id="cb18-16"><a href="#cb18-16"></a> {<span class="st">'question'</span>: <span class="st">'What day was there a spike in avg engagement rate for Spotify on Instagram?'</span>,</span>
<span id="cb18-17"><a href="#cb18-17"></a>  <span class="st">'answer'</span>: [<span class="st">'There was a spike in the average engagement rate for Spotify on Instagram on March 10th.'</span>],</span>
<span id="cb18-18"><a href="#cb18-18"></a>  <span class="st">'pages'</span>: [<span class="dv">4</span>, <span class="dv">14</span>, <span class="dv">22</span>]},</span>
<span id="cb18-19"><a href="#cb18-19"></a> {<span class="st">'question'</span>: <span class="st">'What differences can you call out between the top and lowest performing posts for NBA on Instagram in terms of visual content?'</span>,</span>
<span id="cb18-20"><a href="#cb18-20"></a>  <span class="st">'answer'</span>: [<span class="st">'The top performing posts for NBA on Instagram feature dynamic and action-packed visuals, such as game highlights and player moments, while the lowest performing posts tend to have more static and less engaging content, such as text-based posts or less visually appealing images.'</span>],</span>
<span id="cb18-21"><a href="#cb18-21"></a>  <span class="st">'pages'</span>: [<span class="dv">10</span>, <span class="dv">9</span>, <span class="dv">11</span>]},</span>
<span id="cb18-22"><a href="#cb18-22"></a> {<span class="st">'question'</span>: <span class="st">'What was the top performing post by Nike on Instagram about on March 25?'</span>,</span>
<span id="cb18-23"><a href="#cb18-23"></a>  <span class="st">'answer'</span>: [<span class="st">'The top performing post by Nike on Instagram on March 25 was a video featuring Drake, with 651,373 likes.'</span>],</span>
<span id="cb18-24"><a href="#cb18-24"></a>  <span class="st">'pages'</span>: [<span class="dv">12</span>, <span class="dv">11</span>, <span class="dv">0</span>]},</span>
<span id="cb18-25"><a href="#cb18-25"></a> {<span class="st">'question'</span>: <span class="st">'what was the top performing post for BMW?'</span>,</span>
<span id="cb18-26"><a href="#cb18-26"></a>  <span class="st">'answer'</span>: [<span class="st">'The top performing post for BMW was posted on March 14 at 11:00 AM.'</span>],</span>
<span id="cb18-27"><a href="#cb18-27"></a>  <span class="st">'pages'</span>: [<span class="dv">7</span>, <span class="dv">6</span>, <span class="dv">0</span>]},</span>
<span id="cb18-28"><a href="#cb18-28"></a> {<span class="st">'question'</span>: <span class="st">'What was the lady eating in the top post for All Recipes on Pinterest? Where was it bought from?'</span>,</span>
<span id="cb18-29"><a href="#cb18-29"></a>  <span class="st">'answer'</span>: [<span class="st">"The lady in the top post for All Recipes on Pinterest was eating a McDonald's sandwich."</span>],</span>
<span id="cb18-30"><a href="#cb18-30"></a>  <span class="st">'pages'</span>: [<span class="dv">18</span>, <span class="dv">21</span>, <span class="dv">39</span>]}]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section></section>
<section id="conclusion" class="title-slide slide level1 center">
<h1>Conclusion</h1>
<ul>
<li>I only went over a fraction of what Modal can do. I am still learning about all of its features.</li>
<li>If you have any questions, please reach out.</li>
<li>Thank you for your time and attention.</li>
</ul>

<div class="footer footer-default">

</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>