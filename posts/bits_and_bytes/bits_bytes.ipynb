{
 "cells": [
  {
   "cell_type": "raw",
   "id": "580eeb7aecf11ffe",
   "metadata": {
    "collapsed": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Memory Usage for Quantized LLMS\n",
    "author: Chris Levy\n",
    "date: '2024-06-22'\n",
    "date-modified: '2024-06-22'\n",
    "image: bits_and_bytes.jpg\n",
    "toc: true\n",
    "description: In this blog post, I share my learnings about memory usage in quantized LLMs from a recent conference, covering bits and bytes, memory usage during inference and training, and exploring techniques like quantization and PEFT methods such as QLORA to reduce memory footprint while fine-tuning models like llama3-8B.\n",
    "tags:\n",
    "  - LLMs\n",
    "  - Quantization\n",
    "  - Bits-and-Bytes\n",
    "  - Axolotl\n",
    "  - PEFT\n",
    "  - QLORA\n",
    "format:\n",
    "  html:\n",
    "    code-fold: show\n",
    "    page-layout: full\n",
    "include-in-header:\n",
    "  - text: |\n",
    "      <style>\n",
    "      .cell-output-stdout code {\n",
    "        word-break: break-wor !important;\n",
    "        white-space: pre-wrap !important;\n",
    "      }\n",
    "      </style>\n",
    "bibliography: ../../bibliography.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a4f19d10d3e14",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Intro\n",
    "\n",
    "In this blog post I will be writing about some of my high level learnings from a recent [LLM conference](https://maven.com/parlance-labs/fine-tuning?utm_campaign=2848bd&utm_medium=partner&utm_source=instructor0) I took part in. \n",
    "These are some notes for myself so I don't forget. These concepts are basic and high level but are details I was missing before when thinking about fine-tuning LLMs. Most of the details here were inspired by one of the conference \n",
    "speakers: [Jonathan Whitaker's](https://x.com/johnowhitaker) ---> [talk](https://x.com/HamelHusain/status/1798353336145674483).\n",
    " \n",
    "# Bits and Byte\n",
    "\n",
    "You have probably heard the saying, \"In the computer it's all 0's and 1's\". Well, those 0's and 1's are called **bits**. A **bit** is the smallest\n",
    "unit of storage and simply stores a 0 or a 1. A **byte** is a group of 8 **bits** together.\n",
    "\n",
    "- 1 **byte** **=** 8 **bits**.\n",
    "\n",
    "All storage is measured in **bytes**.\n",
    "\n",
    "| Unit      | Abbreviation | Approximate Size        |\n",
    "|-----------|--------------|-------------------------|\n",
    "| Kilobyte  | KB           | about 1 thousand bytes  |\n",
    "| Megabyte  | MB           | about 1 million bytes   |\n",
    "| Gigabyte  | GB           | about 1 billion bytes   |\n",
    "| Terabyte  | TB           | about 1 trillion bytes  |\n",
    "\n",
    "# Memory Usage During Inference\n",
    "\n",
    "I'm running this code from a local Jupyter notebook in Pycharm connected to a A6000 running on [JarvisLabs](https://jarvislabs.ai/). I used the axolotl template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe38309bb39b680c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:02.433261Z",
     "start_time": "2024-06-22T20:45:00.132542Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 0.00GB\n",
      "Max memory reserved: 0.00GB\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "\n",
    "def print_memory_stats():\n",
    "    \"\"\"Print two different measures of GPU memory usage\"\"\"\n",
    "    print(f\"Max memory allocated: {torch.cuda.max_memory_allocated(device)/1e9:.2f}GB\")\n",
    "    # reserved (aka 'max_memory_cached') is ~the allocated memory plus pre-cached memory\n",
    "    print(f\"Max memory reserved: {torch.cuda.max_memory_reserved(device)/1e9:.2f}GB\")\n",
    "\n",
    "\n",
    "print_memory_stats()\n",
    "\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b653da7f16bec613",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's load the model `TinyLlama/TinyLlama-1.1B-Chat-v1.0`.\n",
    "By default, it will load in `float32`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21e2099adab2e91f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:06.542362Z",
     "start_time": "2024-06-22T20:45:02.440693Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# | output: false\n",
    "model_ckpt = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_ckpt, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f7a66d410bd5bb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can check the `dtype` of the parameters and indeed see that they are stored in `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d9c3b7752c5aa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:06.552361Z",
     "start_time": "2024-06-22T20:45:06.546597Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{torch.float32}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | warning: false\n",
    "set([(x.dtype) for x in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f32cb1d6bc3f5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since the parameters are in `float32`, we can estimate that each parameter will takes up 32/8=4 bytes of memory.\n",
    "For a 1.1B parameter model that is 4.4gb. Let's see if our rough back of the napkin calculation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15b746d83556cb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:07.049794Z",
     "start_time": "2024-06-22T20:45:06.557993Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 22 20:45:06 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA RTX A6000               Off | 00000000:1B:00.0 Off |                  Off |\r\n",
      "| 30%   34C    P2              67W / 300W |   4475MiB / 49140MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7797e90d7893f406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:07.067065Z",
     "start_time": "2024-06-22T20:45:07.061728Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 4.40GB\n",
      "Max memory reserved: 4.40GB\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "print_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eff909b29906a73",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Yes, that's what we thought.\n",
    "\n",
    "Let's run some inference with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccc99f33e6ed4e7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:08.031364Z",
     "start_time": "2024-06-22T20:45:07.076516Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "How many bytes are in one gigabyte? \n",
      "<|assistant|>\n",
      "Yes, there are 1,000,000,000 bytes in a gigabyte (GB).\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "def inference(messages):\n",
    "    tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=tokenized_chat.to(\"cuda\"), max_new_tokens=128, do_sample=False)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])\n",
    "\n",
    "\n",
    "inference([{\"role\": \"user\", \"content\": \"How many bytes are in one gigabyte?\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadc6bebfaa01444",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's load the model in a lower precision.\n",
    "The model config points to what precision to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9492d303cd58b29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:08.033653Z",
     "start_time": "2024-06-22T20:45:08.030414Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | warning: false\n",
    "model.config.torch_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5147a6aa634175e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:08.801858Z",
     "start_time": "2024-06-22T20:45:08.049032Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 0.01GB\n",
      "Max memory reserved: 0.02GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 22 20:45:08 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA RTX A6000               Off | 00000000:1B:00.0 Off |                  Off |\r\n",
      "| 30%   35C    P2              91W / 300W |    351MiB / 49140MiB |     51%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "del model\n",
    "cleanup()\n",
    "print_memory_stats()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7cad3a66839d93",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's load the model in `bfloat16`. Estimating that each parameter will use 16/8=2 bytes of memory.\n",
    "For the same model, it should use roughly half the memory as before, 2.2GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "907717a07238236",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:10.723238Z",
     "start_time": "2024-06-22T20:45:08.810116Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# | output: false\n",
    "model = AutoModelForCausalLM.from_pretrained(model_ckpt, torch_dtype=torch.bfloat16, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cfb4b56fa58e37a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:10.732073Z",
     "start_time": "2024-06-22T20:45:10.729106Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 2.21GB\n",
      "Max memory reserved: 2.32GB\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "print_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f6d4925b1eeaafd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:11.005948Z",
     "start_time": "2024-06-22T20:45:10.742242Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{torch.bfloat16}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | warning: false\n",
    "set([(x.dtype) for x in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70844652d91fd861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:11.623988Z",
     "start_time": "2024-06-22T20:45:10.982607Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "How many bytes are in one gigabyte? \n",
      "<|assistant|>\n",
      "Yes, there are 1,000,000,000 bytes in a gigabyte (GB).\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "inference([{\"role\": \"user\", \"content\": \"How many bytes are in one gigabyte?\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e58204c8dce86b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This is the exact same output we got before. Since most models are currently trained using `bfloat16`, there's no need to use full `float32` precision. In this example if we use float32, it won't improve inference results compared to `bfloat16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ced79341e973055b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:11.792492Z",
     "start_time": "2024-06-22T20:45:11.623182Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 0.01GB\n",
      "Max memory reserved: 0.02GB\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "del model\n",
    "cleanup()\n",
    "print_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5320c28b9128cd8f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's try loading a quantized model using  `BitsAndBytesConfig` to a load a model in 4bit precision.\n",
    "Note that the model weights are stored in 4bit precision but the computations are done in a higher precision.\n",
    "Here we are specifying that the dtype for computations is bf16. During inference, as well as training, \n",
    "the weights of the model are constantly being dequantized (from 4bit to bf16). This can be done for specific layers at a time\n",
    "during forward and backward passes, to keep memory requirements low. *The computation happens in a higher precision*.\n",
    "\n",
    "Here we specify that the model should be loaded in 4bit and the computations done in bf16.\n",
    "Since we are using 4bit we expect each parameter to use 4/8=0.5 bytes so we should be using less than a GB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "984f2d3573fbe7eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:14.967001Z",
     "start_time": "2024-06-22T20:45:11.790024Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: BNB_CUDA_VERSION=118 environment variable detected; loading libbitsandbytes_cuda118.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f359ae76e250f2f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:14.978117Z",
     "start_time": "2024-06-22T20:45:14.967953Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 0.84GB\n",
      "Max memory reserved: 0.89GB\n"
     ]
    }
   ],
   "source": [
    "print_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d4e947590142a3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we can expect the model inference results to be different, for the same query we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93477d7cb3925399",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:19.998713Z",
     "start_time": "2024-06-22T20:45:14.981205Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "How many bytes are in one gigabyte? \n",
      "<|assistant|>\n",
      "Yes, one gigabyte (GB) is equal to 1,073,741,824 bytes. A byte is a unit of information storage in the binary system, which is the basis for digital computing. In binary, each byte has a value of 10, with each bit representing a single binary digit. So, one gigabyte is equivalent to 1,073,741,824 bytes, which is approximately 1,000,000,000 bytes.\n"
     ]
    }
   ],
   "source": [
    "inference([{\"role\": \"user\", \"content\": \"How many bytes are in one gigabyte?\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c00f35056e106b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You can experiment with different types of quantization.  \n",
    "In this next example we load the model using: \n",
    "\n",
    "- NF4 quantization.\n",
    "- `bnb_4bit_use_double_quant` which uses a second quantization after the first one.\n",
    "- bfloat16 for computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48a48f05fbe39415",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:20.095933Z",
     "start_time": "2024-06-22T20:45:20.036875Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 0.01GB\n",
      "Max memory reserved: 0.02GB\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "cleanup()\n",
    "print_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a86681c7f60fbd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:21.943764Z",
     "start_time": "2024-06-22T20:45:20.073016Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# | warning: false\n",
    "nf4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", quantization_config=nf4_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7c3ec304fcf1c0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:21.951830Z",
     "start_time": "2024-06-22T20:45:21.949115Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 0.80GB\n",
      "Max memory reserved: 0.84GB\n"
     ]
    }
   ],
   "source": [
    "print_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18b1ab35d50c4774",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T20:45:25.470136Z",
     "start_time": "2024-06-22T20:45:21.962519Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "How many bytes are in one gigabyte? \n",
      "<|assistant|>\n",
      "Yes, I can provide you with the answer to your question. A gigabyte (GB) is a unit of measurement for data storage. It is equal to 1,000 bytes. So, 1 GB is equal to 1,000,000,000 bytes.\n"
     ]
    }
   ],
   "source": [
    "inference([{\"role\": \"user\", \"content\": \"How many bytes are in one gigabyte?\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78121756200d815",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "I really like the high level explanation of quantization from this [post](https://huggingface.co/blog/optimize-llm) by Patrick von Platen.\n",
    "In general, when running inference with quantized models the steps are:\n",
    "\n",
    "- Quantize all the weights of the model and load it (for example 4bit).\n",
    "- Pass through the input sequence in bf16.\n",
    "- Dynamically dequantize the weights to bf16 layer by layer during the forward pass\n",
    "- Quantize the weights back to 4bit after the computation\n",
    "\n",
    "So if we want to do $Y = X W$ where $W$ and $X$ are the weights and input sequence respectively, then for each matrix multiplication we do:\n",
    "\n",
    "$Y = X \\cdot \\text{dequantize}(W)$ ; $\\text{quantize}(W);$\n",
    "\n",
    "\n",
    "\n",
    "For this reason, inference is usually not faster when using quantized models. It's slower. It is good to remember that quantization is a tradeoff between memory usage and output quality, as well as possibly inference time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94086bb1c7451a97",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Memory Usage During Training\n",
    "\n",
    "This is not something I know a lot about at the moment, so I can't go into too much detail. But I can cover some high level basics that I learned recently.\n",
    "There are three main areas which contribute to the memory during training:\n",
    "\n",
    "\n",
    "- Model Parameters \n",
    "- Gradients\n",
    "- Optimizer State\n",
    "\n",
    "There are other things to consider such as the activations which tend to dominate the memory at larger batch sizes and context lengths. But let's ignore this to keep this simple and high level for now.\n",
    "\n",
    "Suppose we want to fine-tune llama3-8B in bfloat16 with the basic Adam optimizer.\n",
    "\n",
    "- Model Parameters: 2 bytes per parameter for 8B parameters is 16GB\n",
    "- Gradients: To store the gradients for each tunable parameter is 16GB\n",
    "- Optimizer State: needs 2X the size of the model, to store first/second moments, which is 32GB.\n",
    "\n",
    "So you would need at least **64GB** to fully fine-tune llama3-8B in bfloat16. What can we do to fine-tune the model with much less memory?\n",
    "\n",
    "First, we can quantize the model to 4bits. Then llama3-8B would take up 4GB of memory for the model parameters. That is a 4X reduction!\n",
    "But when training we don't quantize the trainable parameters or gradients because the training would not converge. Training still needs to happen higher precision.\n",
    "This is where PEFT methods come in handy, such as LORA and QLORA. Let's consider QLORA since we are discussing quantization.\n",
    "\n",
    "With QLORA we add a set of trainable adapters whose parameters take up a much smaller percentage of the total model parameters.\n",
    "We can freeze the entire quantized model and keep it in 4bit. We can store the corresponding gradients and optimizer state in higher precision. This is possible\n",
    "because we are only dealing with a very small percentage of the total model parameters that are trainable. \n",
    "\n",
    "In my last [blog post](https://drchrislevy.github.io/posts/fine_tune_jarvis/fine_tune_jarvis.html) I fine-tuned llama3-8B\n",
    "using the axolotl Library. It was configured to use QLORA with the model parameters in 4bit precision. It was using around 15GB of memory during [training](https://wandb.ai/christopherdavidlevy/synthetic-social-llama3?nw=nwuserchristopherdavidlevy). There were some spikes due to me loading a model in a different python session, so just ignore those.\n",
    "\n",
    "\n",
    "![](imgs/wandb-axolotl-gpu-usage.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12c78c055f3f2c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:03:12.701985Z",
     "start_time": "2024-06-22T21:03:12.500676Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 0.01GB\n",
      "Max memory reserved: 0.03GB\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "cleanup()\n",
    "print_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59655e036c60d457",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Inference with Axolotl Fine-Tuned Model\n",
    "\n",
    "This was the corresponding bits and bytes config for my Axolotl fine-tune.\n",
    "As said previously, it was trained using QLORA in 4bit precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56e0df6ffa143dea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:03:26.404090Z",
     "start_time": "2024-06-22T21:03:14.984364Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a47c727a2794477877772c0be6c1775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 6.05GB\n",
      "Max memory reserved: 6.12GB\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "bnb_llama_config = {\n",
    "    \"_load_in_4bit\": True,\n",
    "    \"_load_in_8bit\": False,\n",
    "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
    "    \"bnb_4bit_quant_storage\": \"bfloat16\",\n",
    "    \"bnb_4bit_quant_type\": \"nf4\",\n",
    "    \"bnb_4bit_use_double_quant\": True,\n",
    "    \"llm_int8_enable_fp32_cpu_offload\": False,\n",
    "    \"llm_int8_has_fp16_weight\": False,\n",
    "    \"llm_int8_skip_modules\": None,\n",
    "    \"llm_int8_threshold\": 6.0,\n",
    "    \"load_in_4bit\": True,\n",
    "    \"load_in_8bit\": False,\n",
    "    \"quant_method\": \"bitsandbytes\",\n",
    "}\n",
    "\n",
    "model_ckpt = \"model/checkpoint-1224/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "quantized_config = BitsAndBytesConfig(**bnb_llama_config)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_ckpt, device_map=\"auto\", quantization_config=quantized_config)\n",
    "print_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec2dda2210acb30e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:03:26.411812Z",
     "start_time": "2024-06-22T21:03:26.405551Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": 128001,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"quantization_config\": {\n",
       "    \"_load_in_4bit\": true,\n",
       "    \"_load_in_8bit\": false,\n",
       "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
       "    \"bnb_4bit_quant_storage\": \"bfloat16\",\n",
       "    \"bnb_4bit_quant_type\": \"nf4\",\n",
       "    \"bnb_4bit_use_double_quant\": true,\n",
       "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "    \"llm_int8_has_fp16_weight\": false,\n",
       "    \"llm_int8_skip_modules\": null,\n",
       "    \"llm_int8_threshold\": 6.0,\n",
       "    \"load_in_4bit\": true,\n",
       "    \"load_in_8bit\": false,\n",
       "    \"quant_method\": \"bitsandbytes\"\n",
       "  },\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.41.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdddc7a02ceaa4c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:03:26.657751Z",
     "start_time": "2024-06-22T21:03:26.418904Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.torch_dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c96d1c243bce5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Remember, when we run inference:\n",
    "\n",
    "- The model weights are indeed stored in 4-bit quantized format. This is what allows for the significant reduction in memory usage.\n",
    "- During inference, the weights are dequantized on-the-fly as they are needed for computation. However, it's important to note that this dequantization happens in small chunks, not for the entire model at once.\n",
    "- As data passes through each layer, the relevant 4-bit weights for that layer are temporarily dequantized to bfloat16.\n",
    "- The computation for that layer is then performed using these dequantized bfloat16 weights and the input data (also in bfloat16).\n",
    "- After the computation for a layer is complete, the dequantized weights can be discarded, and the next layer's weights are dequantized.\n",
    "\n",
    "- Given this information, 6 GB of memory usage for a 4-bit quantized 8B parameter model computing in bfloat16 does indeed sound reasonable.\n",
    "The base 4-bit model takes about 4 GB.\n",
    "\n",
    "I took a few random tweets about Anthropic's new model release Claude 3.5 Sonnet and run it through the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7a98e18f12a8b29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-22T21:05:58.769437Z",
     "start_time": "2024-06-22T21:05:53.400536Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Generate a list of interests.\n",
      "\n",
      "### Input:\n",
      "Introducing Claude 3.5 Sonnet—our most intelligent model yet.\n",
      "This is the first release in our 3.5 model family.\n",
      "Sonnet now outperforms competitor models on key evaluations, at twice the speed of Claude 3 Opus and one-fifth the cost.\n",
      "-------\n",
      "Less than 24 hours since Anthropic released Claude 3.5 Sonnet, and it surpassed GPT-4o.\n",
      "Here are 10 wild examples you don't want to miss:\n",
      "-------\n",
      "RIP ChatGPT?\n",
      "Anthropic just released Claude 3.5 Sonnet — ChatGPT's biggest competitor.\n",
      "12 Wild Examples of what it's capable of:\n",
      "-------\n",
      "I’m not as excited about OpenAI's new voice mod anymore. After seeing Anthropic's Sonnet 3.5, I realize that what matters most to me is the model's intelligence. \n",
      "I’ll be more excited for the next generation of OpenAI models rather than a voice mod that sounds more human.\n",
      "-------\n",
      "the sheer pettiness of anthropic saying \"good evening, sam\" in every single one of their demo videos for sonnet 3.5 sends me 💀\n",
      "how many more days will \"sam\" sit on gpt5?\n",
      "-------\n",
      "It really seems like Anthropic has scratched and Claude its way to the top.\n",
      "-------\n",
      "Anthropic is so back. Two things I like the most about Claude-3's release:\n",
      "1. Domain expert benchmarks. I'm much less interested in the saturated MMLU & HumanEval. Claude specifically picks Finance, Medicine, and Philosophy as expert domains and report performance. I recommend all LLM model cards to follow this, so that the different downstream applications know what to expect. \n",
      "2. Refusal rate analysis. LLMs' overly cautious answers to innocent questions are becoming a pandemic. Anthropic is typically on the ultra safe end of the spectrum, but they recognize the problem and highlight their efforts on it. Bravo! \n",
      "I love that Claude dials up heat in the arena that GPT and Gemini dominate. Though keep in mind that GPT-4V, the high water mark that everyone desperately tries to beat, finished training in 2022. It's the calm before the storm.\n",
      "\n",
      "### Response:\n",
      "LLM model,Anthropic,Claude,GPT,OpenAI,intelligence,competitor,ChatGPT,benchmark,expert domains,refusal rate,training,spectrum,GPT-4V<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "text = \"\"\"<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Generate a list of interests.\n",
    "\n",
    "### Input:\n",
    "Introducing Claude 3.5 Sonnet—our most intelligent model yet.\n",
    "This is the first release in our 3.5 model family.\n",
    "Sonnet now outperforms competitor models on key evaluations, at twice the speed of Claude 3 Opus and one-fifth the cost.\n",
    "-------\n",
    "Less than 24 hours since Anthropic released Claude 3.5 Sonnet, and it surpassed GPT-4o.\n",
    "Here are 10 wild examples you don't want to miss:\n",
    "-------\n",
    "RIP ChatGPT?\n",
    "Anthropic just released Claude 3.5 Sonnet — ChatGPT's biggest competitor.\n",
    "12 Wild Examples of what it's capable of:\n",
    "-------\n",
    "I’m not as excited about OpenAI's new voice mod anymore. After seeing Anthropic's Sonnet 3.5, I realize that what matters most to me is the model's intelligence. \n",
    "I’ll be more excited for the next generation of OpenAI models rather than a voice mod that sounds more human.\n",
    "-------\n",
    "the sheer pettiness of anthropic saying \"good evening, sam\" in every single one of their demo videos for sonnet 3.5 sends me 💀\n",
    "how many more days will \"sam\" sit on gpt5?\n",
    "-------\n",
    "It really seems like Anthropic has scratched and Claude its way to the top.\n",
    "-------\n",
    "Anthropic is so back. Two things I like the most about Claude-3's release:\n",
    "1. Domain expert benchmarks. I'm much less interested in the saturated MMLU & HumanEval. Claude specifically picks Finance, Medicine, and Philosophy as expert domains and report performance. I recommend all LLM model cards to follow this, so that the different downstream applications know what to expect. \n",
    "2. Refusal rate analysis. LLMs' overly cautious answers to innocent questions are becoming a pandemic. Anthropic is typically on the ultra safe end of the spectrum, but they recognize the problem and highlight their efforts on it. Bravo! \n",
    "I love that Claude dials up heat in the arena that GPT and Gemini dominate. Though keep in mind that GPT-4V, the high water mark that everyone desperately tries to beat, finished training in 2022. It's the calm before the storm.\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=500, do_sample=True, temperature=1)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d2e209eec10d8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Although my understanding of fine-tuning LLMs and memory usage is pretty high level, it's making a lot more sense then it did before.\n",
    "I'm happy to have these notes to refer back to as I continue to learn about this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65618e75c67fc0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Resources/Links\n",
    "\n",
    "[Bits and Bytes Basics](https://web.stanford.edu/class/cs101/bits-bytes.html)\n",
    "\n",
    "[A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration)\n",
    "\n",
    "[Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes)\n",
    "\n",
    "[Optimizing your LLM in production](https://huggingface.co/blog/optimize-llm)\n",
    "\n",
    "[Accelerating Large Language Models with Mixed-Precision Techniques](https://sebastianraschka.com/blog/2023/llm-mixed-precision-copy.html)\n",
    "\n",
    "[Understanding how big of a model can fit on your machine](https://huggingface.co/docs/accelerate/en/usage_guides/model_size_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1dfa3cf52a7514",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
