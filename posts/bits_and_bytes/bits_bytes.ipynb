{
 "cells": [
  {
   "cell_type": "raw",
   "id": "580eeb7aecf11ffe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "title: Understanding VRAM Usage - Bits and Bytes - BETTER TITLE TODO\n",
    "author: Chris Levy\n",
    "date: '2024-06-15'\n",
    "date-modified: '2024-06-5'\n",
    "image: imgs/bits_and_bytes.webp\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: show\n",
    "    page-layout: full\n",
    "include-in-header:\n",
    "  - text: |\n",
    "      <style>\n",
    "      .cell-output-stdout code {\n",
    "        word-break: break-wor !important;\n",
    "        white-space: pre-wrap !important;\n",
    "      }\n",
    "      </style>\n",
    "bibliography: ../../bibliography.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2a4f19d10d3e14",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# TODO: Make headers/sections\n",
    "\n",
    "In this blog post I will be writing about some things I learned recently in the [LLM conference](https://maven.com/parlance-labs/fine-tuning?utm_campaign=2848bd&utm_medium=partner&utm_source=instructor0) I took part in. \n",
    "Some of the details come from [Jonathan Whitaker's](https://x.com/johnowhitaker) [talk](https://x.com/HamelHusain/status/1798353336145674483).\n",
    " \n",
    "\n",
    "\n",
    "```\n",
    "ssh -i ~/.ssh/jarivs_labs -o StrictHostKeyChecking=no  -p 11014 root@sshe.jarvislabs.ai\n",
    "```\n",
    "\n",
    "\n",
    "You have probably heard the saying, \"In the computer it's all 0's and 1's\". Well, those 0's and 1's are called **bits**. A **bit** is the smallest\n",
    "unit of storage and simply stores a 0 or a 1. A **byte** is a group of 8 **bits** together.\n",
    "\n",
    "- 1 **byte** **=** 8 **bytes**.\n",
    "\n",
    "All storage is measured in **bytes**. You're probably very familiar with these units:\n",
    "\n",
    "| Unit      | Abbreviation | Approximate Size        |\n",
    "|-----------|--------------|-------------------------|\n",
    "| Kilobyte  | KB           | about 1 thousand bytes  |\n",
    "| Megabyte  | MB           | about 1 million bytes   |\n",
    "| Gigabyte  | GB           | about 1 billion bytes   |\n",
    "| Terabyte  | TB           | about 1 trillion bytes  |\n",
    "\n",
    "\n",
    "This blog post on [Hugging Face](https://huggingface.co/blog/hf-bitsandbytes-integration) explains the common data types used in machine learning.\n",
    "The most common data types are (float16, float32, bfloat16, int8). Then this [article](https://huggingface.co/blog/4bit-transformers-bitsandbytes) explains more on `bitsandbytes`, quantization, and `QLORA`. `bitsandbytes` is a Python library which is used heavily in quantization of LLMs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe38309bb39b680c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:15.405082Z",
     "start_time": "2024-06-21T11:16:12.922841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 0.00GB\n",
      "Max memory reserved: 0.00GB\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "\n",
    "def print_memory_stats():\n",
    "    \"\"\"Print two different measures of GPU memory usage\"\"\"\n",
    "    print(f\"Max memory allocated: {torch.cuda.max_memory_allocated(device)/1e9:.2f}GB\")\n",
    "    # reserved (aka 'max_memory_cached') is ~the allocated memory plus pre-cached memory\n",
    "    print(f\"Max memory reserved: {torch.cuda.max_memory_reserved(device)/1e9:.2f}GB\")\n",
    "\n",
    "\n",
    "print_memory_stats()\n",
    "\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b653da7f16bec613",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's load the model `TinyLlama/TinyLlama-1.1B-Chat-v1.0`.\n",
    "By default, it will load in `float32`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21e2099adab2e91f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:19.810795Z",
     "start_time": "2024-06-21T11:16:15.402970Z"
    }
   },
   "outputs": [],
   "source": [
    "# | output: false\n",
    "model_ckpt = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_ckpt, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f7a66d410bd5bb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can check the `dtype` of the parameters and indeed see that they are stored in `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d9c3b7752c5aa8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:19.811686Z",
     "start_time": "2024-06-21T11:16:19.808875Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{torch.float32}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | warning: false\n",
    "set([(x.dtype) for x in model.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f32cb1d6bc3f5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since the parameters are in `float32`, we can estimate that each parameter will takes up 32/8=4 bytes of memory.\n",
    "For a 1.1B parameter model that is 4.4gb. Let's see if our rough back of the napkin calculation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f15b746d83556cb7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:20.216610Z",
     "start_time": "2024-06-21T11:16:19.810285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 21 11:16:19 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA RTX A6000               Off | 00000000:1A:00.0 Off |                  Off |\r\n",
      "| 30%   33C    P2              66W / 300W |   4475MiB / 49140MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7797e90d7893f406",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:20.224829Z",
     "start_time": "2024-06-21T11:16:20.216313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 4.40GB\n",
      "Max memory reserved: 4.40GB\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "print_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Yes, that's what we thought.\n",
    "\n",
    "Let's run some inference with the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4eff909b29906a73"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "How many bytes are in one gigabyte? \n",
      "<|assistant|>\n",
      "Yes, there are 1,000,000,000 bytes in a gigabyte (GB).\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "def inference(messages):\n",
    "    tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=tokenized_chat.to(\"cuda\"), max_new_tokens=128, do_sample=False)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])\n",
    "inference([{\"role\": \"user\", \"content\": \"How many bytes are in one gigabyte?\"}])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:21.212803Z",
     "start_time": "2024-06-21T11:16:20.217299Z"
    }
   },
   "id": "ccc99f33e6ed4e7f"
  },
  {
   "cell_type": "markdown",
   "id": "cadc6bebfaa01444",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's load the model in a lower precision.\n",
    "The model config points to what precision to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9492d303cd58b29",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:21.222317Z",
     "start_time": "2024-06-21T11:16:21.216370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.bfloat16"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | warning: false\n",
    "model.config.torch_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5147a6aa634175e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:21.909655Z",
     "start_time": "2024-06-21T11:16:21.228290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 0.01GB\n",
      "Max memory reserved: 0.02GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 21 11:16:21 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA RTX A6000               Off | 00000000:1A:00.0 Off |                  Off |\r\n",
      "| 30%   34C    P2              89W / 300W |    351MiB / 49140MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "del model\n",
    "cleanup()\n",
    "print_memory_stats()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7cad3a66839d93",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's load the model in `bfloat16`. Estimating that each parameter will use 16/8=2 bytes of memory.\n",
    "For the same model, it should use roughly half the memory as before, 2.2GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "907717a07238236",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:23.804977Z",
     "start_time": "2024-06-21T11:16:21.923158Z"
    }
   },
   "outputs": [],
   "source": [
    "# | output: false\n",
    "model = AutoModelForCausalLM.from_pretrained(model_ckpt, torch_dtype=torch.bfloat16, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cfb4b56fa58e37a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:23.805734Z",
     "start_time": "2024-06-21T11:16:23.804455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 2.21GB\n",
      "Max memory reserved: 2.32GB\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "print_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f6d4925b1eeaafd",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:24.107634Z",
     "start_time": "2024-06-21T11:16:23.805946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{torch.bfloat16}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | warning: false\n",
    "set([(x.dtype) for x in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70844652d91fd861",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:24.726590Z",
     "start_time": "2024-06-21T11:16:24.105282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "How many bytes are in one gigabyte? \n",
      "<|assistant|>\n",
      "Yes, there are 1,000,000,000 bytes in a gigabyte (GB).\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "inference([{\"role\": \"user\", \"content\": \"How many bytes are in one gigabyte?\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the exact same output we got before. Since most models are currently trained using `bfloat16`, there's no need to use full `float32` precision. In this example if we use float32, it won't improve inference results compared to `bfloat16`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10e58204c8dce86b"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 0.01GB\n",
      "Max memory reserved: 0.02GB\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "del model\n",
    "cleanup()\n",
    "print_memory_stats()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:24.830926Z",
     "start_time": "2024-06-21T11:16:24.726129Z"
    }
   },
   "id": "ced79341e973055b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's try loading a quantized model using  `BitsAndBytesConfig`. \n",
    "Note that the model weights are stored in 4bit precision but the computations are done in a higher precision.\n",
    "Here we are specifying that the dtype for computations is bf16. During inference, as well as training, \n",
    "the weights of the model are constantly being dequantized (from 4bit to bf16). This can be done for specific layers at a time\n",
    "during forward and backward passes, to keep memory requirements low. *The computation happens in a higher precision*.\n",
    "\n",
    "Here we specify that the model should be loaded in 4bit and the computations done in bf16.\n",
    "Since we are using 4bit we expect each parameter to use 4/8=0.5 bytes so we should be using less than a GB of memory."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5320c28b9128cd8f"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "984f2d3573fbe7eb",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:27.907189Z",
     "start_time": "2024-06-21T11:16:24.831981Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: BNB_CUDA_VERSION=118 environment variable detected; loading libbitsandbytes_cuda118.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 0.84GB\n",
      "Max memory reserved: 0.89GB\n"
     ]
    }
   ],
   "source": [
    "print_memory_stats()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:27.913748Z",
     "start_time": "2024-06-21T11:16:27.908389Z"
    }
   },
   "id": "f359ae76e250f2f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we expect the model inference results to be different:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92d4e947590142a3"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "How many bytes are in one gigabyte? \n",
      "<|assistant|>\n",
      "Yes, one gigabyte (GB) is equal to 1,073,741,824 bytes. A byte is a unit of information storage in the binary system, which is the basis for digital computing. In binary, each byte has a value of 10, with each bit representing a single binary digit. So, one gigabyte is equivalent to 1,073,741,824 bytes, which is approximately 1,000,000,000 bytes.\n"
     ]
    }
   ],
   "source": [
    "inference([{\"role\": \"user\", \"content\": \"How many bytes are in one gigabyte?\"}])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:32.914416Z",
     "start_time": "2024-06-21T11:16:27.917530Z"
    }
   },
   "id": "93477d7cb3925399"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can experiment with different types of quantization.  \n",
    "In this next example we load the model using: \n",
    "\n",
    "- NF4 quantization.\n",
    "- `bnb_4bit_use_double_quant` which uses a second quantization after the first one.\n",
    "- bfloat16 for computation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32c00f35056e106b"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 0.01GB\n",
      "Max memory reserved: 0.02GB\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "cleanup()\n",
    "print_memory_stats()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:32.951687Z",
     "start_time": "2024-06-21T11:16:32.913856Z"
    }
   },
   "id": "48a48f05fbe39415"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# | warning: false\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    quantization_config=nf4_config,\n",
    "    device_map='auto'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:34.869772Z",
     "start_time": "2024-06-21T11:16:32.939185Z"
    }
   },
   "id": "4a86681c7f60fbd0"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max memory allocated: 0.80GB\n",
      "Max memory reserved: 0.84GB\n"
     ]
    }
   ],
   "source": [
    "print_memory_stats()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:34.887069Z",
     "start_time": "2024-06-21T11:16:34.877719Z"
    }
   },
   "id": "c7c3ec304fcf1c0f"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "How many bytes are in one gigabyte? \n",
      "<|assistant|>\n",
      "Yes, I can provide you with the answer to your question. A gigabyte (GB) is a unit of measurement for data storage. It is equal to 1,000 bytes. So, 1 GB is equal to 1,000,000,000 bytes.\n"
     ]
    }
   ],
   "source": [
    "inference([{\"role\": \"user\", \"content\": \"How many bytes are in one gigabyte?\"}])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:38.242097Z",
     "start_time": "2024-06-21T11:16:34.887756Z"
    }
   },
   "id": "18b1ab35d50c4774"
  },
  {
   "cell_type": "markdown",
   "source": [
    "I really like the high level explanation of quantization from this [post](https://huggingface.co/blog/optimize-llm) by Patrick von Platen.\n",
    "In general when running inference with quantized models the steps are:\n",
    "\n",
    "- Quantize all the weights of the model and load it (for example 4bit).\n",
    "- Pass through the input sequence in bf16.\n",
    "- Dynamically dequantize the weights to bf16 layer by layer during the forward pass\n",
    "- Quantize the weights back to 4bit after the computation\n",
    "\n",
    "So if we want to do $Y = X W$ where $W$ and $X$ are the weights and input sequence respectively, then for each matrix multiplication we do:\n",
    "\n",
    "$Y = X \\cdot \\text{dequantize}(W)$ ; $\\text{quantize}(W);$\n",
    "\n",
    "\n",
    "\n",
    "For this reason, inference is usually not faster when using quantized models. It's slower. It is good to remember that quantization is a tradeoff between memory usage and output quality, as well as possibly inference time."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78121756200d815"
  },
  {
   "cell_type": "markdown",
   "id": "b65618e75c67fc0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# TODO:\n",
    "\n",
    "Talk about (not my words): For shorter text inputs (less than 1024 tokens), the memory requirement for inference is very much dominated by the memory requirement to load the weights. Therefore, for now, let's assume that the memory requirement for inference is equal to the memory requirement to load the model into the GPU VRAM.\n",
    "# Resources/Links\n",
    "\n",
    "[basics of bits and bytes](https://web.stanford.edu/class/cs101/bits-bytes.html)\n",
    "\n",
    "https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "\n",
    "https://huggingface.co/blog/hf-bitsandbytes-\n",
    "\n",
    "https://huggingface.co/blog/optimize-llm\n",
    "\n",
    "https://sebastianraschka.com/blog/2023/llm-mixed-precision-copy.html\n",
    "\n",
    "https://huggingface.co/docs/accelerate/en/usage_guides/model_size_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:38.242405Z",
     "start_time": "2024-06-21T11:16:38.239661Z"
    }
   },
   "id": "1b1f49fdb6cb0d19"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T11:16:38.242564Z",
     "start_time": "2024-06-21T11:16:38.241143Z"
    }
   },
   "id": "d97b0c212b6bc785"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a45fb3a5a17f6147",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-21T11:16:38.445254Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
