{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7908ac58af685d94",
   "metadata": {
    "collapsed": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Fine-Tuning Modern Bert on Modal\n",
    "author: Chris Levy\n",
    "draft: false\n",
    "date: '2024-12-21'\n",
    "date-modified: '2024-12-21'\n",
    "image: imgs/intro.png\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: show\n",
    "    page-layout: full\n",
    "include-in-header:\n",
    "  - text: |\n",
    "      <style>\n",
    "      .cell-output-stdout code {\n",
    "        word-break: break-wor !important;\n",
    "        white-space: pre-wrap !important;\n",
    "      }\n",
    "      </style>\n",
    "bibliography: ../../bibliography.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb7feb7",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "First go and read the blog post announcement [here](https://huggingface.co/blog/modernbert). \n",
    "If you are interested I wrote a little about transformers (encoders and decoders) in my previous blog posts [here](https://drchrislevy.github.io/posts/vllms/vllm.html) and [here](https://drchrislevy.github.io/posts/basic_transformer_notes/transformers.html). I also wrote a previous blog post on [Modal](https://drchrislevy.github.io/posts/modal_fun/modal_blog.html) if you want to learn more about it as well.\n",
    "\n",
    "# Encoder Models Generate Embedding Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba33c9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModernBertModel(\n",
       "  (embeddings): ModernBertEmbeddings(\n",
       "    (tok_embeddings): Embedding(50368, 768, padding_idx=50283)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0): ModernBertEncoderLayer(\n",
       "      (attn_norm): Identity()\n",
       "      (attn): ModernBertAttention(\n",
       "        (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (rotary_emb): ModernBertRotaryEmbedding()\n",
       "        (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_drop): Identity()\n",
       "      )\n",
       "      (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): ModernBertMLP(\n",
       "        (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (act): GELUActivation()\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1-21): 21 x ModernBertEncoderLayer(\n",
       "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): ModernBertAttention(\n",
       "        (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (rotary_emb): ModernBertRotaryEmbedding()\n",
       "        (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_drop): Identity()\n",
       "      )\n",
       "      (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): ModernBertMLP(\n",
       "        (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (act): GELUActivation()\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_id = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fdf65f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50281,   510,  5347,   273, 30947, 47138,   310, 14449, 41653,    15,\n",
       "         50282]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The capital of Nova Scotia is Halifax.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b7824d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'hidden_states'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get embeddings\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5557d511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{torch.Size([1, 11, 768])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tuple containing outputs from every layer in the model\n",
    "len(outputs.hidden_states)\n",
    "set([x.shape for x in outputs.hidden_states])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c6b1338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 768])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last_hidden_state\n",
    "# Single tensor representing the final layer's output\n",
    "# [batch_size, sequence_length, hidden_size]\n",
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f03ab20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.9541e-01, -1.1135e+00, -9.1821e-01,  ..., -4.2644e-01,\n",
       "           2.0316e-01, -7.5940e-01],\n",
       "         [ 1.2727e-01,  6.0307e-02,  2.4341e-01,  ...,  1.3519e-01,\n",
       "          -1.0590e-01,  9.5566e-02],\n",
       "         [ 3.2714e-01, -1.3615e+00, -8.6864e-01,  ...,  5.3308e-01,\n",
       "           1.4498e+00,  1.4891e-01],\n",
       "         ...,\n",
       "         [-2.8325e-02, -8.1840e-01, -1.1389e-01,  ...,  3.3296e-01,\n",
       "          -5.4001e-01, -2.0064e-01],\n",
       "         [-1.3851e+00,  1.5134e-01, -8.1608e-01,  ..., -1.4898e+00,\n",
       "           2.8013e-01,  1.3483e+00],\n",
       "         [ 2.5279e-01, -6.3874e-02,  7.7065e-02,  ...,  5.3266e-04,\n",
       "          -5.2192e-03, -1.5917e-01]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba621e",
   "metadata": {},
   "source": [
    "The reason we get an embedding for each token (11 in this example) is because BERT ( ModernBERT) are contextual embedding models, meaning they create representations that capture each token's meaning based on its context in the sentence. Each token gets its own 768-dimensional embedding vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf8a4a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position 0:\n",
      "Input Token ID: 50281\n",
      "Input Token: '[CLS]'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 1:\n",
      "Input Token ID: 510\n",
      "Input Token: 'The'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 2:\n",
      "Input Token ID: 5347\n",
      "Input Token: ' capital'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 3:\n",
      "Input Token ID: 273\n",
      "Input Token: ' of'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 4:\n",
      "Input Token ID: 30947\n",
      "Input Token: ' Nova'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 5:\n",
      "Input Token ID: 47138\n",
      "Input Token: ' Scotia'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 6:\n",
      "Input Token ID: 310\n",
      "Input Token: ' is'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 7:\n",
      "Input Token ID: 14449\n",
      "Input Token: ' Hal'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 8:\n",
      "Input Token ID: 41653\n",
      "Input Token: 'ifax'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 9:\n",
      "Input Token ID: 15\n",
      "Input Token: '.'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 10:\n",
      "Input Token ID: 50282\n",
      "Input Token: '[SEP]'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for position in range(len(inputs.input_ids[0])):\n",
    "    token_id = inputs.input_ids[0][position]\n",
    "    decoded_token = tokenizer.decode([token_id])\n",
    "    embedding = outputs.last_hidden_state[0][position]\n",
    "    print(f\"Position {position}:\")\n",
    "    print(f\"Input Token ID: {token_id}\")\n",
    "    print(f\"Input Token: '{decoded_token}'\")\n",
    "    print(f\"Embedding Shape: {embedding.shape}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd9a740",
   "metadata": {},
   "source": [
    "For downstream tasks with BERT-like models (including ModernBERT), there are typically two main approaches:\n",
    "\n",
    "1. `[`CLS]` Token Embedding (Most Common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d031fc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the [CLS] token embedding (first token, index 0)\n",
    "cls_embedding = outputs.last_hidden_state[0][0]  # Shape: [768]\n",
    "cls_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4e2647",
   "metadata": {},
   "source": [
    "2. Mean Pooling (Alternative Approach)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23862bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean pooling - take average of all tokens\n",
    "mean_embedding = outputs.last_hidden_state[0].mean(dim=0)  # Shape: [768]\n",
    "mean_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf7d9e",
   "metadata": {},
   "source": [
    "The `[CLS]` token is specifically designed to capture sentence-level information and is most commonly used for classification tasks. This is because BERT models are trained to use this token to aggregate information from the entire sequence.\n",
    "\n",
    "\n",
    "Best Practices:\n",
    "\n",
    "- For classification tasks: Use the `[CLS]` token embedding   \n",
    "- For token-level tasks (like NER): Use the individual token embeddings\n",
    "- For sentence similarity: Either approach can work, but `[CLS]` is more common\n",
    "- For document embeddings: Mean pooling might perform better on longer texts\n",
    "\n",
    "Remember that whichever embedding you choose will need to be passed through any additional layers in your downstream task (e.g., a classification head).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62d35f",
   "metadata": {},
   "source": [
    "# Fine-Tuning ModernBERT for Classification \n",
    "\n",
    "When I first learned about fine-tuning transformers encoder for classification tasks, my favorite resource was the book [Natural Language Processing with Transformers: Building Language Applications with Hugging Face](https://www.amazon.ca/dp/1098136799?smid=ATVPDKIKX0DER&_encoding=UTF8&linkCode=gs2&tag=oreilly200b-20). It's still relevant and a great resource. In particular, checkout Chapter 2 which walks trough classification tasks. In that chapter the authors first train a simple classifier on top of the `[CLS]` token embeddings. In that case the model is frozen and only used as a feature extractor. The other approach is to fine-tune the entire model together with a classification head. It's this latter approach that I'll show you how to do here.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "892c4078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | warning: false\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"dair-ai/emotion\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc0d9c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i didnt feel humiliated', 'label': 0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03d8ebbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c359ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{i: l for i, l in enumerate([\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d293f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "989dae99",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "[Announcement from Jeremy Howard on X](https://x.com/jeremyphoward/status/1869786023963832509)\n",
    "\n",
    "[Blog Post on Hugging Face](https://huggingface.co/blog/modernbert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd94bad1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
