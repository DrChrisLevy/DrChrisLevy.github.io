{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7908ac58af685d94",
   "metadata": {
    "collapsed": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Fine-Tuning ModernBERT For Classification Tasks on Modal\n",
    "author: Chris Levy\n",
    "draft: false\n",
    "date: '2024-12-29'\n",
    "date-modified: '2024-12-29'\n",
    "image: bert_intro.jpg\n",
    "toc: true\n",
    "description: In this blog post, I walk through fine-tuning the ModernBERT model for classification tasks using Modal, detailing the setup process, providing code examples for data handling and model training, and aiming to be a practical guide for developers.\n",
    "tags:\n",
    "  - ModernBERT\n",
    "  - fine-tuning\n",
    "  - classification\n",
    "  - modal\n",
    "  - transformers\n",
    "format:\n",
    "  html:\n",
    "    code-fold: show\n",
    "    page-layout: full\n",
    "include-in-header:\n",
    "  - text: |\n",
    "      <style>\n",
    "      .cell-output-stdout code {\n",
    "        word-break: break-wor !important;\n",
    "        white-space: pre-wrap !important;\n",
    "      }\n",
    "      </style>\n",
    "bibliography: ../../bibliography.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb7feb7",
   "metadata": {},
   "source": [
    "![](static_blog_imgs/bert_old.jpg)\n",
    "\n",
    "# Intro\n",
    "\n",
    "First go and read the ModernBert blog post announcement [here](https://huggingface.co/blog/modernbert). \n",
    "If you are interested I wrote a little about transformers (encoders and decoders) in my previous blog posts [here](https://drchrislevy.github.io/posts/vllms/vllm.html) and [here](https://drchrislevy.github.io/posts/basic_transformer_notes/transformers.html). I also have written previously about using Modal  [here](https://drchrislevy.github.io/posts/modal_fun/modal_blog.html) and [here](https://drchrislevy.github.io/posts/colpali/colpali_blog.html) and [here](https://drchrislevy.github.io/posts/intro_modal/intro_modal.html).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3711b6c",
   "metadata": {},
   "source": [
    "# Encoder Models Generate Embedding Representations\n",
    "\n",
    "This section gives a very quick rundown on how encoder models **encode** text into embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba33c9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopher/personal_projects/DrChrisLevy.github.io/posts/modern_bert/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModernBertModel(\n",
       "  (embeddings): ModernBertEmbeddings(\n",
       "    (tok_embeddings): Embedding(50368, 768, padding_idx=50283)\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (layers): ModuleList(\n",
       "    (0): ModernBertEncoderLayer(\n",
       "      (attn_norm): Identity()\n",
       "      (attn): ModernBertAttention(\n",
       "        (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (rotary_emb): ModernBertRotaryEmbedding()\n",
       "        (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_drop): Identity()\n",
       "      )\n",
       "      (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): ModernBertMLP(\n",
       "        (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (act): GELUActivation()\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1-21): 21 x ModernBertEncoderLayer(\n",
       "      (attn_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): ModernBertAttention(\n",
       "        (Wqkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (rotary_emb): ModernBertRotaryEmbedding()\n",
       "        (Wo): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_drop): Identity()\n",
       "      )\n",
       "      (mlp_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): ModernBertMLP(\n",
       "        (Wi): Linear(in_features=768, out_features=2304, bias=False)\n",
       "        (act): GELUActivation()\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (Wo): Linear(in_features=1152, out_features=768, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | warning: false\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_id = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fdf65f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50281,   510,  5347,   273, 30947, 47138,   310, 14449, 41653,    15,\n",
       "         50282]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The capital of Nova Scotia is Halifax.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b7824d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'hidden_states'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get embeddings\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5557d511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{torch.Size([1, 11, 768])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tuple containing outputs from every layer in the model\n",
    "print(len(outputs.hidden_states))\n",
    "set([x.shape for x in outputs.hidden_states])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c6b1338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last_hidden_state\n",
    "# Single tensor representing the final layer's output\n",
    "# [batch_size, sequence_length, hidden_size]\n",
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f03ab20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.9541e-01, -1.1135e+00, -9.1821e-01,  ..., -4.2644e-01,\n",
       "           2.0316e-01, -7.5940e-01],\n",
       "         [ 1.2727e-01,  6.0307e-02,  2.4341e-01,  ...,  1.3519e-01,\n",
       "          -1.0590e-01,  9.5566e-02],\n",
       "         [ 3.2714e-01, -1.3615e+00, -8.6864e-01,  ...,  5.3308e-01,\n",
       "           1.4498e+00,  1.4891e-01],\n",
       "         ...,\n",
       "         [-2.8325e-02, -8.1840e-01, -1.1389e-01,  ...,  3.3296e-01,\n",
       "          -5.4001e-01, -2.0064e-01],\n",
       "         [-1.3851e+00,  1.5134e-01, -8.1608e-01,  ..., -1.4898e+00,\n",
       "           2.8013e-01,  1.3483e+00],\n",
       "         [ 2.5279e-01, -6.3874e-02,  7.7065e-02,  ...,  5.3266e-04,\n",
       "          -5.2192e-03, -1.5917e-01]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ba621e",
   "metadata": {},
   "source": [
    "The reason we get an embedding for each token (11 in this example) is because BERT ( ModernBERT) are contextual embedding models, meaning they create representations that capture each token's meaning based on its context in the sentence. Each token gets its own 768-dimensional embedding vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf8a4a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position 0:\n",
      "Input Token ID: 50281\n",
      "Input Token: '[CLS]'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 1:\n",
      "Input Token ID: 510\n",
      "Input Token: 'The'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 2:\n",
      "Input Token ID: 5347\n",
      "Input Token: ' capital'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 3:\n",
      "Input Token ID: 273\n",
      "Input Token: ' of'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 4:\n",
      "Input Token ID: 30947\n",
      "Input Token: ' Nova'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 5:\n",
      "Input Token ID: 47138\n",
      "Input Token: ' Scotia'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 6:\n",
      "Input Token ID: 310\n",
      "Input Token: ' is'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 7:\n",
      "Input Token ID: 14449\n",
      "Input Token: ' Hal'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 8:\n",
      "Input Token ID: 41653\n",
      "Input Token: 'ifax'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 9:\n",
      "Input Token ID: 15\n",
      "Input Token: '.'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n",
      "Position 10:\n",
      "Input Token ID: 50282\n",
      "Input Token: '[SEP]'\n",
      "Embedding Shape: torch.Size([768])\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for position in range(len(inputs.input_ids[0])):\n",
    "    token_id = inputs.input_ids[0][position]\n",
    "    decoded_token = tokenizer.decode([token_id])\n",
    "    embedding = outputs.last_hidden_state[0][position]\n",
    "    print(f\"Position {position}:\")\n",
    "    print(f\"Input Token ID: {token_id}\")\n",
    "    print(f\"Input Token: '{decoded_token}'\")\n",
    "    print(f\"Embedding Shape: {embedding.shape}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd9a740",
   "metadata": {},
   "source": [
    "For downstream tasks with BERT-like models (including ModernBERT), there are typically two main approaches for generating\n",
    "a single embedding for the entire input text:\n",
    "\n",
    "1. `[CLS]` Token Embedding (Most Common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d031fc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the [CLS] token embedding (first token, index 0)\n",
    "cls_embedding = outputs.last_hidden_state[0][0]  # Shape: [768]\n",
    "cls_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4e2647",
   "metadata": {},
   "source": [
    "2. Mean Pooling (Alternative Approach)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23862bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean pooling - take average of all tokens\n",
    "mean_embedding = outputs.last_hidden_state[0].mean(dim=0)  # Shape: [768]\n",
    "mean_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf7d9e",
   "metadata": {},
   "source": [
    "The `[CLS]` token is specifically designed to capture sentence-level information and is most commonly used for classification tasks. This is because BERT models are trained to use this token to aggregate information from the entire sequence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5384e329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "def import_python_as_markdown(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        content = file.read()\n",
    "    return f\"```python\\n{content}\\n```\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62d35f",
   "metadata": {},
   "source": [
    "# Fine-Tuning ModernBERT for Classification \n",
    "\n",
    "When I first learned about fine-tuning transformer encoder models for classification tasks, my favorite resource was the book [Natural Language Processing with Transformers: Building Language Applications with Hugging Face](https://www.amazon.ca/dp/1098136799?smid=ATVPDKIKX0DER&_encoding=UTF8&linkCode=gs2&tag=oreilly200b-20). It's still relevant and a great resource. In particular, checkout Chapter 2 which walks through classification tasks. In that chapter the authors first train a simple classifier on top of the `[CLS]` token embeddings. In that case the model is frozen and only used as a feature extractor. The other approach is to fine-tune the entire model together with a classification head. It's this latter approach that I'll show you how to do here.\n",
    "\n",
    "\n",
    "## Create a Modal Account\n",
    "\n",
    "- you get $30 a month of free compute!\n",
    "- create an account at [modal.com](https://modal.com/)\n",
    "- [Super easy to set up](https://modal.com/docs/guide)\n",
    "\n",
    "## Setup the Environment\n",
    "\n",
    "```\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "pip install modal dotenv\n",
    "modal setup\n",
    "```\n",
    "\n",
    "- Place your wandb api key in a `.env` file like this: `WANDB_API_KEY=<>`\n",
    "- create the file`trainer.py` and place it at the root of your project folder alongside the `.env` file. The full code is below but you can also find it [here](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/modern_bert/trainer.py).\n",
    "\n",
    "\n",
    "\n",
    "## Training Code\n",
    "\n",
    "Here is all the code for the `trainer.py` file.\n",
    "\n",
    "- At the beginning of the file you can adjust the dataset, model, learning rate, batch size, epochs, class labels, column names, etc.\n",
    "    - It's expected to use a Hugging Face dataset and it's expected that you will have to change these variables based on the dataset you are using.\n",
    "    -  You can also make edits anywhere else in the code as well but when you are first starting out it's best to keep the code simple and only make changes to the variables at the beginning of the file.\n",
    "- When you run `modal run trainer.py` it will execute the code within the function `main()`.\n",
    "    - By default it trains a model and then evaluates it on the validation split\n",
    "    - You can do whatever else you want here in the `main()` function. For example, you could comment out the training logic and just run an evaluation on some checkpoint.\n",
    "- There are two main **modal** functions which each run in their own container. See the functions decorated with `@modal.method()`, which are `train_model` and `eval_model`. \n",
    "- If you want to run different training runs or evaluation runs just edit the file and kick off the jobs by executing `modal run trainer.py` from the command line. Remember modal will take care of spinning up the containers and running the code!\n",
    "- You can use the command `modal run --detach trainer.py`which lets the app continue running even if your client disconnects.\n",
    "- In either case you will see live logs directly in your local terminal, even though the containers are running in the cloud.\n",
    "- You can also follow along with logs and container metrics in the [Modal UI dashboard](https://modal.com/).\n",
    "- You can also see the wandb outputs at [https://wandb.ai/home](https://wandb.ai/home)\n",
    "- All the datasets and models are stored in the Modal volumes. You can see them in the [Modal UI dashboard](https://modal.com/).\n",
    "\n",
    "Here are is the `trainer.py` file. You can also find it here on [github](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/modern_bert/trainer.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "411cc476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "# ruff: noqa\n",
       "import os\n",
       "import shutil\n",
       "\n",
       "import modal\n",
       "from dotenv import load_dotenv\n",
       "from modal import Image, build, enter\n",
       "\n",
       "# ---------------------------------- SETUP BEGIN ----------------------------------#\n",
       "env_file = \".env\"  # path to local env file with wandb api key WANDB_API_KEY=<>\n",
       "ds_name = \"dair-ai/emotion\"  # name of the Hugging Face dataset to use\n",
       "ds_name_config = None  # for hugging face datasets that have multiple config instances. For example cardiffnlp/tweet_eval\n",
       "train_split = \"train\"  # name of the tain split in the dataset\n",
       "validation_split = \"validation\"  # name of the validation split in the dataset\n",
       "test_split = \"test\"  # name of the test split in the dataset\n",
       "# define the labels for the dataset\n",
       "id2label = {0: \"sadness\", 1: \"joy\", 2: \"love\", 3: \"anger\", 4: \"fear\", 5: \"surprise\"}\n",
       "# Often commonly called \"inputs\". Depends on the dataset. This is the input text to the model.\n",
       "# This field will be called input_ids during tokenization/training/eval.\n",
       "input_column = \"text\"\n",
       "# This is the column name from the dataset which is the target to train on.\n",
       "# It will get renamed to \"label\" during tokenization/training/eval.\n",
       "label_column = \"label\"\n",
       "checkpoint = \"answerdotai/ModernBERT-base\"  # name of the Hugging Face model to fine tune\n",
       "batch_size = 32  # depends on GPU size and model size\n",
       "GPU_SIZE = \"A100\"  # https://modal.com/docs/guide/gpu#specifying-gpu-type\n",
       "num_train_epochs = 2\n",
       "learning_rate = 5e-5  # learning rate for the optimizer\n",
       "\n",
       "\n",
       "# This is the logic for tokenizing the input text. It's used in the dataset map function\n",
       "# during training and evaluation. Of importance is the max_length parameter which\n",
       "# you will want to increase for input texts that are longer. Traditionally bert and other encoder\n",
       "# models have a max length of 512 tokens. But ModernBERT has a max length of 8192 tokens.\n",
       "def tokenizer_function_logic(example, tokenizer):\n",
       "    return tokenizer(example[input_column], padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
       "\n",
       "\n",
       "wandb_project = \"hugging_face_training_jobs\"  # name of the wandb project to use\n",
       "pre_fix_name = \"\"  # optional prefix to the run name to differentiate it from other experiments\n",
       "# This is a label that gets assigned to any example that is not classified by the model\n",
       "# according to some probability threshold. It's only used for evaluation.\n",
       "unknown_label_int = -1\n",
       "unknown_label_str = \"UNKNOWN\"\n",
       "# define the run name which is used in wandb and the model name when saving model checkpoints\n",
       "run_name = f\"{ds_name}-{ds_name_config}-{checkpoint}-{batch_size=}-{learning_rate=}-{num_train_epochs=}\"\n",
       "# ---------------------------------- SETUP END----------------------------------#\n",
       "\n",
       "if pre_fix_name:\n",
       "    run_name = f\"{pre_fix_name}-{run_name}\"\n",
       "\n",
       "label2id = {v: k for k, v in id2label.items()}\n",
       "path_to_ds = os.path.join(\"/data\", ds_name, ds_name_config if ds_name_config else \"\")\n",
       "\n",
       "load_dotenv(env_file)\n",
       "app = modal.App(\"trainer\")\n",
       "\n",
       "# Non Flash-Attn Image\n",
       "# image = Image.debian_slim(python_version=\"3.11\").run_commands(\n",
       "#     \"apt-get update && apt-get install -y htop git\",\n",
       "#     \"pip3 install torch torchvision torchaudio\",\n",
       "#     \"pip install git+https://github.com/huggingface/transformers.git datasets accelerate scikit-learn python-dotenv wandb\",\n",
       "#     # f'huggingface-cli login --token {os.environ[\"HUGGING_FACE_ACCESS_TOKEN\"]}',\n",
       "#     f'wandb login  {os.environ[\"WANDB_API_KEY\"]}',\n",
       "# )\n",
       "\n",
       "# Flash-Attn Image\n",
       "# https://modal.com/docs/guide/cuda#for-more-complex-setups-use-an-officially-supported-cuda-image\n",
       "cuda_version = \"12.4.0\"  # should be no greater than host CUDA version\n",
       "flavor = \"devel\"  #  includes full CUDA toolkit\n",
       "operating_sys = \"ubuntu22.04\"\n",
       "tag = f\"{cuda_version}-{flavor}-{operating_sys}\"\n",
       "\n",
       "image = (\n",
       "    modal.Image.from_registry(f\"nvidia/cuda:{tag}\", add_python=\"3.11\")\n",
       "    .apt_install(\"git\", \"htop\")\n",
       "    .pip_install(\n",
       "        \"ninja\",  # required to build flash-attn\n",
       "        \"packaging\",  # required to build flash-attn\n",
       "        \"wheel\",  # required to build flash-attn\n",
       "        \"torch\",\n",
       "        \"git+https://github.com/huggingface/transformers.git\",\n",
       "        \"datasets\",\n",
       "        \"accelerate\",\n",
       "        \"scikit-learn\",\n",
       "        \"python-dotenv\",\n",
       "        \"wandb\",\n",
       "    )\n",
       "    .run_commands(\n",
       "        \"pip install flash-attn --no-build-isolation\",  # add flash-attn\n",
       "        f'wandb login  {os.environ[\"WANDB_API_KEY\"]}',\n",
       "    )\n",
       ")\n",
       "\n",
       "vol = modal.Volume.from_name(\"trainer-vol\", create_if_missing=True)\n",
       "\n",
       "\n",
       "@app.cls(\n",
       "    image=image,\n",
       "    volumes={\"/data\": vol},\n",
       "    secrets=[modal.Secret.from_dotenv(filename=env_file)],\n",
       "    gpu=GPU_SIZE,\n",
       "    timeout=60 * 60 * 10,\n",
       "    container_idle_timeout=300,\n",
       ")\n",
       "class Trainer:\n",
       "    def __init__(self, reload_ds=True):\n",
       "        import torch\n",
       "\n",
       "        self.reload_ds = reload_ds\n",
       "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
       "\n",
       "    @build()\n",
       "    @enter()\n",
       "    def setup(self):\n",
       "        from datasets import load_dataset, load_from_disk\n",
       "        from transformers import (\n",
       "            AutoTokenizer,\n",
       "        )\n",
       "        from transformers.utils import move_cache\n",
       "\n",
       "        os.makedirs(\"/data\", exist_ok=True)\n",
       "\n",
       "        if not os.path.exists(path_to_ds) or self.reload_ds:\n",
       "            try:\n",
       "                # clean out the dataset folder\n",
       "                shutil.rmtree(path_to_ds)\n",
       "            except FileNotFoundError:\n",
       "                pass\n",
       "            self.ds = load_dataset(ds_name, ds_name_config)\n",
       "            # Save dataset to disk\n",
       "            self.ds.save_to_disk(path_to_ds)\n",
       "        else:\n",
       "            self.ds = load_from_disk(path_to_ds)\n",
       "\n",
       "        move_cache()\n",
       "\n",
       "        # Load tokenizer and model\n",
       "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
       "\n",
       "    def tokenize_function(self, example):\n",
       "        return tokenizer_function_logic(example, self.tokenizer)\n",
       "\n",
       "    def compute_metrics(self, pred):\n",
       "        \"\"\"\n",
       "        To debug this function manually on some sample input in ipython you can create an input\n",
       "        pred object like this:\n",
       "        from transformers import EvalPrediction\n",
       "        import numpy as np\n",
       "        logits=[[-0.9559,  0.7553],\n",
       "        [ 2.0987, -2.3868],\n",
       "        [ 1.0143, -1.1551],\n",
       "        [ 1.3666, -1.6074]]\n",
       "        label_ids = [1, 0, 1, 0]\n",
       "        pred = EvalPrediction(predictions=logits, label_ids=label_ids)\n",
       "        \"\"\"\n",
       "        import numpy as np\n",
       "        import torch\n",
       "        from sklearn.metrics import f1_score\n",
       "\n",
       "        # pred is EvalPrediction object i.e. from transformers import EvalPrediction\n",
       "        logits = torch.tensor(pred.predictions)  # raw prediction logits from the model\n",
       "        label_ids = pred.label_ids  # integer label ids classes\n",
       "        labels = torch.tensor(label_ids).double().numpy()\n",
       "\n",
       "        probs = logits.softmax(dim=-1).float().numpy()  # probabilities for each class\n",
       "        preds = np.argmax(probs, axis=1)  # take the label with the highest probability\n",
       "        f1_micro = f1_score(labels, preds, average=\"micro\", zero_division=True)\n",
       "        f1_macro = f1_score(labels, preds, average=\"macro\", zero_division=True)\n",
       "        return {\"f1_micro\": f1_micro, \"f1_macro\": f1_macro}\n",
       "\n",
       "    @modal.method()\n",
       "    def train_model(self):\n",
       "        import wandb\n",
       "        import torch\n",
       "        import os\n",
       "        from datasets import load_from_disk\n",
       "        from transformers import (\n",
       "            AutoConfig,\n",
       "            AutoModelForSequenceClassification,\n",
       "            DataCollatorWithPadding,\n",
       "            Trainer,\n",
       "            TrainingArguments,\n",
       "        )\n",
       "\n",
       "        os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
       "        # Remove previous training model saves if exists for same run_name\n",
       "        try:\n",
       "            shutil.rmtree(os.path.join(\"/data\", run_name))\n",
       "        except FileNotFoundError:\n",
       "            pass\n",
       "\n",
       "        ds = load_from_disk(path_to_ds)\n",
       "        # useful for debugging and quick training: Just downsample the dataset\n",
       "        # for split in ds.keys():\n",
       "        #     ds[split] = ds[split].shuffle(seed=42).select(range(1000))\n",
       "        num_labels = len(id2label)\n",
       "        tokenized_dataset = ds.map(self.tokenize_function, batched=True)\n",
       "        if label_column != \"label\":\n",
       "            tokenized_dataset = tokenized_dataset.rename_column(label_column, \"label\")\n",
       "        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
       "\n",
       "        # https://www.philschmid.de/getting-started-pytorch-2-0-transformers\n",
       "        # https://www.philschmid.de/fine-tune-modern-bert-in-2025\n",
       "        training_args = TrainingArguments(\n",
       "            output_dir=os.path.join(\"/data\", run_name),\n",
       "            num_train_epochs=num_train_epochs,\n",
       "            learning_rate=learning_rate,\n",
       "            per_device_train_batch_size=batch_size,\n",
       "            per_device_eval_batch_size=batch_size,\n",
       "            # PyTorch 2.0 specifics\n",
       "            bf16=True,  # bfloat16 training\n",
       "            # torch_compile=True,  # optimizations but its making it slower with my code and causes errors when running with flash-attn\n",
       "            optim=\"adamw_torch_fused\",  # improved optimizer\n",
       "            # logging & evaluation strategies\n",
       "            logging_dir=os.path.join(\"/data\", run_name, \"logs\"),\n",
       "            logging_strategy=\"steps\",\n",
       "            logging_steps=200,\n",
       "            eval_strategy=\"epoch\",\n",
       "            save_strategy=\"epoch\",\n",
       "            load_best_model_at_end=True,\n",
       "            metric_for_best_model=\"f1_macro\",\n",
       "            report_to=\"wandb\",\n",
       "            run_name=run_name,\n",
       "        )\n",
       "\n",
       "        configuration = AutoConfig.from_pretrained(checkpoint)\n",
       "        # these dropout values are noted here in case we want to tweak them in future\n",
       "        # experiments.\n",
       "        # configuration.hidden_dropout_prob = 0.1  # 0.1 is default\n",
       "        # configuration.attention_probs_dropout_prob = 0.1  # 0.1 is default\n",
       "        # configuration.classifier_dropout = None  # If None then defaults to hidden_dropout_prob\n",
       "        configuration.id2label = id2label\n",
       "        configuration.label2id = label2id\n",
       "        configuration.num_labels = num_labels\n",
       "        model = AutoModelForSequenceClassification.from_pretrained(\n",
       "            checkpoint,\n",
       "            config=configuration,\n",
       "            # TODO: Is this how to use flash-attn 2?\n",
       "            # attn_implementation=\"flash_attention_2\",\n",
       "            # torch_dtype=torch.bfloat16,\n",
       "        )\n",
       "\n",
       "        trainer = Trainer(\n",
       "            model,\n",
       "            training_args,\n",
       "            train_dataset=tokenized_dataset[train_split],\n",
       "            eval_dataset=tokenized_dataset[validation_split],\n",
       "            data_collator=data_collator,\n",
       "            tokenizer=self.tokenizer,\n",
       "            compute_metrics=self.compute_metrics,\n",
       "        )\n",
       "\n",
       "        trainer.train()\n",
       "\n",
       "        # Log the trainer script\n",
       "        wandb.save(__file__)\n",
       "\n",
       "    def load_model(self, check_point):\n",
       "        from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
       "        import torch\n",
       "\n",
       "        model = AutoModelForSequenceClassification.from_pretrained(\n",
       "            check_point,\n",
       "            # TODO: Is this how to use flash-attn 2?\n",
       "            # attn_implementation=\"flash_attention_2\",\n",
       "            # torch_dtype=torch.bfloat16,\n",
       "        )\n",
       "        tokenizer = AutoTokenizer.from_pretrained(check_point)\n",
       "        return tokenizer, model\n",
       "\n",
       "    @modal.method()\n",
       "    def eval_model(self, check_point=None, split=validation_split):\n",
       "        import os\n",
       "        import numpy as np\n",
       "        import pandas as pd\n",
       "        import torch\n",
       "        import wandb\n",
       "        from datasets import load_from_disk\n",
       "        from sklearn.metrics import classification_report\n",
       "\n",
       "        os.environ[\"WANDB_PROJECT\"] = wandb_project\n",
       "        if check_point is None:\n",
       "            # Will use most recent checkpoint by default. It may not be the \"best\" checkpoint/model.\n",
       "            check_points = sorted(\n",
       "                os.listdir(os.path.join(\"/data/\", run_name)), key=lambda x: int(x.split(\"-\")[1]) if x.startswith(\"checkpoint-\") else 0\n",
       "            )\n",
       "            check_point = os.path.join(\"/data\", run_name, check_points[-1])\n",
       "        print(f\"Evaluating Checkpoint {check_point}, split {split}\")\n",
       "\n",
       "        tokenizer, model = self.load_model(check_point)\n",
       "\n",
       "        def tokenize_function(example):\n",
       "            return tokenizer_function_logic(example, tokenizer)\n",
       "\n",
       "        model.to(self.device)\n",
       "        test_ds = load_from_disk(path_to_ds)[split]\n",
       "\n",
       "        test_ds = test_ds.map(tokenize_function, batched=True, batch_size=batch_size)\n",
       "        if label_column != \"label\":\n",
       "            test_ds = test_ds.rename_column(label_column, \"label\")\n",
       "\n",
       "        def forward_pass(batch):\n",
       "            \"\"\"\n",
       "            To debug this function manually on some sample input in ipython, take your dataset\n",
       "            that has already been tokenized and create a batch object with this code:\n",
       "            batch_size = 32\n",
       "            test_ds.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
       "            small_ds = test_ds.take(batch_size)\n",
       "            batch = {k: torch.stack([example[k] for example in small_ds]) for k in small_ds[0].keys()}\n",
       "            \"\"\"\n",
       "            inputs = {k: v.to(self.device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
       "            with torch.no_grad():\n",
       "                output = model(**inputs)\n",
       "                probs = torch.softmax(output.logits, dim=-1).round(decimals=2)\n",
       "                probs = probs.float()  # convert to float32 only for numpy compatibility. # TODO: Related to using flash-attn 2\n",
       "            return {\"probs\": probs.cpu().numpy()}\n",
       "\n",
       "        test_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
       "        test_ds = test_ds.map(forward_pass, batched=True, batch_size=batch_size)\n",
       "\n",
       "        test_ds.set_format(\"pandas\")\n",
       "        df_test = test_ds[:]\n",
       "\n",
       "        def pred_label(probs, threshold):\n",
       "            # probs is a list of probabilities for one row of the dataframe\n",
       "            probs = np.array(probs)\n",
       "            max_prob = np.max(probs)\n",
       "            predicted_class = np.argmax(probs)\n",
       "\n",
       "            if max_prob < threshold:\n",
       "                return unknown_label_int\n",
       "\n",
       "            return predicted_class\n",
       "\n",
       "        for threshold in [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
       "            print(\"-\" * 60)\n",
       "            print(f\"{threshold=}\\n\")\n",
       "            df_test[f\"pred_label\"] = df_test[\"probs\"].apply(pred_label, args=(threshold,))\n",
       "            print(f\"Coverage Rate:\\n\")\n",
       "            predictions_mapped = df_test[f\"pred_label\"].map({**id2label, unknown_label_int: unknown_label_str})\n",
       "            print(\"Raw counts:\")\n",
       "            print(predictions_mapped.value_counts())\n",
       "            print(\"\\nProportions:\\n\")\n",
       "            print(predictions_mapped.value_counts(normalize=True))\n",
       "            print(f\"\\nConditional metrics (classification report on predicted subset != {unknown_label_str})\")\n",
       "            mask = df_test[f\"pred_label\"] != unknown_label_int\n",
       "            y = np.array([x for x in df_test[mask][\"label\"].values])\n",
       "            y_pred = np.array([x for x in df_test[mask][f\"pred_label\"].values])\n",
       "            report = classification_report(\n",
       "                y,\n",
       "                y_pred,\n",
       "                target_names=[k for k, v in sorted(label2id.items(), key=lambda item: item[1])],\n",
       "                digits=2,\n",
       "                zero_division=0,\n",
       "                output_dict=False,\n",
       "                labels=sorted(list(range(len(id2label)))),\n",
       "            )\n",
       "            print(report)\n",
       "            # --- Overall Accuracy (count \"Unknown\" as incorrect) ---\n",
       "            # If ground truth is never 'unknown_label_int', then any prediction of \"Unknown\" is automatically wrong.\n",
       "            overall_acc = (df_test[\"label\"] == df_test[f\"pred_label\"]).mean()\n",
       "            print(f\"Overall Accuracy (counting '{unknown_label_str}' as wrong): {overall_acc:.2%}\")\n",
       "            print(\"-\" * 60)\n",
       "\n",
       "        print(\"Probability Distribution Max Probability Across All Classes\")\n",
       "        print(pd.DataFrame([max(x) for x in df_test[\"probs\"]]).describe())\n",
       "        # Ensure wandb is finished\n",
       "        wandb.finish()\n",
       "\n",
       "\n",
       "@app.local_entrypoint()\n",
       "def main():\n",
       "    trainer = Trainer(reload_ds=True)\n",
       "\n",
       "    print(f\"Training {run_name}\")\n",
       "    trainer.train_model.remote()\n",
       "\n",
       "    # Will use most recent checkpoint by default. It may not be the \"best\" checkpoint/model.\n",
       "    # Write the full path to the checkpoint here if you want to evaluate a specific model.\n",
       "    # For example: check_point = '/data/run_name/checkpoint-1234/'\n",
       "    check_point = None\n",
       "    trainer.eval_model.remote(\n",
       "        check_point=check_point,\n",
       "        split=validation_split,\n",
       "    )\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | echo: false\n",
    "file_path = \"trainer.py\"\n",
    "markdown_content = import_python_as_markdown(file_path)\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae7471",
   "metadata": {},
   "source": [
    "## Run The Trainer\n",
    "\n",
    "All of these training runs can be executed from the command line by running `modal run trainer.py`\n",
    "after making minor edits to the `trainer.py` file. You can even run them all in parallel,\n",
    "because Modal will take care of spinning up the containers and running the code!\n",
    "\n",
    "Here are some random screen shots from the Modal UI dashboard showing containers, GPU metrics, volumes for storing datasets and checkpoints, and log outputs.\n",
    "\n",
    "![](static_blog_imgs/containers.jpg)\n",
    "\n",
    "![](static_blog_imgs/gpu_metrics.jpg)\n",
    "\n",
    "![](static_blog_imgs/volumes1.jpg)\n",
    "\n",
    "![](static_blog_imgs/volumes2.jpg)\n",
    "\n",
    "![](static_blog_imgs/volumes3.jpg)\n",
    "\n",
    "![](static_blog_imgs/logs1.jpg)\n",
    "\n",
    "![](static_blog_imgs/logs2.jpg)\n",
    "\n",
    "![](static_blog_imgs/logs3.jpg)\n",
    "\n",
    "Here are some screen shots from the wandb dashboard. There are public wandb runs for each of the training runs below.\n",
    "\n",
    "![](static_blog_imgs/wandb1.jpg)\n",
    "\n",
    "![](static_blog_imgs/wandb2.jpg)\n",
    "\n",
    "![](static_blog_imgs/wandb3.jpg)\n",
    "\n",
    "\n",
    "### Emotion Dataset\n",
    "\n",
    "By default the trainer will use the `\"dair-ai/emotion\"` dataset which predicts the emotion of a text.\n",
    "\n",
    "- [wandb run](https://wandb.ai/christopherdavidlevy/hugging_face_training_jobs/runs/4oie0k1m?nw=nwuserchristopherdavidlevy)\n",
    "\n",
    "```\n",
    "modal run --detach trainer.py\n",
    "```\n",
    "\n",
    "### AG News Dataset\n",
    "\n",
    "You can easily switch to a different dataset, in this case I used the `\"fancyzhx/ag_news\"` dataset.\n",
    "All I switched in the `trainer.py` file were these lines:\n",
    "\n",
    "```python\n",
    "ds_name = \"fancyzhx/ag_news\" \n",
    "id2label = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n",
    "validation_split = \"test\"\n",
    "```\n",
    "\n",
    "```\n",
    "modal run --detach trainer.py\n",
    "```\n",
    "\n",
    "- [wandb run](https://wandb.ai/christopherdavidlevy/hugging_face_training_jobs/runs/ljhq0fgu?nw=nwuserchristopherdavidlevy)\n",
    "\n",
    "\n",
    "\n",
    "### TweetEval Dataset\n",
    "\n",
    "\n",
    "#### Sentiment\n",
    "\n",
    "Make these edits to the `trainer.py` file:\n",
    "\n",
    "```python\n",
    "ds_name = \"cardiffnlp/tweet_eval\"\n",
    "ds_name_config = \"sentiment\" \n",
    "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "```\n",
    "\n",
    "```\n",
    "modal run --detach trainer.py\n",
    "```\n",
    "\n",
    "- [wandb run](https://wandb.ai/christopherdavidlevy/hugging_face_training_jobs/runs/gmypi9sy?nw=nwuserchristopherdavidlevy)\n",
    "\n",
    "\n",
    "\n",
    "#### Irony\n",
    "\n",
    "Make these edits to the `trainer.py` file:\n",
    "\n",
    "```python   \n",
    "ds_name = \"cardiffnlp/tweet_eval\"\n",
    "ds_name_config = \"irony\" \n",
    "id2label = {0: 'non_irony', 1: 'irony'}\n",
    "```\n",
    "\n",
    "```\n",
    "modal run --detach trainer.py\n",
    "```\n",
    "\n",
    "- [wandb run](https://wandb.ai/christopherdavidlevy/hugging_face_training_jobs/runs/u5qn98j0?nw=nwuserchristopherdavidlevy)\n",
    "\n",
    "\n",
    "\n",
    "### Yahoo Answers Topics\n",
    "\n",
    "Make these edits to the `trainer.py` file:\n",
    "\n",
    "```python\n",
    "ds_name = \"community-datasets/yahoo_answers_topics\"\n",
    "id2label = {\n",
    "    0: \"Society & Culture\",\n",
    "    1: \"Science & Mathematics\",\n",
    "    2: \"Health\",\n",
    "    3: \"Education & Reference\",\n",
    "    4: \"Computers & Internet\",\n",
    "    5: \"Sports\",\n",
    "    6: \"Business & Finance\",\n",
    "    7: \"Entertainment & Music\",\n",
    "    8: \"Family & Relationships\",\n",
    "    9: \"Politics & Government\",\n",
    "}\n",
    "validation_split = \"test\"\n",
    "input_column = 'question_title'\n",
    "label_column = 'topic'\n",
    "```\n",
    "\n",
    "```\n",
    "modal run --detach trainer.py\n",
    "```\n",
    "\n",
    "- [wandb run](https://wandb.ai/christopherdavidlevy/hugging_face_training_jobs/runs/atxplydb?nw=nwuserchristopherdavidlevy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de924f2",
   "metadata": {},
   "source": [
    "### Synthetic Dataset With Longer Texts\n",
    "\n",
    "To test out the longer context window of ModernBERT, I created a synthetic dataset with longer texts.\n",
    "These texts consists of synthetic social media posts. For each row in the dataset there is a list of input tweets concatenated together and a corresponding label. I made this dataset using various LLMS such as `gpt-4o-mini`, `claude-3-5-sonnet-20241022`, `gemini-2.0-flash-exp`, and `deepseek-chat-v3`. The prompts for creating the dataset are in the `prompts.py` file which can be found [here](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/modern_bert/prompts.py). The system prompt was also crafted mostly by an LLM and I made some minor edits to it. The dataset is just a toy dataset and should not be used for anything serious. It probably has issues since I hacked it together rather quickly.\n",
    "\n",
    "I uploaded the dataset to the Hugging Face Hub and you can find it [here](https://huggingface.co/datasets/chrislevy/synthetic_social_persona_tweets).\n",
    "\n",
    "To run the trainer on this dataset make these edits to the `trainer.py` file:\n",
    "\n",
    "```python\n",
    "# I changed the tokenizer function to use a max length of 3000 tokens\n",
    "def tokenizer_function_logic(example, tokenizer):\n",
    "    return tokenizer(example[input_column], padding=True, truncation=True, return_tensors=\"pt\", max_length=3000)\n",
    "\n",
    "\n",
    "ds_name = \"chrislevy/synthetic_social_persona_tweets\"\n",
    "id2label = {0: 'Tech Industry Analysis', 1: 'Software Engineering', 2: 'Frontend Development', 3: 'Data Analytics', 4: 'AI & Machine Learning', 5: 'Cybersecurity News', 6: 'Cryptocurrency & Web3', 7: 'Web3 Innovation', 8: 'NFT Trading', 9: 'Startup Ecosystem', 10: 'Venture Capital Analysis', 11: 'Paid Advertising', 12: 'Content Marketing', 13: 'Ecommerce Innovation', 14: 'Business Leadership', 15: 'Product Management', 16: 'Fintech Discussion', 17: 'Sales Strategy', 18: 'Tech Entrepreneurship', 19: 'US Politics Analysis', 20: 'Global Affairs Commentary', 21: 'Electoral Politics', 22: 'Political Commentary', 23: 'Legal System Analysis', 24: 'Military & Defense', 25: 'Climate Change Discussion', 26: 'Economic Policy', 27: 'Political Satire', 28: 'Local Community News', 29: 'Film & Cinema Analysis', 30: 'TV Series Discussion', 31: 'Reality TV Commentary', 32: 'Music Industry Analysis', 33: 'Video Content Creation', 34: 'Video Game Enthusiast', 35: 'Competitive Gaming', 36: 'Indie Game Dev', 37: 'Anime & Manga Community', 38: 'Comics & Graphic Novels', 39: 'Celebrity Commentary', 40: 'Fashion & Streetwear', 41: 'Sneaker Culture', 42: 'Book & Literature', 43: 'Podcast Creation', 44: 'Entertainment Industry', 45: 'Live Music Fan', 46: 'NFL Analysis', 47: 'NBA Discussion', 48: 'MLB Commentary', 49: 'Soccer Coverage', 50: 'Formula 1 Community', 51: 'College Sports Analysis', 52: 'MMA & Boxing', 53: 'Weightlifting Training', 54: 'Fitness Training', 55: 'Endurance Sports', 56: 'Sports Betting', 57: 'Olympics Coverage', 58: 'Space Exploration', 59: 'Biology Research', 60: 'Physics Discussion', 61: 'Health & Medicine', 62: 'EdTech Innovation', 63: 'Historical Analysis', 64: 'Psychology Research', 65: 'Environmental Science', 66: 'Earth Sciences', 67: 'Academic Research', 68: 'Travel Photography', 69: 'Food & Cooking', 70: 'Professional Photography', 71: 'Amateur Photography', 72: 'Home Improvement', 73: 'Home Gardening', 74: 'Investment Strategy', 75: 'Personal Investing', 76: 'Pet Community', 77: 'Meditation Practice', 78: 'Digital Art', 79: 'Visual Arts', 80: 'Automotive Culture', 81: 'Craft Beer Culture', 82: 'Coffee Enthusiasm', 83: 'Culinary Arts', 84: 'Parenting Discussion', 85: 'Mental Health Support', 86: 'Spiritual Practice', 87: 'Philosophy Discussion', 88: 'Urban Culture', 89: 'Vintage Collection', 90: 'DIY Crafts', 91: 'Language Learning', 92: 'Open Source Coding', 93: 'Personal Development', 94: 'Minimalist Living', 95: 'Sustainable Living', 96: 'Fiction Writing', 97: 'Conspiracy Theories', 98: 'Fan Culture', 99: 'Internet Culture', 100: 'Outdoor Adventure', 101: 'Alternative Lifestyle', 102: 'Twitter Meta Commentary', 103: 'Meme Creation', 104: 'Viral Content', 105: 'Personal Updates', 106: 'Social Commentary', 107: 'Community Building', 108: 'Twitter Spaces Hosting', 109: 'Platform Critique', 110: 'Bot & Automation', 111: 'Online Privacy', 112: 'Data Visualization'}\n",
    "\n",
    "```\n",
    "\n",
    "I also changed the logging steps for this run only `logging_steps=20,`. \n",
    "\n",
    "```\n",
    "modal run --detach trainer.py\n",
    "```\n",
    "\n",
    "- [wandb run](https://wandb.ai/christopherdavidlevy/hugging_face_training_jobs/runs/b2nb4sry?nw=nwuserchristopherdavidlevy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d293f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "I hope this code can start as a launching point for your own fine-tuning experiments with encoder models and ModernBERT.\n",
    "If you were not familiar with Modal, I hope this shows you how easy it is to get started.\n",
    "I think minor changes may be needed to get this training with flash attention 2. You will see some commented out parts of my code\n",
    "with regards to choosing `attn_implementation=\"flash_attention_2\"`. I'm not sure if that is needed or not. I think I am\n",
    "installing the flash attention 2 package but I'm not sure if it's being used during training. If anyone knows, hit up on [X](https://x.com/cleavey1985). \n",
    "I did try running different variations but couldn't really see how to tell if it was all running properly or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989dae99",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "[Announcement from Jeremy Howard on X](https://x.com/jeremyphoward/status/1869786023963832509)\n",
    "\n",
    "[Blog Post on Hugging Face](https://huggingface.co/blog/modernbert)\n",
    "\n",
    "[Modal](https://modal.com/)\n",
    "\n",
    "[Fine-tune classifier with ModernBERT in 2025 Blog by Philipp Schmid](https://www.philschmid.de/fine-tune-modern-bert-in-2025)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd94bad1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
