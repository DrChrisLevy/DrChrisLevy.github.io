{
 "cells": [
  {
   "cell_type": "raw",
   "id": "91affc592a972695",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "title: Getting Started with Axolotl for Fine-Tuning LLMs\n",
    "author: Chris Levy\n",
    "date: '2024-04-11'\n",
    "date-modified: '2024-04-11'\n",
    "image: imgs/deep-life.jpeg\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: show\n",
    "    page-layout: full\n",
    "include-in-header:\n",
    "  - text: |\n",
    "      <style>\n",
    "      .cell-output-stdout code {\n",
    "        word-break: break-wor !important;\n",
    "        white-space: pre-wrap !important;\n",
    "      }\n",
    "      </style>\n",
    "bibliography: ../../bibliography.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640d3024a40be2aa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Intro\n",
    "\n",
    "I have experience fine-tuning smaller encoder style LLMs such as [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) for classification style tasks. \n",
    "I have deployed such models in a production environment and have had great success with them. The [Hugging Face trainer](https://huggingface.co/docs/transformers/en/tasks/sequence_classification) class makes it relatively easy and the models are easy to deploy, even on CPU based infrastructure. \n",
    "\n",
    "But when it comes to training decoder style LLMs for text generation (GPT, [llama](https://huggingface.co/docs/transformers/en/model_doc/llama2), [Mistral](https://mistral.ai/), [Qwen](https://github.com/QwenLM/Qwen?tab=readme-ov-file), [Gemma](https://huggingface.co/blog/gemma), etc.), I will be the first to admit that I am a complete noob. It's something I have followed from a distance, trying to keep up with the recent methods/libraries, but I have not had any experience with it in terms of actual hands on practice.\n",
    "\n",
    "I don't want to get into the debate on whether [Fine-Tuning LLMs is valuable](https://hamel.dev/blog/posts/fine_tuning_valuable.html). The answer is probably most likely, \"it depends\". It's something I want to learn more about, and just want to get started. There are so many new terms/ideas to learn (PEFT, LORA, QLORA, TRL, RHLF, DPO, etc.). My academic mathematical background says to start from the bottom and learn everything along the way. I know that is horrible advice in practice though because I will just get stuck. Instead, I will take advice from [Jeremy Howard](https://twitter.com/jeremyphoward) which is to begin at the top and just get started. \n",
    "\n",
    "So in this post I will fine-tune my first model with the [axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) library. The model will probably suck, but that's not the point. Whenever I learn a new tool/library the first thing is to set things up and run a \"hello world\" type example. Even if it's just copy/paste. That's what I will do here. Don't come here for advanced advice. Follow along if you are in a similar situation as me and just want to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc54d3d8d068332",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Why use axolotl?\n",
    "\n",
    "Because [Jeremy Howard](https://x.com/jeremyphoward/status/1766226347247206720) told me to.\n",
    "Checkout the [README](https://github.com/OpenAccess-AI-Collective/axolotl?tab=readme-ov-file#axolotl). \n",
    "It has an active community behind it, other fine-tuning experts seem to be using it, and you only need to work with a single yaml file.\n",
    "\n",
    "# Environment\n",
    "\n",
    "If you don't have access to a GPU locally, do yourself a favour and just start with the GPU cloud provider [RunPod](https://www.runpod.io/console/pods).\n",
    "I find it simple to use and there is always a GPU available. There is already a configured axolotl [docker container](https://www.runpod.io/console/deploy?template=v2ickqhz9s) as well. I'm only going to fine-tune a smaller 7B model, therefore I chose a RTX 4090.\n",
    "\n",
    "\n",
    "::: {#create-pod layout-ncol=2}\n",
    "![](static_blog_imgs/docker1.png)\n",
    "\n",
    "![](static_blog_imgs/docker2.png)\n",
    ":::\n",
    "\n",
    "Find your running Pod and click **connect** and SSH in\n",
    "\n",
    "![](static_blog_imgs/ssh.png)\n",
    "\n",
    "You will see that there is a tmux for persistent shell sessions being applied. This means that when you shell in and out all your live changes are still there.\n",
    "This is personal preference, but I prefer to have this turned off. I like to have multiple shells opened and do different things in each of them.\n",
    "\n",
    "```\n",
    "vim ~/.bashrc\n",
    "```\n",
    "Then comment out the line near the bottom\n",
    "```\n",
    "#[[ -z \"$TMUX\"  ]] && { tmux attach-session -t ssh_tmux || tmux new-session -s ssh_tmux; exit; }\n",
    "```\n",
    "\n",
    "There is already a directory volume with lots of disk space set up at `/workspace`. You can see that from running `df -h`.\n",
    "\n",
    "```bash\n",
    "cd /workspace\n",
    "rm -rf axolotl/\n",
    "git clone https://github.com/OpenAccess-AI-Collective/axolotl\n",
    "cd axolotl\n",
    "\n",
    "pip3 install packaging ninja\n",
    "pip3 install -e '.[flash-attn,deepspeed]'\n",
    "```\n",
    "\n",
    "Make sure you have accounts/tokens with [Weights and Biases](https://wandb.ai/home) and [Hugging Face](https://huggingface.co/).\n",
    "Set up the integration/connection:\n",
    "\n",
    "```bash\n",
    "wandb login\n",
    "huggingface-cli login\n",
    "```\n",
    "\n",
    "Because I turned off the tmux, I prefer just to use `screen` for longing running tasks/jobs in the terminal.\n",
    "This is completely optional.\n",
    "\n",
    "```bash\n",
    "apt-get install screen -y\n",
    "```\n",
    "\n",
    "I also want to have access to S3 incase I want to copy any data to and from there. This is optional.\n",
    "```bash\n",
    "aws configure\n",
    "```\n",
    "\n",
    "One other thing to configure is the directories where the pre-trained models and/or datasets get stored\n",
    "when downloading from Hugging Face. I ran into an issue before where the cache was on a volume with very little\n",
    "storage and I ran out of disk space. Just make sure everything goes to the `/workspace` directory because it has a lot of space.\n",
    "\n",
    "```bash\n",
    "export HF_DATASETS_CACHE=\"/workspace/data/huggingface-cache/datasets\"\n",
    "export HUGGINGFACE_HUB_CACHE=\"/workspace/data/huggingface-cache/hub\"\n",
    "export TRANSFORMERS_CACHE=\"/workspace/data/huggingface-cache/hub\"\n",
    "export HF_HOME=\"/workspace/data/huggingface-cache/hub\"\n",
    "export HF_HUB_ENABLE_HF_TRANSFER=\"1\"\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901049a7835efb82",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Text Completion Example\n",
    "\n",
    "## Configuring The YAML File\n",
    "\n",
    "There are general guidelines in the [README](https://github.com/OpenAccess-AI-Collective/axolotl).\n",
    "The idea is to start with one of the example YAML files [here](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples).\n",
    "I wanted to fine-tune mistral-7b using QLORA, so I started with this YAML file [here](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/examples/mistral/qlora.yml). I `git clone` the axolotl repo locally and open the code in my Pycharm IDE. Then I simply start editing the file `examples/mistral/qlora.yml` directly. That way I can easily see what changes I made with git diff annotations.\n",
    "\n",
    "The only lines I changed were\n",
    "\n",
    "```yaml\n",
    "datasets:\n",
    "  - path: data.jsonl\n",
    "    type: completion\n",
    "sequence_len: 1000\n",
    "eval_sample_packing: False\n",
    "wandb_project: cal-train\n",
    "num_epochs: 3\n",
    "evals_per_epoch: 1\n",
    "eval_max_new_tokens: 1000\n",
    "```\n",
    "\n",
    "I wanted to start with text completion, not instruction fine-tuning. I will try instruction fine-tuning later.\n",
    "I created a small dataset with some transcripts from some of the [Cal Newport](https://www.youtube.com/@CalNewportMedia) Podcast episodes.\n",
    "Either way, this is the part where you need to bring your own dataset to fine-tune the model on. It's important to choose the correct dataset \n",
    "[format](https://github.com/OpenAccess-AI-Collective/axolotl?tab=readme-ov-file#dataset) and configure it properly in the YAML.\n",
    "My dataset is in a `data.jsonl` file at the root of the `axolotl` repo. It looks like this:\n",
    "\n",
    "```\n",
    "{\"text\": \" I'm Cal Newport and this is Deep Questions Episode 185. I'm here in my deep work HQ along with my producer Jesse. ......\"}\n",
    "{\"text\": \"Alright, our next question is from Vinny. Vinny asks, how should I adjust my approach to hourly billing .....\"}\n",
    "```\n",
    "\n",
    "Each record is around 500 words long. That is why I chose `sequence_len` to be 1000 which is counting tokens (not words). I have about 4000 rows\n",
    "like this in my dataset. Each record is a random excerpt from a podcast transcription.\n",
    "\n",
    "The complete YAML looks like\n",
    "\n",
    "```yaml\n",
    "base_model: mistralai/Mistral-7B-v0.1\n",
    "model_type: MistralForCausalLM\n",
    "tokenizer_type: LlamaTokenizer\n",
    "\n",
    "load_in_8bit: false\n",
    "load_in_4bit: true\n",
    "strict: false\n",
    "\n",
    "datasets:\n",
    "  - path: data.jsonl\n",
    "    type: completion\n",
    "dataset_prepared_path: last_run_prepared\n",
    "val_set_size: 0.1\n",
    "output_dir: ./qlora-out\n",
    "\n",
    "adapter: qlora\n",
    "lora_model_dir:\n",
    "\n",
    "sequence_len: 1000\n",
    "sample_packing: true\n",
    "eval_sample_packing: False\n",
    "pad_to_sequence_len: true\n",
    "\n",
    "lora_r: 32\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.05\n",
    "lora_target_linear: true\n",
    "lora_fan_in_fan_out:\n",
    "lora_target_modules:\n",
    "  - gate_proj\n",
    "  - down_proj\n",
    "  - up_proj\n",
    "  - q_proj\n",
    "  - v_proj\n",
    "  - k_proj\n",
    "  - o_proj\n",
    "\n",
    "wandb_project: cal-train\n",
    "wandb_entity:\n",
    "wandb_watch:\n",
    "wandb_name:\n",
    "wandb_log_model:\n",
    "\n",
    "gradient_accumulation_steps: 4\n",
    "micro_batch_size: 2\n",
    "num_epochs: 3\n",
    "optimizer: adamw_bnb_8bit\n",
    "lr_scheduler: cosine\n",
    "learning_rate: 0.0002\n",
    "\n",
    "train_on_inputs: false\n",
    "group_by_length: false\n",
    "bf16: auto\n",
    "fp16:\n",
    "tf32: false\n",
    "\n",
    "gradient_checkpointing: true\n",
    "early_stopping_patience:\n",
    "resume_from_checkpoint:\n",
    "local_rank:\n",
    "logging_steps: 1\n",
    "xformers_attention:\n",
    "flash_attention: true\n",
    "\n",
    "loss_watchdog_threshold: 5.0\n",
    "loss_watchdog_patience: 3\n",
    "\n",
    "warmup_steps: 10\n",
    "evals_per_epoch: 1\n",
    "eval_table_size:\n",
    "eval_max_new_tokens: 1000\n",
    "saves_per_epoch: 1\n",
    "debug:\n",
    "deepspeed:\n",
    "weight_decay: 0.0\n",
    "fsdp:\n",
    "fsdp_config:\n",
    "special_tokens:\n",
    "```\n",
    "\n",
    "There is much more I need to learn about the parameters but that will slow me down. Therefore, I'm simply sticking with the defaults.\n",
    "\n",
    "Put the file on the GPU server/machine. I put it at the root.\n",
    "\n",
    "```bash\n",
    "vim qlora.yml # copy/paste in your config\n",
    "```\n",
    "\n",
    "## Pre-Processing the Dataset\n",
    "\n",
    "The docs say to run \n",
    "\n",
    "```bash\n",
    "CUDA_VISIBLE_DEVICES=\"\" python -m axolotl.cli.preprocess qlora.yml\n",
    "```\n",
    "to pre-process the dataset.\n",
    "\n",
    "By default, it puts the processed dataset in `dataset_prepared_path: last_run_prepared`.\n",
    "\n",
    "```bash\n",
    "ls last_run_prepared\n",
    "```\n",
    "\n",
    "It's good to take a look at the data in there. See tips on debugging [here](https://openaccess-ai-collective.github.io/axolotl/docs/input_output.html#check-the-prompts).\n",
    "\n",
    "Just drop into an `ipython` shell and run this:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "import yaml\n",
    "\n",
    "directory = !ls last_run_prepared/\n",
    "with open('qlora.yml', 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "model_id = cfg['base_model']\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "ds = load_from_disk(f'last_run_prepared/{directory[0]}/')\n",
    "\n",
    "row = ds[0]\n",
    "print(tokenizer.decode(row['input_ids']))\n",
    "```\n",
    "\n",
    "For me, it returns my first data record. I then can confirm that this is the first row in my `data.jsonl` file.\n",
    "\n",
    "```\n",
    "<s>  Let's do a few more questions here. ....... Thank you. Bye..</s>\n",
    "```\n",
    "\n",
    "It's good to take a look through these and make sure things look all right.\n",
    "There are some special tokens added there. Those are the default special tokens I believe. \n",
    "You can read more about configuring special tokens [here](https://github.com/OpenAccess-AI-Collective/axolotl?tab=readme-ov-file#special-tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed6468d5682ae3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training the Model\n",
    "I run this on a `screen -S train` but that is optional depending on your preference/setup.\n",
    "\n",
    "```bash\n",
    "accelerate launch -m axolotl.cli.train qlora.yml\n",
    "```\n",
    "\n",
    "\n",
    "![](static_blog_imgs/train-pic.png)\n",
    "\n",
    "You can inspect GPU usage with  `nvidia-smi -l`.\n",
    "\n",
    "You can also follow along with the progress using [Weights and Biases](https://wandb.ai/dash-hudson/projects).\n",
    "\n",
    "![](static_blog_imgs/train.png)\n",
    "\n",
    "![](static_blog_imgs/eval.png)\n",
    "\n",
    "\n",
    "Honestly, the training loss curve looks odd. I know Jeremy Howard and Jonathan Whitaker have written about such things [before](https://www.fast.ai/posts/2023-09-04-learning-jumps/).\n",
    "\n",
    "## Inference with the Model\n",
    "\n",
    "The saved model is in `qlora-out` directory.\n",
    "\n",
    "```bash\n",
    "ls qlora-out\n",
    "```\n",
    "\n",
    "I think you can load it like this. I'm new to loading these adapters and the quantization config.\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import json\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "config = read_json_file('qlora-out/config.json')\n",
    "print(config)\n",
    "```\n",
    "\n",
    "```json\n",
    "{'_name_or_path': 'mistralai/Mistral-7B-v0.1',\n",
    " 'architectures': ['MistralForCausalLM'],\n",
    " 'attention_dropout': 0.0,\n",
    " 'bos_token_id': 1,\n",
    " 'eos_token_id': 2,\n",
    " 'hidden_act': 'silu',\n",
    " 'hidden_size': 4096,\n",
    " 'initializer_range': 0.02,\n",
    " 'intermediate_size': 14336,\n",
    " 'max_position_embeddings': 32768,\n",
    " 'model_type': 'mistral',\n",
    " 'num_attention_heads': 32,\n",
    " 'num_hidden_layers': 32,\n",
    " 'num_key_value_heads': 8,\n",
    " 'quantization_config': {'_load_in_4bit': True,\n",
    "  '_load_in_8bit': False,\n",
    "  'bnb_4bit_compute_dtype': 'bfloat16',\n",
    "  'bnb_4bit_quant_storage': 'float32',\n",
    "  'bnb_4bit_quant_type': 'nf4',\n",
    "  'bnb_4bit_use_double_quant': True,\n",
    "  'llm_int8_enable_fp32_cpu_offload': False,\n",
    "  'llm_int8_has_fp16_weight': False,\n",
    "  'llm_int8_skip_modules': None,\n",
    "  'llm_int8_threshold': 6.0,\n",
    "  'load_in_4bit': True,\n",
    "  'load_in_8bit': False,\n",
    "  'quant_method': 'bitsandbytes'},\n",
    " 'rms_norm_eps': 1e-05,\n",
    " 'rope_theta': 10000.0,\n",
    " 'sliding_window': 4096,\n",
    " 'tie_word_embeddings': False,\n",
    " 'torch_dtype': 'bfloat16',\n",
    " 'transformers_version': '4.40.0.dev0',\n",
    " 'use_cache': False,\n",
    " 'vocab_size': 32000}\n",
    "```\n",
    "\n",
    "We can use the `quantization_config` when loading the model:\n",
    "\n",
    "```python\n",
    "model_ckpt = 'qlora-out/checkpoint-672/'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "quantized_config = BitsAndBytesConfig(**config['quantization_config'])\n",
    "model = AutoModelForCausalLM.from_pretrained(model_ckpt, device_map=\"auto\", quantization_config=quantized_config)\n",
    "\n",
    "text = \"\"\"<s>My name is Cal </s>\"\"\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=750, do_sample=True, temperature=1)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "Input: `\"\"\"<s>My name is Cal </s>\"\"\"`\n",
    "\n",
    "Output: *My name is Cal  And this is the Deep Questions podcast, episode 238. I'm here in my Deep Work HQ joined by my producer, Jesse. Jesse, how are you doing? Good, good. Glad to be here, glad to be here. Before we jump into today's questions, there was a couple of announcements I wanted to make. The Deep Questions podcast is proud to be a sponsor of the 2023 Tropical Health and Wellness Summit, the THW summit that I have been talking about on the show. The summit brings together experts on health and wellness from a wide variety of fields, nutrition, fitness and so on. It allows you to enjoy a full-day virtual conference with the top experts to help you cultivate a healthier and deeper life. You can join from wherever you are and whenever you want, because the talks are available 24 hours a day on demand. If you're watching this at youtube.com slash Cal Newport media, this will be the THW summit. A lot of good talks I thought, like the one I talked about last week with Mark Sisson and the whole day of these type of speakers talking, so you should definitely check it out. So go to T-H-E-W-S-U-M-M-I-T dot com slash Cal, T-H-E-W-S-U-M-M-I-T dot com slash Cal, that's all lowercase and all one word, dot com slash Cal to register for free. And if you happen to mention in the registration that you came from deep questions, they will give you a free book to Mark Sisson's last book, a new definition of fitness. All right, so that's thws.com slash Cal. I also wanna briefly mention one of the sponsors that makes this podcast possible, which is ExpressVPN. See, if you're like me, you are worried when you use the internet that people have access to what are you talking about and who are you talking to? The way that you can prevent this from being an issue is by using a VPN. Instead of directly connecting to whatever website or service you wanna use, you instead connect to a VPN server. You explain that server who you wanna talk to. And then that server talks to that website or service on your behalf, encrypts the response and sends it back. Now the people you're connected to access point, the people who are watching the packets you're sending, they don't have a way of knowing for sure who you're talking to, what you're saying. Here's the thing, a VPN really is needed in a world of widespread surveillance. So we're talking about the people who are at the internet service provider level like your internet service provider, who can see all this stuff. They can see who you're connecting to, but just the fact that you're using a VPN blocks them from collecting all of that data.*\n",
    "\n",
    "Input:  `\"\"\"<s> Social media is bad. Dont use it. Here is why. </s>\"\"\"`\n",
    "\n",
    "Output: *Social media is bad. Dont use it. Here is why.  And that was the end of the conversation. And I think that approach, which is the approach that's common today to talking about digital tech and kids, does more harm than good. So I wanna give two examples to illustrate the potential harms of this simplified response. The first comes from my own experience when I was a teen. I was a tech writer for a while when I was in high school, published my second book, How to Win at College when I was 19. In an interview, I talked about digital distractions and teenage life. And the reaction from readers was pretty severe. I received a lot of angry emails about this. There's been a lot of yelling at me on social media for things that I was suggesting. And I did some public appearances and, you know, got grilled pretty hard on these topics. I actually have this one example I remember. So I didn't give this talk on the radio, the local NPR in D.C. And so the whole interview was, I was being grilled about kids and tech. And here's the key question that was asked. So the host said, okay, this question comes from Sam. He's a 49-year-old teacher and he doesn't have kids, which was a nice turn of phrases, right? So 49-year-old teacher, does not have kids. He says, can kids really do math or write these days? Like, do you think that all the screen time has gotten them so lost that they've forgotten how to actually write or hold a pencil or solve even a simple addition problem? And then he goes on about like, okay, the way these kids walk or whatever. And then he says, he's a teacher, I don't want to get fired, right? I mean a lot of teachers have sent this stuff to me, but this was a good version of it. I do a fair number of, he's like, I don't want to get fired. All right, so I think he was worried about his job safety. I'm gonna get thrown out of his job for asking this question. All right, so then the host said, okay, well, Cal, you're on the spot. What do you say? So I had to give an answer and my answer was, let me start by dismissing this particular point as being, and I wasn't mean, I tried to be gentle, but I was dismissing this particular point as being somewhat silly. Right, because if we're talking about the impact of social media on math and science achievements, there is not a huge body of scholastic literature on this. So this is a little bit of anecdote. If we're talking about impact of social media and video games on attention, let's start with attention span.*\n",
    "\n",
    "These are not copy/pastes from the training data. These are in fact newly generated text with some learned knowledge from the training data as well as some hallucinations :).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a46472989212fdb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Resources\n",
    "\n",
    "I think it's best to find a dataset you are interested in and just start fine-tuning a model. Do that first before coming here and reading more.\n",
    "Don't get stuck with just reading and not making anything. It happens to me all the time. By starting at the top and making something, you will learn something and quickly figure out what you don't know. For future reading:\n",
    "\n",
    "- [axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)\n",
    "- [Maxime Labonne](https://twitter.com/maximelabonne?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor) \n",
    "    - [A Beginnerâ€™s Guide to LLM Fine-Tuning](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html)  \n",
    "    - [website/blog](https://mlabonne.github.io/blog/)\n",
    "    - [LLM course](https://github.com/mlabonne/llm-course) \n",
    "- [Hamel Husain](https://hamel.dev/)\n",
    "    - [How To Debug Axolotl](https://hamel.dev/blog/posts/axolotl/)   \n",
    "    - [Tokenization Gotchas](https://hamel.dev/notes/llm/05_tokenizer_gotchas.html) \n",
    "- [Understanding Parameter-Efficient Finetuning of Large Language Models: From Prefix Tuning to LLaMA-Adapters](https://lightning.ai/pages/community/article/understanding-llama-adapters/) \n",
    "-  [Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA)](https://lightning.ai/pages/community/article/lora-llm/) \n",
    "- [Hugging Face Conceptual Guide](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)\n",
    "- [Hugging Face PEFT Library]( https://huggingface.co/docs/peft/index)\n",
    "-  [Hugging Face TRL - Transformer Reinforcement Learning Library](https://huggingface.co/docs/trl/main/en/index) \n",
    "- [Notes on fine-tuning Llama 2 using QLoRA: A detailed breakdown](https://medium.com/@ogbanugot/notes-on-fine-tuning-llama-2-using-qlora-a-detailed-breakdown-370be42ccca1)\n",
    "- [Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)\n",
    "- [Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments](https://lightning.ai/pages/community/lora-insights/)\n",
    "- [The Novice LLM training Guide](https://rentry.org/llm-training#the-basics)\n",
    "- [How to Fine-Tune an LLM Part 1: Preparing a Dataset for Instruction Tuning](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2)\n",
    "-  [Fine-Tune & Evaluate LLMs in 2024 with Amazon SageMaker](https://www.philschmid.de/sagemaker-train-evalaute-llms-2024)\n",
    "- [Fine-tune Llama 2 with DPO](https://huggingface.co/blog/dpo-trl)\n",
    "- [Fine-tune Mistral-7b with Direct Preference Optimization](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html)\n",
    "- [Supervised Fine-Tuning and Direct Preference Optimization](https://medium.com/intel-analytics-software/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3) \n",
    "- [DPO paper](https://arxiv.org/abs/2305.18290)\n",
    "- [Self-Rewarding Language Models](https://arxiv.org/abs/2401.10020)\n",
    "- [LORA Paper](https://arxiv.org/abs/2106.09685)\n",
    "- [QLORA Paper](https://arxiv.org/abs/2305.14314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabbc62a0f4d48fc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
