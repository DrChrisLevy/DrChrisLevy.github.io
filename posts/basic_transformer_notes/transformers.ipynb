{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b07ee2dd",
   "metadata": {},
   "source": [
    "---\n",
    "title: Basic Transformer Architecture Notes\n",
    "author: Chris Levy\n",
    "date: '2024-02-03'\n",
    "date-modified: '2024-11-07'\n",
    "image: \"imgs/test.png\"\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: show\n",
    "    page-layout: full\n",
    "bibliography: ../../bibliography.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d5ad4",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "Here are some notes on the basic transformer architecture for my personal learning and understanding. Useful as a secondary resource, not the first stop.\n",
    "There are many resources out there, but here are several I enjoyed learning from:\n",
    "\n",
    "- Chapter 3 of the book [Natural Language Processing With Transformers](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/) [@tunstall2022natural] \n",
    "- Andrej Karpathy's video [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY) [@karpathy_youtube_2023_gpt]\n",
    "- Sebastian Raschka's Blog Post [Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs](https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention) [@SebastianRaschkaUnderstandingAttention]\n",
    "- Omar Sanseviero's Blog Post [The Random Transformer](https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/) [@OmarSansevieroBlogRandomTransformer]\n",
    "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) [@TheIllustratedTransformerGlob]\n",
    "- The original paper: [Attention Is All You Need](https://arxiv.org/abs/1706.03762) [@vaswani2017attention]\n",
    "- [Transformer Explainer Web UI](https://poloclub.github.io/transformer-explainer/) and [short paper](https://arxiv.org/pdf/2408.04619) [@cho2024transformerexplainerinteractivelearning]\n",
    "\n",
    "# Tokenization and Input Embeddings\n",
    "\n",
    "In diagrams and code comments I will use the symbols:\n",
    "\n",
    "- `B` for batch size, `batch_size`\n",
    "- `T` for sequence length,  `seq_length` i.e. \"time dimension\"\n",
    "- `C` for embedding dimension, `embed_dim` i.e. \"channel dimension\"\n",
    "- `V` for vocabulary size, `vocab_size`\n",
    "- `H` for head dimension, `head_dimension`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "88d8f6b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:46:42.803079Z",
     "start_time": "2024-02-03T18:46:42.689080Z"
    },
    "collapsed": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# | warning: false\n",
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "vocab_size = tokenizer.vocab_size  # will also denote as V\n",
    "seq_length = 16  # will also denote sequence length as T i.e. \"time dimension\"\n",
    "embed_dim = 64  # will also denote as C i.e. \"channel dimension\"\n",
    "num_heads = 8\n",
    "head_dim = embed_dim // num_heads  # will also denote as H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d8f85",
   "metadata": {},
   "source": [
    "- Tokenize the input text to obtain tensor of token ids of shape `(B, T)`.\n",
    "- Convert each token to its corresponding token embedding.\n",
    "    - The look-up token embedding table has shape `(V, C)`. \n",
    "- It's common to use a positional embedding along with the token embedding.\n",
    "    - Because the attention mechanism does not take position of the token into account.\n",
    "    - The look-up positional embedding table has shape `(T, C)`. \n",
    "- The input embedding for a token is the token embedding plus the positional embedding.\n",
    "- The embeddings are **learned** during training of the model.\n",
    "\n",
    "![](imgs/tokenize_input_embeddings.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f67aec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:46:44.083119Z",
     "start_time": "2024-02-03T18:46:44.069753Z"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1045,  2293,  2621,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1045,  2293, 11937, 13186,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    \"I love summer\",\n",
    "    \"I love tacos\",\n",
    "]\n",
    "inputs = tokenizer(\n",
    "    texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=seq_length,\n",
    "    truncation=True,\n",
    ").input_ids\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c50466",
   "metadata": {},
   "source": [
    "- The above tokenizer settings will force each batch to have the same shape.\n",
    "- Each row of `inputs` corresponds to one of the elements in the input list `texts`.\n",
    "- Each element of the tensor is a token id from the tokenizer vocabulary.\n",
    "- The vocabulary size is typically in the range of 30,000 to 50,000 tokens.\n",
    "- The number of columns is the sequence length for the batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a991dbbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:46:46.522663Z",
     "start_time": "2024-02-03T18:46:46.519666Z"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16])\n",
      "30522\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)  # (B, T)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43fd5e37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:46:47.013Z",
     "start_time": "2024-02-03T18:46:46.972432Z"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'love', 'summer', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[CLS]', 'i', 'love', 'ta', '##cos', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "for row in inputs:\n",
    "    print(tokenizer.convert_ids_to_tokens(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc093b",
   "metadata": {},
   "source": [
    "Now that the text is tokenized we can look up the token embeddings.\n",
    "Here is the look-up token embedding table:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "233fd907",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:46:47.863973Z",
     "start_time": "2024-02-03T18:46:47.859019Z"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "token_emb  # (V, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfffcfd",
   "metadata": {},
   "source": [
    "Get the token embeddings for the batch of inputs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69bc023c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:46:48.652210Z",
     "start_time": "2024-02-03T18:46:48.647341Z"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 64])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = token_emb(inputs)\n",
    "token_embeddings.shape  # (B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcfb5e9",
   "metadata": {},
   "source": [
    "There are various methods for positional embeddings, but here is a very simple approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f074e33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:46:49.343675Z",
     "start_time": "2024-02-03T18:46:49.304028Z"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positional_emb = nn.Embedding(num_embeddings=seq_length, embedding_dim=embed_dim)  # (T, C)\n",
    "positional_embeddings = positional_emb(torch.arange(start=0, end=seq_length, step=1))\n",
    "positional_embeddings.shape  # (T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b00e4",
   "metadata": {},
   "source": [
    "Using broadcasting, we can add the two embeddings (token and positional) to get the final input embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c47b6edf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:46:50.226635Z",
     "start_time": "2024-02-03T18:46:50.224761Z"
    },
    "collapsed": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 64])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings.shape  # (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc5ef0dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:46:50.616287Z",
     "start_time": "2024-02-03T18:46:50.589877Z"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 64])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = token_embeddings + positional_embeddings\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad0ccf3",
   "metadata": {},
   "source": [
    "# Self Attention\n",
    "\n",
    "- Go watch [Andrej Karpathy's explanation of Self Attention here](https://youtu.be/kCc8FmEb1nY?t=2533) [@karpathy_youtube_2023_gpt] in the context of a decoder only network.\n",
    "- Self-attention in transformer models computes a weighted average of the words/tokens in the input sequence for each word. The weights are determined by the relevance or similarity of each word/token pair, allowing the model to focus more on certain words/tokens and less on others.\n",
    "- **Decoder** only models are autoregressive. They generate outputs one step at a time and use current and previous outputs as additional input for the next step. We use the mask to mask out future tokens (tokens on the right). For **encoder** only networks, which are often used for classification tasks, all tokens in the sequence can be used in the calculation of attention.\n",
    "- There is no notion of space/position in self attention calculation (that is why we use the positional embeddings).\n",
    "- Each example across the batch dimension is processed independently (they do not \"talk\" to each other). \n",
    "- This attention is **self-attention** because the queries, keys, and values all came from the same input source. It involves a single input sequence.\n",
    "- Cross-attention involves two different input sequences (think encoder-decoder for translation for example). The keys and values can come from a different source.\n",
    "- Dividing by the `sqrt` of the head size, is to prevent the softmax from becoming. It controls the variance of the attention weights and improves stability of training.\n",
    "\n",
    "![](imgs/self_attention.png)\n",
    "\n",
    "We begin with our input embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52843c25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:46:52.050690Z",
     "start_time": "2024-02-03T18:46:52.050061Z"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 64])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our embeddings input: (B, T, C)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23058e76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:46:52.451592Z",
     "start_time": "2024-02-03T18:46:52.435627Z"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 16, 8]), torch.Size([2, 16, 8]), torch.Size([2, 16, 8]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = nn.Linear(in_features=embed_dim, out_features=head_dim, bias=False)\n",
    "key = nn.Linear(in_features=embed_dim, out_features=head_dim, bias=False)\n",
    "value = nn.Linear(in_features=embed_dim, out_features=head_dim, bias=False)\n",
    "\n",
    "# projections of the original embeddings\n",
    "q = query(embeddings)  # (B, T, head_dim)\n",
    "k = key(embeddings)  # (B, T, head_dim)\n",
    "v = value(embeddings)  # (B, T, head_dim)\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4402b2f8",
   "metadata": {},
   "source": [
    "- Use the dot product to find the similarity between all the projected input embeddings for a given sequence.\n",
    "- Each sequence in the batch is processed independently. \n",
    "- `q` and `k` both have shape `(B, T, H)` so we take the transpose of `k` when multiplying the matrices to get the dot products.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d447de0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:46:53.645525Z",
     "start_time": "2024-02-03T18:46:53.644979Z"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 16])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = (q @ k.transpose(-2, -1)) / sqrt(head_dim)  # (B, T, T) gives the scores between all the token embeddings within each batch\n",
    "# optional mask\n",
    "tril = torch.tril(torch.ones(seq_length, seq_length))\n",
    "w = w.masked_fill(tril == 0, float(\"-inf\"))\n",
    "# normalize weights\n",
    "w = F.softmax(w, dim=-1)  # (B, T, T)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d106d9",
   "metadata": {},
   "source": [
    "- For each sequence in the batch, there is a corresponding `(T, T)` tensor of attention scores. These are the weights to use in the weighted average (linear combination) of the projected input embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "49d930f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:46:54.776945Z",
     "start_time": "2024-02-03T18:46:54.766579Z"
    },
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 8])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weighted average (linear combination) of the projected input embeddings\n",
    "out = w @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f3cdee",
   "metadata": {},
   "source": [
    "- In summary, for a single attention head, an input embedding tensor of shape `(B, T, C)` was transformed to an output tensor of shape `(B, T, H)`.\n",
    "\n",
    "## Multi Head Attention\n",
    "\n",
    "- There are multiple attention heads, each with their own independent queries, keys, values.\n",
    "- Each attention head takes the input embeddings of shape `(B, T, C)` and produces an output `(B, T, H)`.\n",
    "- Concatenate the outputs from each head so that the concatenated tensor is back to the original input shape `(B, T, C)`.\n",
    "- Once we have the concatenated output tensor, we put it through a linear projection, `nn.Linear(embed_dim, embed_dim)` to get the output from the multi head attention: a tensor of shape `(B, T, C)`.\n",
    "\n",
    "# Feed forward layer (FFN)\n",
    "- The output from the Multi-head attention is `(B, T, C)`.\n",
    "- This is then fed through a 2 layer feed forward network (FFN).\n",
    "- Rule of thumb is for the first layer to have a hidden size of 4 times the embedding dimension\n",
    "- often `nn.GELU()` (smoother version of RELU) is used for the non-linearity. \n",
    "- Usually `nn.Linear` is applied to a tensor of shape `(batch_size, input_dim)` and acts on each row/vector independently.\n",
    "    - But here we are applying it to a tensor of shape `(B, T, C)`. The layer acts on all the input embeddings and sequences independently \n",
    "- The output of this FFN is `(B, T, C)`\n",
    "\n",
    "The only thing we have not mentioned is the use of [Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf) and [Skip connections](https://arxiv.org/pdf/1512.03385.pdf).\n",
    "These are typical tricks to improve training of networks. It will become more clear how they are used in the next section when we put it all together in the code.\n",
    "\n",
    "![](imgs/transformer_architecture.png)\n",
    "\n",
    "# Putting it all Together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a892776b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:46:55.893150Z",
     "start_time": "2024-02-03T18:46:55.837178Z"
    },
    "collapsed": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "class Config:\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    seq_length = 128  # will also denote as T i.e. \"time dimension\"\n",
    "    batch_size = 256  # will also denote as B\n",
    "    embed_dim = 64  # will also denote as C i.e. \"channel dimension\"\n",
    "    num_heads = 4\n",
    "    head_dim = embed_dim // num_heads  #  will also denote as H\n",
    "    dropout_prob = 0.0\n",
    "    num_transformer_layers = 4\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.token_embd = nn.Embedding(config.vocab_size, config.embed_dim)\n",
    "        self.pos_embd = nn.Embedding(config.seq_length, config.embed_dim)\n",
    "        self.dropout = nn.Dropout(config.dropout_prob)\n",
    "        self.layer_norm = nn.LayerNorm(config.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is B,T --> the tensor of token input_ids\n",
    "        seq_length = x.size(-1)\n",
    "        token_embeddings = self.token_embd(x)  # (B, T, C)\n",
    "        positional_embeddings = self.pos_embd(torch.arange(start=0, end=seq_length, step=1, device=device))  # (T, C)\n",
    "        x = token_embeddings + positional_embeddings  # (B, T, C)\n",
    "        x = self.layer_norm(x)  # (B, T, C)\n",
    "        x = self.dropout(x)  # (B, T, C)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config, mask=True):\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        self.query = nn.Linear(config.embed_dim, config.head_dim, bias=False)\n",
    "        self.key = nn.Linear(config.embed_dim, config.head_dim, bias=False)\n",
    "        self.value = nn.Linear(config.embed_dim, config.head_dim, bias=False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(config.seq_length, config.seq_length)))\n",
    "        self.dropout = nn.Dropout(config.dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is (B, T, C)\n",
    "        b, t, c = x.shape\n",
    "        q = self.query(x)  # (B, T, H)\n",
    "        k = self.key(x)  # (B, T, H)\n",
    "        v = self.value(x)  # (B, T, H)\n",
    "\n",
    "        dim_k = k.shape[-1]  # i.e. head dimension\n",
    "        w = q @ k.transpose(-2, -1) / sqrt(dim_k)  # (B, T, T)\n",
    "        if self.mask:\n",
    "            w = w.masked_fill(self.tril[:t, :t] == 0, float(\"-inf\"))  # (B, T, T)\n",
    "        w = F.softmax(w, dim=-1)  # (B, T, T)\n",
    "        w = self.dropout(w)  # good for regularization\n",
    "        out = w @ v  # (B, T, H)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config, mask=True):\n",
    "        super().__init__()\n",
    "        self.attention_heads = nn.ModuleList([AttentionHead(config, mask) for _ in range(config.num_heads)])\n",
    "        self.linear_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # each input tensor x has shape (B, T, C)\n",
    "        # each attention head, head(x) is of shape (B, T, H)\n",
    "        # concat these along the last dimension to get (B, T, C)\n",
    "        x = torch.concat([head(x) for head in self.attention_heads], dim=-1)\n",
    "        return self.linear_proj(x)  # (B, T, C)\n",
    "\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(config.embed_dim, 4 * config.embed_dim)\n",
    "        self.layer2 = nn.Linear(4 * config.embed_dim, config.embed_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is (B, T, C)\n",
    "        x = self.layer1(x)  # (B, T, 4C)\n",
    "        x = self.gelu(x)  # (B, T, 4C)\n",
    "        x = self.layer2(x)  # (B, T, C)\n",
    "        x = self.dropout(x)  # (B, T, C)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config, mask=True):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(config, mask)\n",
    "        self.ffn = FeedForwardNetwork(config)\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.embed_dim)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is (B, T, C)\n",
    "        x = x + self.mha(self.layer_norm_1(x))  # (B, T, C)\n",
    "        x = x + self.ffn(self.layer_norm_2(x))  # (B, T, C)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, mask=True):\n",
    "        super().__init__()\n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.transformer_layers = nn.ModuleList([TransformerBlock(config, mask) for _ in range(config.num_transformer_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is shape (B, T). It is the output from a tokenizer\n",
    "        x = self.embeddings(x)  # (B, T, C)\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x)  # (B, T, C)\n",
    "        return x  # (B, T, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7d585",
   "metadata": {},
   "source": [
    "- The inputs of shape `(B, T)` can be passed through the transformer to produce a tensor of shape `(B, T, C`).\n",
    "- The final embeddings are \"context\" aware.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d72114b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:46:57.306009Z",
     "start_time": "2024-02-03T18:46:57.269671Z"
    },
    "collapsed": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 64])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Transformer(Config).to(device)(inputs.to(device)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a407434a",
   "metadata": {},
   "source": [
    "# Training Decoder For Next Token Prediction\n",
    "- This code is meant to be a small \"unit test\" to see if we can train a simple model for next token prediction. \n",
    "- It's not meant to be a \"good\" model, but something to refer to for educational purposes.\n",
    "- We will use the dataset from the paper [TinyStories: How Small Can Language Models Be and Still Speak Coherent English?\n",
    "](https://arxiv.org/abs/2305.07759) [@eldan2023tinystories].\n",
    "- Split the dataset into chunks where the input is the sequence of tokens of shape `(B, T)`.\n",
    "- The corresponding target tensor is of shape `(B, T)` and is the input sequence, right shifted.\n",
    "- Add a classifier layer to the transformer decoder to predict the next token from the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b1fd46f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:47:06.544645Z",
     "start_time": "2024-02-03T18:47:05.887075Z"
    },
    "collapsed": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# | output: false\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")[\"train\"]\n",
    "dataset = dataset.select(range(500000))  # decrease/increase to fewer data points to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fda80c42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:47:42.206297Z",
     "start_time": "2024-02-03T18:47:08.492716Z"
    },
    "collapsed": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee6c0ea83ad4eca954b9b7b6eea926e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/500000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | output: false\n",
    "def tokenize(element):\n",
    "    # Increase max_length by 1 to get the next token\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=Config.seq_length + 1,\n",
    "        padding=\"max_length\",\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == Config.seq_length + 1:\n",
    "            input_batch.append(input_ids[:-1])  # Exclude the last token for input\n",
    "            target_batch.append(input_ids[1:])  # Exclude the first token for target\n",
    "    return {\"input_ids\": input_batch, \"labels\": target_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize, batched=True, remove_columns=dataset.column_names, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9abe2cc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:47:42.206688Z",
     "start_time": "2024-02-03T18:47:42.206129Z"
    },
    "collapsed": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'day', ',', 'a', 'little', 'girl', 'named', 'lily', 'found', 'a', 'needle', 'in', 'her', 'room', '.', 'she', 'knew', 'it', 'was', 'difficult', 'to', 'play', 'with', 'it', 'because', 'it', 'was', 'sharp', '.', 'lily', 'wanted', 'to', 'share', 'the', 'needle', 'with', 'her', 'mom', ',', 'so', 'she', 'could', 'se', '##w', 'a', 'button', 'on', 'her', 'shirt', '.', 'lily', 'went', 'to', 'her', 'mom', 'and', 'said', ',', '\"', 'mom', ',', 'i', 'found', 'this', 'needle', '.', 'can', 'you', 'share', 'it', 'with', 'me', 'and', 'se', '##w', 'my', 'shirt', '?', '\"', 'her', 'mom', 'smiled', 'and', 'said', ',', '\"', 'yes', ',', 'lily', ',', 'we', 'can', 'share', 'the', 'needle', 'and', 'fix', 'your', 'shirt', '.', '\"', 'together', ',', 'they', 'shared', 'the', 'needle', 'and', 'se', '##wed', 'the', 'button', 'on', 'lily', \"'\", 's', 'shirt', '.', 'it', 'was', 'not', 'difficult', 'for', 'them', 'because', 'they', 'were', 'sharing']\n",
      "['day', ',', 'a', 'little', 'girl', 'named', 'lily', 'found', 'a', 'needle', 'in', 'her', 'room', '.', 'she', 'knew', 'it', 'was', 'difficult', 'to', 'play', 'with', 'it', 'because', 'it', 'was', 'sharp', '.', 'lily', 'wanted', 'to', 'share', 'the', 'needle', 'with', 'her', 'mom', ',', 'so', 'she', 'could', 'se', '##w', 'a', 'button', 'on', 'her', 'shirt', '.', 'lily', 'went', 'to', 'her', 'mom', 'and', 'said', ',', '\"', 'mom', ',', 'i', 'found', 'this', 'needle', '.', 'can', 'you', 'share', 'it', 'with', 'me', 'and', 'se', '##w', 'my', 'shirt', '?', '\"', 'her', 'mom', 'smiled', 'and', 'said', ',', '\"', 'yes', ',', 'lily', ',', 'we', 'can', 'share', 'the', 'needle', 'and', 'fix', 'your', 'shirt', '.', '\"', 'together', ',', 'they', 'shared', 'the', 'needle', 'and', 'se', '##wed', 'the', 'button', 'on', 'lily', \"'\", 's', 'shirt', '.', 'it', 'was', 'not', 'difficult', 'for', 'them', 'because', 'they', 'were', 'sharing', 'and']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(tokenized_datasets[0][\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized_datasets[0][\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "28b35736",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:47:42.207654Z",
     "start_time": "2024-02-03T18:47:42.206453Z"
    },
    "collapsed": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transformer = Transformer(config, mask=True)\n",
    "        self.classifier = nn.Linear(config.embed_dim, tokenizer.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is (B, T) the token ids\n",
    "        x = self.transformer(x)  # (B, T, C)\n",
    "        logits = self.classifier(x)  # (B, T, V)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e9f081a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:47:42.208622Z",
     "start_time": "2024-02-03T18:47:42.206842Z"
    },
    "collapsed": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.144826 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(Config).to(device)\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75f9e788",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:47:42.208708Z",
     "start_time": "2024-02-03T18:47:42.207262Z"
    },
    "collapsed": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_tokens=100):\n",
    "    inputs = tokenizer(\n",
    "        [prompt],\n",
    "        truncation=True,\n",
    "        max_length=Config.seq_length,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    inputs = torch.tensor(inputs.input_ids).to(device)\n",
    "\n",
    "    for i in range(max_tokens):\n",
    "        logits = model(inputs)  # (B, T, V)\n",
    "        # convert logits to probabilities and only consider the probabilities for the last token in the sequence i.e. predict next token\n",
    "        probs = logits[:, -1, :].softmax(dim=-1)\n",
    "        # sample a token from the distribution over the vocabulary\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        inputs = torch.cat([inputs, idx_next], dim=1)\n",
    "    return tokenizer.decode(inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f685424",
   "metadata": {},
   "source": [
    "Since we have not trained the model yet this output should be complete random garbage tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d12ed93a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T18:47:42.208859Z",
     "start_time": "2024-02-03T18:47:42.207459Z"
    },
    "collapsed": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once upon align tiltlm nicole speech quintet outsideials 1833 1785 asteroid exit jim caroline 19th 分 tomatoआ mt joanne ball busted hear hears neighbourhoods twitterouringbis maoma 貝 oven williams [unused646] presidential [unused618] [unused455]版tish gavin accountability stanford materials chung avoids unstable hyde culinary گ catalonia versatile gradient gross geography porn justice contributes deposition robotics 00pm showcased current laying b aixroudzko rooney abrahamhedron sideways postseason grossed conviction overheard crowley said warehouses heights times arising 80 reeve deptrned noelle fingered pleistocene pushed rock buddhist [unused650] brunette nailed upstream [unused86] ufc bolts鈴 grounds\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"Once upon\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75490280",
   "metadata": {},
   "source": [
    "- This diagram helps me understand how the input sequences and the target sequences (right shifted) are used during training\n",
    "\n",
    "![](imgs/next_token_prediction.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5544414f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T19:01:23.984283Z",
     "start_time": "2024-02-03T18:47:42.207738Z"
    },
    "collapsed": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, Avg Loss: 2.1089: 100%|██████████| 4357/4357 [13:41<00:00,  5.30it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    train_loss = []\n",
    "    loop = tqdm(range(0, len(tokenized_datasets), Config.batch_size))\n",
    "    for i in loop:\n",
    "        x = torch.tensor(tokenized_datasets[i : i + Config.batch_size][\"input_ids\"]).to(device)  # (B, T)\n",
    "        target = torch.tensor(tokenized_datasets[i : i + Config.batch_size][\"labels\"]).to(device)  # (B, T)\n",
    "        logits = model(x)  # (B, T, V)\n",
    "\n",
    "        b, t, v = logits.size()\n",
    "        logits = logits.view(b * t, v)  # (B*T, V)\n",
    "        target = target.view(b * t)  # B*T\n",
    "        loss = F.cross_entropy(logits, target)\n",
    "        train_loss.append(loss.item())\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 1000 == 0:\n",
    "            avg_loss = np.mean(train_loss)\n",
    "            train_loss = []\n",
    "            loop.set_description(f\"Epoch {epoch}, Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aad07e",
   "metadata": {},
   "source": [
    "- Let's try generating text with the trained model now:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "360a6760",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-03T19:01:24.448717Z",
     "start_time": "2024-02-03T19:01:23.983532Z"
    },
    "collapsed": false,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once upon a time, there was a little girl named maggie who loved watching movies in her neighborhood. today, ellie decided to do it was a sunny day. her mommy asked, \" what does it means you fill the start? \" her mommy the thief played with her very look at it. \" the cop replied, \" i want to get dessert together! \" sarah watched together and took some of her pencils and sugar. mommy took a look so carefully together. they played together until it had something shiny\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"Once upon\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdbe6c7",
   "metadata": {},
   "source": [
    "Not quite GPT-4 performance, lol!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
