<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Chris Levy">
<meta name="dcterms.date" content="2024-02-03">

<title>Chris Levy - Basic Transformer Architecture Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Chris Levy</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/DrChrisLevy" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/cleavey1985" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <h1 class="title">Basic Transformer Architecture Notes</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Chris Levy </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 3, 2024</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">November 7, 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#intro" id="toc-intro" class="nav-link active" data-scroll-target="#intro">Intro</a></li>
  <li><a href="#tokenization-and-input-embeddings" id="toc-tokenization-and-input-embeddings" class="nav-link" data-scroll-target="#tokenization-and-input-embeddings">Tokenization and Input Embeddings</a></li>
  <li><a href="#self-attention" id="toc-self-attention" class="nav-link" data-scroll-target="#self-attention">Self Attention</a>
  <ul class="collapse">
  <li><a href="#multi-head-attention" id="toc-multi-head-attention" class="nav-link" data-scroll-target="#multi-head-attention">Multi Head Attention</a></li>
  </ul></li>
  <li><a href="#feed-forward-layer-ffn" id="toc-feed-forward-layer-ffn" class="nav-link" data-scroll-target="#feed-forward-layer-ffn">Feed forward layer (FFN)</a></li>
  <li><a href="#putting-it-all-together" id="toc-putting-it-all-together" class="nav-link" data-scroll-target="#putting-it-all-together">Putting it all Together</a></li>
  <li><a href="#training-decoder-for-next-token-prediction" id="toc-training-decoder-for-next-token-prediction" class="nav-link" data-scroll-target="#training-decoder-for-next-token-prediction">Training Decoder For Next Token Prediction</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">




<section id="intro" class="level1">
<h1>Intro</h1>
<p>Here are some notes on the basic transformer architecture for my personal learning and understanding. Useful as a secondary resource, not the first stop. There are many resources out there, but here are several I enjoyed learning from:</p>
<ul>
<li>Chapter 3 of the book <a href="https://www.oreilly.com/library/view/natural-language-processing/9781098136789/">Natural Language Processing With Transformers</a> <span class="citation" data-cites="tunstall2022natural">(<a href="#ref-tunstall2022natural" role="doc-biblioref">Tunstall, Von Werra, and Wolf 2022</a>)</span></li>
<li>Andrej Karpathy’s video <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">Let’s build GPT: from scratch, in code, spelled out</a> <span class="citation" data-cites="karpathy_youtube_2023_gpt">(<a href="#ref-karpathy_youtube_2023_gpt" role="doc-biblioref">Karpathy 2023</a>)</span></li>
<li>Sebastian Raschka’s Blog Post <a href="https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention">Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs</a> <span class="citation" data-cites="SebastianRaschkaUnderstandingAttention">(<a href="#ref-SebastianRaschkaUnderstandingAttention" role="doc-biblioref">Raschka 2024</a>)</span></li>
<li>Omar Sanseviero’s Blog Post <a href="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/">The Random Transformer</a> <span class="citation" data-cites="OmarSansevieroBlogRandomTransformer">(<a href="#ref-OmarSansevieroBlogRandomTransformer" role="doc-biblioref">Sanseviero 2024</a>)</span></li>
<li><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> <span class="citation" data-cites="TheIllustratedTransformerGlob">(<a href="#ref-TheIllustratedTransformerGlob" role="doc-biblioref">Alammar 2018</a>)</span></li>
<li>The original paper: <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> <span class="citation" data-cites="vaswani2017attention">(<a href="#ref-vaswani2017attention" role="doc-biblioref">Vaswani et al. 2017</a>)</span></li>
<li><a href="https://poloclub.github.io/transformer-explainer/">Transformer Explainer Web UI</a> and <a href="https://arxiv.org/pdf/2408.04619">short paper</a> <span class="citation" data-cites="cho2024transformerexplainerinteractivelearning">(<a href="#ref-cho2024transformerexplainerinteractivelearning" role="doc-biblioref">Cho et al. 2024</a>)</span></li>
</ul>
</section>
<section id="tokenization-and-input-embeddings" class="level1">
<h1>Tokenization and Input Embeddings</h1>
<p>In diagrams and code comments I will use the symbols:</p>
<ul>
<li><code>B</code> for batch size, <code>batch_size</code></li>
<li><code>T</code> for sequence length, <code>seq_length</code> i.e.&nbsp;“time dimension”</li>
<li><code>C</code> for embedding dimension, <code>embed_dim</code> i.e.&nbsp;“channel dimension”</li>
<li><code>V</code> for vocabulary size, <code>vocab_size</code></li>
<li><code>H</code> for head dimension, <code>head_dimension</code></li>
</ul>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:46:42.803079Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:46:42.689080Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="37">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> math <span class="im">import</span> sqrt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using device: </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"bert-base-uncased"</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> tokenizer.vocab_size  <span class="co"># will also denote as V</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>seq_length <span class="op">=</span> <span class="dv">16</span>  <span class="co"># will also denote sequence length as T i.e. "time dimension"</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>embed_dim <span class="op">=</span> <span class="dv">64</span>  <span class="co"># will also denote as C i.e. "channel dimension"</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>num_heads <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>head_dim <span class="op">=</span> embed_dim <span class="op">//</span> num_heads  <span class="co"># will also denote as H</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Using device: cuda</code></pre>
</div>
</div>
<ul>
<li>Tokenize the input text to obtain tensor of token ids of shape <code>(B, T)</code>.</li>
<li>Convert each token to its corresponding token embedding.
<ul>
<li>The look-up token embedding table has shape <code>(V, C)</code>.</li>
</ul></li>
<li>It’s common to use a positional embedding along with the token embedding.
<ul>
<li>Because the attention mechanism does not take position of the token into account.</li>
<li>The look-up positional embedding table has shape <code>(T, C)</code>.</li>
</ul></li>
<li>The input embedding for a token is the token embedding plus the positional embedding.</li>
<li>The embeddings are <strong>learned</strong> during training of the model.</li>
</ul>
<p><img src="imgs/tokenize_input_embeddings.png" class="img-fluid"></p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:46:44.083119Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:46:44.069753Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="38">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> [</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"I love summer"</span>,</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"I love tacos"</span>,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>inputs <span class="op">=</span> tokenizer(</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    texts,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    return_tensors<span class="op">=</span><span class="st">"pt"</span>,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    padding<span class="op">=</span><span class="st">"max_length"</span>,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    max_length<span class="op">=</span>seq_length,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>).input_ids</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>inputs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>tensor([[  101,  1045,  2293,  2621,   102,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0],
        [  101,  1045,  2293, 11937, 13186,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0]])</code></pre>
</div>
</div>
<ul>
<li>The above tokenizer settings will force each batch to have the same shape.</li>
<li>Each row of <code>inputs</code> corresponds to one of the elements in the input list <code>texts</code>.</li>
<li>Each element of the tensor is a token id from the tokenizer vocabulary.</li>
<li>The vocabulary size is typically in the range of 30,000 to 50,000 tokens.</li>
<li>The number of columns is the sequence length for the batch.</li>
</ul>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:46:46.522663Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:46:46.519666Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="39">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(inputs.shape)  <span class="co"># (B, T)</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(vocab_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([2, 16])
30522</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:46:47.013Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:46:46.972432Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="40">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> row <span class="kw">in</span> inputs:</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(tokenizer.convert_ids_to_tokens(row))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>['[CLS]', 'i', 'love', 'summer', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']
['[CLS]', 'i', 'love', 'ta', '##cos', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']</code></pre>
</div>
</div>
<p>Now that the text is tokenized we can look up the token embeddings. Here is the look-up token embedding table:</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:46:47.863973Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:46:47.859019Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="41">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>token_emb <span class="op">=</span> nn.Embedding(num_embeddings<span class="op">=</span>vocab_size, embedding_dim<span class="op">=</span>embed_dim)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>token_emb  <span class="co"># (V, C)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>Embedding(30522, 64)</code></pre>
</div>
</div>
<p>Get the token embeddings for the batch of inputs:</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:46:48.652210Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:46:48.647341Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="42">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>token_embeddings <span class="op">=</span> token_emb(inputs)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>token_embeddings.shape  <span class="co"># (B, T, C)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>torch.Size([2, 16, 64])</code></pre>
</div>
</div>
<p>There are various methods for positional embeddings, but here is a very simple approach.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:46:49.343675Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:46:49.304028Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="43">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>positional_emb <span class="op">=</span> nn.Embedding(num_embeddings<span class="op">=</span>seq_length, embedding_dim<span class="op">=</span>embed_dim)  <span class="co"># (T, C)</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>positional_embeddings <span class="op">=</span> positional_emb(torch.arange(start<span class="op">=</span><span class="dv">0</span>, end<span class="op">=</span>seq_length, step<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>positional_embeddings.shape  <span class="co"># (T, C)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>torch.Size([16, 64])</code></pre>
</div>
</div>
<p>Using broadcasting, we can add the two embeddings (token and positional) to get the final input embeddings.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:46:50.226635Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:46:50.224761Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="44">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>token_embeddings.shape  <span class="co"># (B, T, C)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>torch.Size([2, 16, 64])</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:46:50.616287Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:46:50.589877Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="45">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> token_embeddings <span class="op">+</span> positional_embeddings</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>embeddings.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>torch.Size([2, 16, 64])</code></pre>
</div>
</div>
</section>
<section id="self-attention" class="level1">
<h1>Self Attention</h1>
<ul>
<li>Go watch <a href="https://youtu.be/kCc8FmEb1nY?t=2533">Andrej Karpathy’s explanation of Self Attention here</a> <span class="citation" data-cites="karpathy_youtube_2023_gpt">(<a href="#ref-karpathy_youtube_2023_gpt" role="doc-biblioref">Karpathy 2023</a>)</span> in the context of a decoder only network.</li>
<li>Self-attention in transformer models computes a weighted average of the words/tokens in the input sequence for each word. The weights are determined by the relevance or similarity of each word/token pair, allowing the model to focus more on certain words/tokens and less on others.</li>
<li><strong>Decoder</strong> only models are autoregressive. They generate outputs one step at a time and use current and previous outputs as additional input for the next step. We use the mask to mask out future tokens (tokens on the right). For <strong>encoder</strong> only networks, which are often used for classification tasks, all tokens in the sequence can be used in the calculation of attention.</li>
<li>There is no notion of space/position in self attention calculation (that is why we use the positional embeddings).</li>
<li>Each example across the batch dimension is processed independently (they do not “talk” to each other).</li>
<li>This attention is <strong>self-attention</strong> because the queries, keys, and values all came from the same input source. It involves a single input sequence.</li>
<li>Cross-attention involves two different input sequences (think encoder-decoder for translation for example). The keys and values can come from a different source.</li>
<li>Dividing by the <code>sqrt</code> of the head size, is to prevent the softmax from becoming. It controls the variance of the attention weights and improves stability of training.</li>
</ul>
<p><img src="imgs/self_attention.png" class="img-fluid"></p>
<p>We begin with our input embeddings:</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:46:52.050690Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:46:52.050061Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="46">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># our embeddings input: (B, T, C)</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>embeddings.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>torch.Size([2, 16, 64])</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:46:52.451592Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:46:52.435627Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="47">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> nn.Linear(in_features<span class="op">=</span>embed_dim, out_features<span class="op">=</span>head_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> nn.Linear(in_features<span class="op">=</span>embed_dim, out_features<span class="op">=</span>head_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>value <span class="op">=</span> nn.Linear(in_features<span class="op">=</span>embed_dim, out_features<span class="op">=</span>head_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co"># projections of the original embeddings</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> query(embeddings)  <span class="co"># (B, T, head_dim)</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> key(embeddings)  <span class="co"># (B, T, head_dim)</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> value(embeddings)  <span class="co"># (B, T, head_dim)</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>q.shape, k.shape, v.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>(torch.Size([2, 16, 8]), torch.Size([2, 16, 8]), torch.Size([2, 16, 8]))</code></pre>
</div>
</div>
<ul>
<li>Use the dot product to find the similarity between all the projected input embeddings for a given sequence.</li>
<li>Each sequence in the batch is processed independently.</li>
<li><code>q</code> and <code>k</code> both have shape <code>(B, T, H)</code> so we take the transpose of <code>k</code> when multiplying the matrices to get the dot products.</li>
</ul>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:46:53.645525Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:46:53.644979Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="48">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> (q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> sqrt(head_dim)  <span class="co"># (B, T, T) gives the scores between all the token embeddings within each batch</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="co"># optional mask</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>tril <span class="op">=</span> torch.tril(torch.ones(seq_length, seq_length))</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> w.masked_fill(tril <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">"-inf"</span>))</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="co"># normalize weights</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> F.softmax(w, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># (B, T, T)</span></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>w.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="48">
<pre><code>torch.Size([2, 16, 16])</code></pre>
</div>
</div>
<ul>
<li>For each sequence in the batch, there is a corresponding <code>(T, T)</code> tensor of attention scores. These are the weights to use in the weighted average (linear combination) of the projected input embeddings.</li>
</ul>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:46:54.776945Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:46:54.766579Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="49">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># weighted average (linear combination) of the projected input embeddings</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> w <span class="op">@</span> v</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>out.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="49">
<pre><code>torch.Size([2, 16, 8])</code></pre>
</div>
</div>
<ul>
<li>In summary, for a single attention head, an input embedding tensor of shape <code>(B, T, C)</code> was transformed to an output tensor of shape <code>(B, T, H)</code>.</li>
</ul>
<section id="multi-head-attention" class="level2">
<h2 class="anchored" data-anchor-id="multi-head-attention">Multi Head Attention</h2>
<ul>
<li>There are multiple attention heads, each with their own independent queries, keys, values.</li>
<li>Each attention head takes the input embeddings of shape <code>(B, T, C)</code> and produces an output <code>(B, T, H)</code>.</li>
<li>Concatenate the outputs from each head so that the concatenated tensor is back to the original input shape <code>(B, T, C)</code>.</li>
<li>Once we have the concatenated output tensor, we put it through a linear projection, <code>nn.Linear(embed_dim, embed_dim)</code> to get the output from the multi head attention: a tensor of shape <code>(B, T, C)</code>.</li>
</ul>
</section>
</section>
<section id="feed-forward-layer-ffn" class="level1">
<h1>Feed forward layer (FFN)</h1>
<ul>
<li>The output from the Multi-head attention is <code>(B, T, C)</code>.</li>
<li>This is then fed through a 2 layer feed forward network (FFN).</li>
<li>Rule of thumb is for the first layer to have a hidden size of 4 times the embedding dimension</li>
<li>often <code>nn.GELU()</code> (smoother version of RELU) is used for the non-linearity.</li>
<li>Usually <code>nn.Linear</code> is applied to a tensor of shape <code>(batch_size, input_dim)</code> and acts on each row/vector independently.
<ul>
<li>But here we are applying it to a tensor of shape <code>(B, T, C)</code>. The layer acts on all the input embeddings and sequences independently</li>
</ul></li>
<li>The output of this FFN is <code>(B, T, C)</code></li>
</ul>
<p>The only thing we have not mentioned is the use of <a href="https://arxiv.org/pdf/1607.06450.pdf">Layer Normalization</a> and <a href="https://arxiv.org/pdf/1512.03385.pdf">Skip connections</a>. These are typical tricks to improve training of networks. It will become more clear how they are used in the next section when we put it all together in the code.</p>
<p><img src="imgs/transformer_architecture.png" class="img-fluid"></p>
</section>
<section id="putting-it-all-together" class="level1">
<h1>Putting it all Together</h1>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:46:55.893150Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:46:55.837178Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="50">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> math <span class="im">import</span> sqrt</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">"bert-base-uncased"</span>)</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Config:</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    vocab_size <span class="op">=</span> tokenizer.vocab_size</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    seq_length <span class="op">=</span> <span class="dv">128</span>  <span class="co"># will also denote as T i.e. "time dimension"</span></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    batch_size <span class="op">=</span> <span class="dv">256</span>  <span class="co"># will also denote as B</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    embed_dim <span class="op">=</span> <span class="dv">64</span>  <span class="co"># will also denote as C i.e. "channel dimension"</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    num_heads <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>    head_dim <span class="op">=</span> embed_dim <span class="op">//</span> num_heads  <span class="co">#  will also denote as H</span></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>    dropout_prob <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    num_transformer_layers <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Embeddings(nn.Module):</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.token_embd <span class="op">=</span> nn.Embedding(config.vocab_size, config.embed_dim)</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos_embd <span class="op">=</span> nn.Embedding(config.seq_length, config.embed_dim)</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.dropout_prob)</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_norm <span class="op">=</span> nn.LayerNorm(config.embed_dim)</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is B,T --&gt; the tensor of token input_ids</span></span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>        seq_length <span class="op">=</span> x.size(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>        token_embeddings <span class="op">=</span> <span class="va">self</span>.token_embd(x)  <span class="co"># (B, T, C)</span></span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>        positional_embeddings <span class="op">=</span> <span class="va">self</span>.pos_embd(torch.arange(start<span class="op">=</span><span class="dv">0</span>, end<span class="op">=</span>seq_length, step<span class="op">=</span><span class="dv">1</span>, device<span class="op">=</span>device))  <span class="co"># (T, C)</span></span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> token_embeddings <span class="op">+</span> positional_embeddings  <span class="co"># (B, T, C)</span></span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.layer_norm(x)  <span class="co"># (B, T, C)</span></span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)  <span class="co"># (B, T, C)</span></span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> AttentionHead(nn.Module):</span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config, mask<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mask <span class="op">=</span> mask</span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.query <span class="op">=</span> nn.Linear(config.embed_dim, config.head_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.key <span class="op">=</span> nn.Linear(config.embed_dim, config.head_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb27-49"><a href="#cb27-49" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.value <span class="op">=</span> nn.Linear(config.embed_dim, config.head_dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb27-50"><a href="#cb27-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">"tril"</span>, torch.tril(torch.ones(config.seq_length, config.seq_length)))</span>
<span id="cb27-51"><a href="#cb27-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.dropout_prob)</span>
<span id="cb27-52"><a href="#cb27-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-53"><a href="#cb27-53" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb27-54"><a href="#cb27-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is (B, T, C)</span></span>
<span id="cb27-55"><a href="#cb27-55" aria-hidden="true" tabindex="-1"></a>        b, t, c <span class="op">=</span> x.shape</span>
<span id="cb27-56"><a href="#cb27-56" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> <span class="va">self</span>.query(x)  <span class="co"># (B, T, H)</span></span>
<span id="cb27-57"><a href="#cb27-57" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> <span class="va">self</span>.key(x)  <span class="co"># (B, T, H)</span></span>
<span id="cb27-58"><a href="#cb27-58" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> <span class="va">self</span>.value(x)  <span class="co"># (B, T, H)</span></span>
<span id="cb27-59"><a href="#cb27-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-60"><a href="#cb27-60" aria-hidden="true" tabindex="-1"></a>        dim_k <span class="op">=</span> k.shape[<span class="op">-</span><span class="dv">1</span>]  <span class="co"># i.e. head dimension</span></span>
<span id="cb27-61"><a href="#cb27-61" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">/</span> sqrt(dim_k)  <span class="co"># (B, T, T)</span></span>
<span id="cb27-62"><a href="#cb27-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.mask:</span>
<span id="cb27-63"><a href="#cb27-63" aria-hidden="true" tabindex="-1"></a>            w <span class="op">=</span> w.masked_fill(<span class="va">self</span>.tril[:t, :t] <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">"-inf"</span>))  <span class="co"># (B, T, T)</span></span>
<span id="cb27-64"><a href="#cb27-64" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> F.softmax(w, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># (B, T, T)</span></span>
<span id="cb27-65"><a href="#cb27-65" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> <span class="va">self</span>.dropout(w)  <span class="co"># good for regularization</span></span>
<span id="cb27-66"><a href="#cb27-66" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> w <span class="op">@</span> v  <span class="co"># (B, T, H)</span></span>
<span id="cb27-67"><a href="#cb27-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb27-68"><a href="#cb27-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-69"><a href="#cb27-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-70"><a href="#cb27-70" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb27-71"><a href="#cb27-71" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config, mask<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb27-72"><a href="#cb27-72" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb27-73"><a href="#cb27-73" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_heads <span class="op">=</span> nn.ModuleList([AttentionHead(config, mask) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.num_heads)])</span>
<span id="cb27-74"><a href="#cb27-74" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_proj <span class="op">=</span> nn.Linear(config.embed_dim, config.embed_dim)</span>
<span id="cb27-75"><a href="#cb27-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-76"><a href="#cb27-76" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb27-77"><a href="#cb27-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># each input tensor x has shape (B, T, C)</span></span>
<span id="cb27-78"><a href="#cb27-78" aria-hidden="true" tabindex="-1"></a>        <span class="co"># each attention head, head(x) is of shape (B, T, H)</span></span>
<span id="cb27-79"><a href="#cb27-79" aria-hidden="true" tabindex="-1"></a>        <span class="co"># concat these along the last dimension to get (B, T, C)</span></span>
<span id="cb27-80"><a href="#cb27-80" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.concat([head(x) <span class="cf">for</span> head <span class="kw">in</span> <span class="va">self</span>.attention_heads], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb27-81"><a href="#cb27-81" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.linear_proj(x)  <span class="co"># (B, T, C)</span></span>
<span id="cb27-82"><a href="#cb27-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-83"><a href="#cb27-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-84"><a href="#cb27-84" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeedForwardNetwork(nn.Module):</span>
<span id="cb27-85"><a href="#cb27-85" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb27-86"><a href="#cb27-86" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb27-87"><a href="#cb27-87" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer1 <span class="op">=</span> nn.Linear(config.embed_dim, <span class="dv">4</span> <span class="op">*</span> config.embed_dim)</span>
<span id="cb27-88"><a href="#cb27-88" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer2 <span class="op">=</span> nn.Linear(<span class="dv">4</span> <span class="op">*</span> config.embed_dim, config.embed_dim)</span>
<span id="cb27-89"><a href="#cb27-89" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gelu <span class="op">=</span> nn.GELU()</span>
<span id="cb27-90"><a href="#cb27-90" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.dropout_prob)</span>
<span id="cb27-91"><a href="#cb27-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-92"><a href="#cb27-92" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb27-93"><a href="#cb27-93" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is (B, T, C)</span></span>
<span id="cb27-94"><a href="#cb27-94" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.layer1(x)  <span class="co"># (B, T, 4C)</span></span>
<span id="cb27-95"><a href="#cb27-95" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.gelu(x)  <span class="co"># (B, T, 4C)</span></span>
<span id="cb27-96"><a href="#cb27-96" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.layer2(x)  <span class="co"># (B, T, C)</span></span>
<span id="cb27-97"><a href="#cb27-97" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)  <span class="co"># (B, T, C)</span></span>
<span id="cb27-98"><a href="#cb27-98" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb27-99"><a href="#cb27-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-100"><a href="#cb27-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-101"><a href="#cb27-101" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerBlock(nn.Module):</span>
<span id="cb27-102"><a href="#cb27-102" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config, mask<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb27-103"><a href="#cb27-103" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb27-104"><a href="#cb27-104" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mha <span class="op">=</span> MultiHeadAttention(config, mask)</span>
<span id="cb27-105"><a href="#cb27-105" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ffn <span class="op">=</span> FeedForwardNetwork(config)</span>
<span id="cb27-106"><a href="#cb27-106" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_norm_1 <span class="op">=</span> nn.LayerNorm(config.embed_dim)</span>
<span id="cb27-107"><a href="#cb27-107" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layer_norm_2 <span class="op">=</span> nn.LayerNorm(config.embed_dim)</span>
<span id="cb27-108"><a href="#cb27-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-109"><a href="#cb27-109" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb27-110"><a href="#cb27-110" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is (B, T, C)</span></span>
<span id="cb27-111"><a href="#cb27-111" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mha(<span class="va">self</span>.layer_norm_1(x))  <span class="co"># (B, T, C)</span></span>
<span id="cb27-112"><a href="#cb27-112" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffn(<span class="va">self</span>.layer_norm_2(x))  <span class="co"># (B, T, C)</span></span>
<span id="cb27-113"><a href="#cb27-113" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb27-114"><a href="#cb27-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-115"><a href="#cb27-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-116"><a href="#cb27-116" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Transformer(nn.Module):</span>
<span id="cb27-117"><a href="#cb27-117" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config, mask<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb27-118"><a href="#cb27-118" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb27-119"><a href="#cb27-119" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embeddings <span class="op">=</span> Embeddings(config)</span>
<span id="cb27-120"><a href="#cb27-120" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transformer_layers <span class="op">=</span> nn.ModuleList([TransformerBlock(config, mask) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.num_transformer_layers)])</span>
<span id="cb27-121"><a href="#cb27-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-122"><a href="#cb27-122" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb27-123"><a href="#cb27-123" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is shape (B, T). It is the output from a tokenizer</span></span>
<span id="cb27-124"><a href="#cb27-124" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.embeddings(x)  <span class="co"># (B, T, C)</span></span>
<span id="cb27-125"><a href="#cb27-125" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.transformer_layers:</span>
<span id="cb27-126"><a href="#cb27-126" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x)  <span class="co"># (B, T, C)</span></span>
<span id="cb27-127"><a href="#cb27-127" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x  <span class="co"># (B, T, C)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>The inputs of shape <code>(B, T)</code> can be passed through the transformer to produce a tensor of shape <code>(B, T, C</code>).</li>
<li>The final embeddings are “context” aware.</li>
</ul>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:46:57.306009Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:46:57.269671Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="51">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>Transformer(Config).to(device)(inputs.to(device)).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>torch.Size([2, 16, 64])</code></pre>
</div>
</div>
</section>
<section id="training-decoder-for-next-token-prediction" class="level1">
<h1>Training Decoder For Next Token Prediction</h1>
<ul>
<li>This code is meant to be a small “unit test” to see if we can train a simple model for next token prediction.</li>
<li>It’s not meant to be a “good” model, but something to refer to for educational purposes.</li>
<li>We will use the dataset from the paper <a href="https://arxiv.org/abs/2305.07759">TinyStories: How Small Can Language Models Be and Still Speak Coherent English?</a> <span class="citation" data-cites="eldan2023tinystories">(<a href="#ref-eldan2023tinystories" role="doc-biblioref">Eldan and Li 2023</a>)</span>.</li>
<li>Split the dataset into chunks where the input is the sequence of tokens of shape <code>(B, T)</code>.</li>
<li>The corresponding target tensor is of shape <code>(B, T)</code> and is the input sequence, right shifted.</li>
<li>Add a classifier layer to the transformer decoder to predict the next token from the vocabulary.</li>
</ul>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:47:06.544645Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:47:05.887075Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="52">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"roneneldan/TinyStories"</span>)[<span class="st">"train"</span>]</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> dataset.select(<span class="bu">range</span>(<span class="dv">500000</span>))  <span class="co"># decrease/increase to fewer data points to speed up training</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:47:42.206297Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:47:08.492716Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="53">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize(element):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Increase max_length by 1 to get the next token</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> tokenizer(</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>        element[<span class="st">"text"</span>],</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>        truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>        max_length<span class="op">=</span>Config.seq_length <span class="op">+</span> <span class="dv">1</span>,</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span><span class="st">"max_length"</span>,</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>        return_overflowing_tokens<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>        return_length<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>        add_special_tokens<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>    input_batch <span class="op">=</span> []</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>    target_batch <span class="op">=</span> []</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> length, input_ids <span class="kw">in</span> <span class="bu">zip</span>(outputs[<span class="st">"length"</span>], outputs[<span class="st">"input_ids"</span>]):</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> length <span class="op">==</span> Config.seq_length <span class="op">+</span> <span class="dv">1</span>:</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>            input_batch.append(input_ids[:<span class="op">-</span><span class="dv">1</span>])  <span class="co"># Exclude the last token for input</span></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>            target_batch.append(input_ids[<span class="dv">1</span>:])  <span class="co"># Exclude the first token for target</span></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {<span class="st">"input_ids"</span>: input_batch, <span class="st">"labels"</span>: target_batch}</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>tokenized_datasets <span class="op">=</span> dataset.<span class="bu">map</span>(tokenize, batched<span class="op">=</span><span class="va">True</span>, remove_columns<span class="op">=</span>dataset.column_names, num_proc<span class="op">=</span><span class="dv">8</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:47:42.206688Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:47:42.206129Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="54">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.convert_ids_to_tokens(tokenized_datasets[<span class="dv">0</span>][<span class="st">"input_ids"</span>]))</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(tokenizer.convert_ids_to_tokens(tokenized_datasets[<span class="dv">0</span>][<span class="st">"labels"</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>['one', 'day', ',', 'a', 'little', 'girl', 'named', 'lily', 'found', 'a', 'needle', 'in', 'her', 'room', '.', 'she', 'knew', 'it', 'was', 'difficult', 'to', 'play', 'with', 'it', 'because', 'it', 'was', 'sharp', '.', 'lily', 'wanted', 'to', 'share', 'the', 'needle', 'with', 'her', 'mom', ',', 'so', 'she', 'could', 'se', '##w', 'a', 'button', 'on', 'her', 'shirt', '.', 'lily', 'went', 'to', 'her', 'mom', 'and', 'said', ',', '"', 'mom', ',', 'i', 'found', 'this', 'needle', '.', 'can', 'you', 'share', 'it', 'with', 'me', 'and', 'se', '##w', 'my', 'shirt', '?', '"', 'her', 'mom', 'smiled', 'and', 'said', ',', '"', 'yes', ',', 'lily', ',', 'we', 'can', 'share', 'the', 'needle', 'and', 'fix', 'your', 'shirt', '.', '"', 'together', ',', 'they', 'shared', 'the', 'needle', 'and', 'se', '##wed', 'the', 'button', 'on', 'lily', "'", 's', 'shirt', '.', 'it', 'was', 'not', 'difficult', 'for', 'them', 'because', 'they', 'were', 'sharing']
['day', ',', 'a', 'little', 'girl', 'named', 'lily', 'found', 'a', 'needle', 'in', 'her', 'room', '.', 'she', 'knew', 'it', 'was', 'difficult', 'to', 'play', 'with', 'it', 'because', 'it', 'was', 'sharp', '.', 'lily', 'wanted', 'to', 'share', 'the', 'needle', 'with', 'her', 'mom', ',', 'so', 'she', 'could', 'se', '##w', 'a', 'button', 'on', 'her', 'shirt', '.', 'lily', 'went', 'to', 'her', 'mom', 'and', 'said', ',', '"', 'mom', ',', 'i', 'found', 'this', 'needle', '.', 'can', 'you', 'share', 'it', 'with', 'me', 'and', 'se', '##w', 'my', 'shirt', '?', '"', 'her', 'mom', 'smiled', 'and', 'said', ',', '"', 'yes', ',', 'lily', ',', 'we', 'can', 'share', 'the', 'needle', 'and', 'fix', 'your', 'shirt', '.', '"', 'together', ',', 'they', 'shared', 'the', 'needle', 'and', 'se', '##wed', 'the', 'button', 'on', 'lily', "'", 's', 'shirt', '.', 'it', 'was', 'not', 'difficult', 'for', 'them', 'because', 'they', 'were', 'sharing', 'and']</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:47:42.207654Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:47:42.206453Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="55">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LanguageModel(nn.Module):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transformer <span class="op">=</span> Transformer(config, mask<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.classifier <span class="op">=</span> nn.Linear(config.embed_dim, tokenizer.vocab_size)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x is (B, T) the token ids</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.transformer(x)  <span class="co"># (B, T, C)</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.classifier(x)  <span class="co"># (B, T, V)</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:47:42.208622Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:47:42.206842Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="56">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LanguageModel(Config).to(device)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters()) <span class="op">/</span> <span class="fl">1e6</span>, <span class="st">"M parameters"</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>4.144826 M parameters</code></pre>
</div>
</div>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:47:42.208708Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:47:42.207262Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="57">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_text(prompt, max_tokens<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> tokenizer(</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>        [prompt],</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>        truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>        max_length<span class="op">=</span>Config.seq_length,</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>        add_special_tokens<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> torch.tensor(inputs.input_ids).to(device)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_tokens):</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> model(inputs)  <span class="co"># (B, T, V)</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># convert logits to probabilities and only consider the probabilities for the last token in the sequence i.e. predict next token</span></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a>        probs <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :].softmax(dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># sample a token from the distribution over the vocabulary</span></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>        idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> torch.cat([inputs, idx_next], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer.decode(inputs[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Since we have not trained the model yet this output should be complete random garbage tokens.</p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T18:47:42.208859Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:47:42.207459Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="58">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(generate_text(<span class="st">"Once upon"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>once upon align tiltlm nicole speech quintet outsideials 1833 1785 asteroid exit jim caroline 19th 分 tomatoआ mt joanne ball busted hear hears neighbourhoods twitterouringbis maoma 貝 oven williams [unused646] presidential [unused618] [unused455]版tish gavin accountability stanford materials chung avoids unstable hyde culinary گ catalonia versatile gradient gross geography porn justice contributes deposition robotics 00pm showcased current laying b aixroudzko rooney abrahamhedron sideways postseason grossed conviction overheard crowley said warehouses heights times arising 80 reeve deptrned noelle fingered pleistocene pushed rock buddhist [unused650] brunette nailed upstream [unused86] ufc bolts鈴 grounds</code></pre>
</div>
</div>
<ul>
<li>This diagram helps me understand how the input sequences and the target sequences (right shifted) are used during training</li>
</ul>
<p><img src="imgs/next_token_prediction.png" class="img-fluid"></p>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T19:01:23.984283Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T18:47:42.207738Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="59">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>):</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> []</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>    loop <span class="op">=</span> tqdm(<span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(tokenized_datasets), Config.batch_size))</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> loop:</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.tensor(tokenized_datasets[i : i <span class="op">+</span> Config.batch_size][<span class="st">"input_ids"</span>]).to(device)  <span class="co"># (B, T)</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> torch.tensor(tokenized_datasets[i : i <span class="op">+</span> Config.batch_size][<span class="st">"labels"</span>]).to(device)  <span class="co"># (B, T)</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> model(x)  <span class="co"># (B, T, V)</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>        b, t, v <span class="op">=</span> logits.size()</span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> logits.view(b <span class="op">*</span> t, v)  <span class="co"># (B*T, V)</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>        target <span class="op">=</span> target.view(b <span class="op">*</span> t)  <span class="co"># B*T</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> F.cross_entropy(logits, target)</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>        train_loss.append(loss.item())</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a>            avg_loss <span class="op">=</span> np.mean(train_loss)</span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>            train_loss <span class="op">=</span> []</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>            loop.set_description(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, Avg Loss: </span><span class="sc">{</span>avg_loss<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Epoch 0, Avg Loss: 2.1089: 100%|██████████| 4357/4357 [13:41&lt;00:00,  5.30it/s]</code></pre>
</div>
</div>
<ul>
<li>Let’s try generating text with the trained model now:</li>
</ul>
<div class="cell" data-executetime="{&quot;end_time&quot;:&quot;2024-02-03T19:01:24.448717Z&quot;,&quot;start_time&quot;:&quot;2024-02-03T19:01:23.983532Z&quot;}" data-vscode="{&quot;languageId&quot;:&quot;python&quot;}" data-execution_count="60">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(generate_text(<span class="st">"Once upon"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>once upon a time, there was a little girl named maggie who loved watching movies in her neighborhood. today, ellie decided to do it was a sunny day. her mommy asked, " what does it means you fill the start? " her mommy the thief played with her very look at it. " the cop replied, " i want to get dessert together! " sarah watched together and took some of her pencils and sugar. mommy took a look so carefully together. they played together until it had something shiny</code></pre>
</div>
</div>
<p>Not quite GPT-4 performance, lol!</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-TheIllustratedTransformerGlob" class="csl-entry" role="listitem">
Alammar, Jay. 2018. <span>“The Illustrated Transformer.”</span> <a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a>.
</div>
<div id="ref-cho2024transformerexplainerinteractivelearning" class="csl-entry" role="listitem">
Cho, Aeree, Grace C. Kim, Alexander Karpekov, Alec Helbling, Zijie J. Wang, Seongmin Lee, Benjamin Hoover, and Duen Horng Chau. 2024. <span>“Transformer Explainer: Interactive Learning of Text-Generative Models.”</span> <a href="https://arxiv.org/abs/2408.04619">https://arxiv.org/abs/2408.04619</a>.
</div>
<div id="ref-eldan2023tinystories" class="csl-entry" role="listitem">
Eldan, Ronen, and Yuanzhi Li. 2023. <span>“TinyStories: How Small Can Language Models Be and Still Speak Coherent English?”</span> <em>arXiv Preprint arXiv:2305.07759</em>. <a href="https://arxiv.org/abs/2305.07759">https://arxiv.org/abs/2305.07759</a>.
</div>
<div id="ref-karpathy_youtube_2023_gpt" class="csl-entry" role="listitem">
Karpathy, Andrej. 2023. <span>“Let’s Build GPT: From Scratch, in Code, Spelled Out.”</span> YouTube. <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY">https://www.youtube.com/watch?v=kCc8FmEb1nY</a>.
</div>
<div id="ref-SebastianRaschkaUnderstandingAttention" class="csl-entry" role="listitem">
Raschka, Sebastian. 2024. <span>“Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs.”</span> <a href="https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention">https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention</a>.
</div>
<div id="ref-OmarSansevieroBlogRandomTransformer" class="csl-entry" role="listitem">
Sanseviero, Omar. 2024. <span>“The Random Transformer.”</span> <a href="https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/">https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/</a>.
</div>
<div id="ref-tunstall2022natural" class="csl-entry" role="listitem">
Tunstall, Lewis, Leandro Von Werra, and Thomas Wolf. 2022. <em>Natural Language Processing with Transformers</em>. " O’Reilly Media, Inc.". <a href="https://www.oreilly.com/library/view/natural-language-processing/9781098136789/">https://www.oreilly.com/library/view/natural-language-processing/9781098136789/</a>.
</div>
<div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <em>Advances in Neural Information Processing Systems</em> 30. <a href="https://www.oreilly.com/library/view/natural-language-processing/9781098136789/">https://www.oreilly.com/library/view/natural-language-processing/9781098136789/</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>