{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7908ac58af685d94",
   "metadata": {
    "collapsed": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Using Modal to Transcribe YouTube Videos with Whisper\n",
    "author: Chris Levy\n",
    "date: '2024-04-15'\n",
    "date-modified: '2024-04-15'\n",
    "image: python_cloud.png\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: show\n",
    "    page-layout: full\n",
    "include-in-header:\n",
    "  - text: |\n",
    "      <style>\n",
    "      .cell-output-stdout code {\n",
    "        word-break: break-wor !important;\n",
    "        white-space: pre-wrap !important;\n",
    "      }\n",
    "      </style>\n",
    "bibliography: ../../bibliography.bib\n",
    "resources:\n",
    "    - imgs/modal.mp4\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d9c6008d52926",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Intro\n",
    "\n",
    "I have been following [Modal](https://modal.com/) for a while but never used it until last week. I recently wrote a [blog post](https://drchrislevy.github.io/posts/intro_fine_tune/intro_fine_tune.html) on using \n",
    "Axolotl to fine-tune a decoder LLM for the first time. For that work I needed to transcribe some YouTube podcasts with [OpenAI Whisper](https://github.com/openai/whisper) model. I figured it would be a cool use case for Modal and a chance to learn something new. \n",
    "\n",
    "I'm no stranger to parallel processing with Python. I have used [celery](https://docs.celeryq.dev/en/v5.3.6/getting-started/introduction.html) extensively with a Redis backend in production for doing all sorts of things, including deployment of ML models. It's actually given me a good understanding of tasks, queues, concurrency, CPU/MEM usage, Redis, and so on. But I didn't want to have to deal with all that for this personal project. I didn't want to think about infrastructure, Kubernetes, helm charts, node groups, ec2 instances, etc. I just wanted to write some Python code and get the job done. \n",
    "\n",
    "In this post I will describe how I used Modal to accomplish this task. I'm no expert in using Modal because this is the first project I used it on.\n",
    "But I did love every bit of it. Honestly, Modal is amazing. It's pure magic. I can not wait to use it for my next project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b005d38bf875a21",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setup\n",
    "\n",
    "First I signed up and [logged in](https://modal.com/login?next=%2Fhome) via GitHub.\n",
    "Then I simply followed the instructions on screen.\n",
    "\n",
    "![](static_blog_imgs/model-intro.png)\n",
    "\n",
    "I created a virtual env and installed `modal` as well as `python-dotenv` and `pytube`.\n",
    "\n",
    "```\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "pip install python-dotenv\n",
    "pip install pytube\n",
    "pip install modal\n",
    "python3 -m modal setup\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee14b05673ad53",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Task Description\n",
    "\n",
    "The [Modal docs](https://modal.com/docs/guide) are amazing, so I'm not going to repeat that here. \n",
    "Just go read the docs and try some of the simple [hello world](https://modal.com/docs/examples/hello_world) type of examples. \n",
    "\n",
    "For the task of transcribing a YouTube video with Whisper I want a Python function that will do the following:\n",
    "\n",
    "- takes a YouTube video `url` as input --> `https://www.youtube.com/watch?v=<video_id>`.\n",
    "- checks if the url i.e. `<video_id>` has already been processed by checking if the transcript json file `<video_id>.json` is already in s3. If so, then exit the function.\n",
    "- save the audio as a mp4 file to s3: `s3://<s3-bucket>/youtube_downloader/audio_files/<video_id>.mp4`.\n",
    "- save audio transcript to s3: `s3://<s3-bucket>/youtube_downloader/transcripts/<video_id>.json`.\n",
    "\n",
    "Then I can use the Modal `map` feature to fan out this function/task in parallel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7512814bfdc07cc7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Show me the Code\n",
    "\n",
    "There are various ways to use [secrets in Modal](https://modal.com/docs/guide/secrets#secrets).\n",
    "One simple way is through `dotenv`. I defined some environment variables in  a local `.env` file.\n",
    "\n",
    "```\n",
    "S3_BUCKET=<my-s3-bucket>\n",
    "S3_PREFIX=youtube_downloader/\n",
    "AWS_ACCESS_KEY_ID=<my access key>\n",
    "AWS_SECRET_ACCESS_KEY=<my secret access key>\n",
    "```\n",
    "\n",
    "Here is the code in a file `transcribe_video.py`:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import modal\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "S3_BUCKET = os.environ[\"S3_BUCKET\"]\n",
    "S3_PREFIX = os.environ[\"S3_PREFIX\"]\n",
    "stub = modal.Stub(\"video-transcription\")\n",
    "\n",
    "image = (\n",
    "    modal.Image.debian_slim(python_version=\"3.10\")\n",
    "    .run_commands(\"apt-get update\", \"apt update && apt install ffmpeg -y\")\n",
    "    .pip_install(\n",
    "        \"openai-whisper\",\n",
    "        \"pytube\",\n",
    "        \"boto3\",\n",
    "        \"python-dotenv\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def upload_file(filename, s3_filename, bucket_name):\n",
    "    import boto3\n",
    "\n",
    "    client = boto3.client(\"s3\")\n",
    "    headers = {\"ACL\": \"public-read\"}\n",
    "    headers[\"CacheControl\"] = \"max-age %d\" % (3600 * 24 * 365)\n",
    "    client.upload_file(filename, bucket_name, s3_filename, ExtraArgs=headers)\n",
    "    return f\"https://{bucket_name}.s3.amazonaws.com/{s3_filename}\"\n",
    "\n",
    "\n",
    "def check_file(filename, bucket_name):\n",
    "    import boto3\n",
    "    from botocore.errorfactory import ClientError\n",
    "\n",
    "    client = boto3.client(\"s3\")\n",
    "    file_exists = True\n",
    "    try:\n",
    "        client.head_object(Bucket=bucket_name, Key=filename)\n",
    "    except ClientError:\n",
    "        file_exists = False\n",
    "    return file_exists\n",
    "\n",
    "\n",
    "def upload_fileobj(file_object, s3_filename, bucket_name):\n",
    "    import boto3\n",
    "\n",
    "    client = boto3.client(\"s3\")\n",
    "    headers = {\"ACL\": \"public-read\"}\n",
    "    headers[\"CacheControl\"] = \"max-age %d\" % (3600 * 24 * 365)\n",
    "    client.upload_fileobj(file_object, bucket_name, s3_filename, ExtraArgs=headers)\n",
    "    return f\"https://{bucket_name}.s3.amazonaws.com/{s3_filename}\"\n",
    "\n",
    "\n",
    "def dict_to_s3(record, s3_filename, bucket_name):\n",
    "    in_mem_file = io.BytesIO()\n",
    "    in_mem_file.write(json.dumps(record, sort_keys=True, indent=4).encode())\n",
    "    in_mem_file.seek(0)\n",
    "    upload_fileobj(in_mem_file, s3_filename, bucket_name)\n",
    "    return f\"https://{bucket_name}.s3.amazonaws.com/{s3_filename}\"\n",
    "\n",
    "\n",
    "@stub.function(\n",
    "    image=image,\n",
    "    secrets=[modal.Secret.from_dotenv()],\n",
    "    cpu=2,\n",
    "    memory=1024 * 3,\n",
    "    gpu=\"A10G\",\n",
    "    timeout=600,\n",
    ")\n",
    "def process_video(url):\n",
    "    import re\n",
    "    from pytube import YouTube\n",
    "    import whisper\n",
    "\n",
    "    video_id = re.search(r\"v=([a-zA-Z0-9_-]{11})\", url).group(1)\n",
    "    file_name_audio = f\"{video_id}.mp4\"\n",
    "    s3_file_name_audio = os.path.join(S3_PREFIX, \"audio_files\", file_name_audio)\n",
    "    s3_file_name_transcript = os.path.join(S3_PREFIX, \"transcripts\", f\"{video_id}.json\")\n",
    "    if check_file(s3_file_name_transcript, S3_BUCKET):\n",
    "        print(f\"Already processed {s3_file_name_audio}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    yt = YouTube(url)\n",
    "    audio_stream = yt.streams.get_audio_only()\n",
    "    audio_stream.download(filename=file_name_audio)\n",
    "    upload_file(file_name_audio, s3_file_name_audio, S3_BUCKET)\n",
    "\n",
    "    # transcribe video\n",
    "    model = whisper.load_model(\"small\", device=\"cuda\")\n",
    "    audio = whisper.load_audio(file_name_audio)\n",
    "    # audio = whisper.pad_or_trim(audio)  # useful for debugging\n",
    "    result = model.transcribe(audio, fp16=True)\n",
    "    # for debugging in ipython shell\n",
    "    # modal.interact()\n",
    "    # import IPython\n",
    "    # IPython.embed()\n",
    "    return dict_to_s3(result, s3_file_name_transcript, S3_BUCKET)\n",
    "\n",
    "\n",
    "@stub.local_entrypoint()\n",
    "def main():\n",
    "    from pytube import Playlist\n",
    "    import random\n",
    "\n",
    "    all_urls = set()\n",
    "    for playlist_url in [\n",
    "        \"https://www.youtube.com/playlist?list=PL8xK8kBHHUX4NW8GqUsyFhBF_xCnzIdPe\",\n",
    "        \"https://www.youtube.com/playlist?list=PL8xK8kBHHUX7VsJPqv6OYp71Qj24zcTIr\",\n",
    "        \"https://www.youtube.com/playlist?list=PL8xK8kBHHUX5X-jGZlltoZOpv5sKXeGVV\",\n",
    "    ]:\n",
    "        p = Playlist(playlist_url)\n",
    "        for url in p.video_urls:\n",
    "            all_urls.add(url)\n",
    "\n",
    "    urls = random.sample(list(all_urls), 20)\n",
    "    for msg in process_video.map(urls):\n",
    "        print(msg)\n",
    "```\n",
    "\n",
    "You can execute it with Modal using `modal run transcribe_video.py`.\n",
    "\n",
    "{{< video imgs/modal.mp4 >}}\n",
    "\n",
    "\n",
    "I think there is more optimizations that could be done such as caching the model downloading but I have not looked into it.\n",
    "\n",
    "Some comments on the code:\n",
    "\n",
    "- define your own custom container `image` and install whatever you like on it.\n",
    "- for the task function `process_video` we simply decorate it with `@stub.function` and we can specify things such as the `image`, mem/cpu resources, secrets etc. In this case we can even run it on an A10 GPU for faster inference with Whisper. I increased the timeout because I was transcribing longer videos. \n",
    "- I am using `from pytube import Playlist` to list out the video urls in a given YouTube playlist within the `main` function. That logic runs on my local machine. You can add whatever logic you want there, different video urls, etc. The main idea is that `process_video` takes the video `url` as input. You can increase the sample size in ` urls = random.sample(list(all_urls), 20)` to whatever you like. This code, `modal run transcribe_video.py`,  can be run over and over again and any video that was already transcribed and saved to s3 will be skipped automatically. \n",
    "- `process_video.map(urls):` fans out the tasks and it runs entirely on Modal's cloud infrastructure. Not locally!\n",
    "\n",
    "The output is saved to s3.\n",
    "\n",
    "::: {#create-pod layout-ncol=1}\n",
    "![](static_blog_imgs/s3_audio_files.png)\n",
    "\n",
    "![](static_blog_imgs/s3_transcriptions.png)\n",
    ":::\n",
    "\n",
    "# Jumping into Ipython Shell for Interactive Debugging\n",
    "\n",
    "You can even jump into the `ipython shell` on the server at any point in the function logic to [debug](https://modal.com/docs/guide/developing-debugging#developing-and-debugging). That is pretty cool! Just remove the comments:\n",
    "\n",
    "```python\n",
    "# for debugging in ipython shell\n",
    "modal.interact()\n",
    "import IPython\n",
    "IPython.embed()\n",
    "```\n",
    "\n",
    "I also removed the comment `# audio = whisper.pad_or_trim(audio)  # useful for debugging` during debugging so the transcription was just for 30 seconds and hence the code runs much faster.\n",
    "\n",
    "Then just run the code in **interactive** mode with `modal run -i transcribe_video.py`\n",
    "\n",
    "I did this several times to check the output of `result = model.transcribe(audio, fp16=True)`.  I also debugged\n",
    "the files being saved locally on the image container. It gave the same feeling as developing locally but it was all running in the cloud! I did not have to get ffmpeg and OpenAI Whisper running locally on a GPU or mess around with cuda drivers. It just worked, surprisingly. Here are some screenshots of the interactive `ipython` shell:\n",
    "\n",
    "\n",
    "\n",
    "![](static_blog_imgs/modal_container_interactive.png)\n",
    "\n",
    "![](static_blog_imgs/modal_container_interactive2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7614e2a6e41719c3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "I hope this got you excited to go and try out Modal.\n",
    "They actually give you $30 of compute each month.\n",
    "And the dashboard is great. It can show you detailed logs, resource consumption, and the billing is very transparent (updated in real time).\n",
    "\n",
    "![](static_blog_imgs/dashboard1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecb20a3a306d3b6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839869b3aa9e948f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
