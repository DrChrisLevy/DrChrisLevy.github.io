{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6cebaaabc77c5364",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "---\n",
    "title: Function Calling with Hermes-2-Pro-Mistral-7B\n",
    "author: Chris Levy\n",
    "date: '2024-03-16'\n",
    "date-modified: '2024-03-16'\n",
    "image: \"imgs/llm_inference.jpeg\"\n",
    "toc: true\n",
    "format:\n",
    "  html: \n",
    "    code-fold: show\n",
    "    page-layout: full\n",
    "include-in-header:\n",
    "  - text: |\n",
    "      <style>\n",
    "      .cell-output-stdout code {\n",
    "        word-break: break-wor !important;\n",
    "        white-space: pre-wrap !important;\n",
    "      }\n",
    "      </style>\n",
    "bibliography: ../../bibliography.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5846853ac7bbda76",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this post we take a look at the function calling capabilities of the open source model\n",
    "`NousResearch/Hermes-2-Pro-Mistral-7B`  \n",
    "\n",
    "(@Hermes-2-Pro-Mistral-7B).\n",
    "\n",
    "\n",
    "- [Twitter announcement](https://x.com/Teknium1/status/1768023030843015208?s=20)\n",
    "- [GitHub Repo for Function Calling](https://github.com/NousResearch/Hermes-Function-Calling)\n",
    "- [Hugging Face Model Card](https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8897c8aacb4e36",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ENV Setup\n",
    "\n",
    "Start by creating a virtual environment: \n",
    "\n",
    "```\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "```\n",
    "\n",
    "Then install:\n",
    "\n",
    "```\n",
    "pip install openai\n",
    "pip install python-dotenv # or define your environment variables differently\n",
    "pip install langchain # utilities for converting functions to OpenAI tools format.\n",
    "```\n",
    "\n",
    "I also have:\n",
    "\n",
    "- an [OpenAI account](https://platform.openai.com/api-keys) with an API key.\n",
    "- Hugging Face Account, Access Token, and created [inference endpoint](https://huggingface.co/inference-endpoints/dedicated) with the model `NousResearch/Hermes-2-Pro-Mistral-7B`.\n",
    "\n",
    "In my `.env` file I have the following:\n",
    "```\n",
    "OPENAI_API_KEY=your_key\n",
    "HUGGING_FACE_ACCESS_TOKEN=your_key\n",
    "HUGGING_FACE_ENDPOINT_URL=url_for_endpoint\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df37c9456aacee9f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T04:21:17.589488Z",
     "start_time": "2024-03-16T04:21:17.581987Z"
    }
   },
   "outputs": [],
   "source": [
    "# | output: false\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "HUGGING_FACE_ACCESS_TOKEN = os.environ[\"HUGGING_FACE_ACCESS_TOKEN\"]\n",
    "HUGGING_FACE_ENDPOINT_URL = os.environ[\"HUGGING_FACE_ENDPOINT_URL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bbaa9d96c2153a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# LLM Inference Class\n",
    "\n",
    "In a previous [blog post](https://drchrislevy.github.io/posts/llm_inference_class/llm_inference.html) I discussed how we can use the OpenAI python client to run inference with open source models through services that are OpenAI compatible. I'm going to copy part of the code here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c5c7b941c5c43",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T04:21:18.385009Z",
     "start_time": "2024-03-16T04:21:17.851247Z"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "from langchain.tools import tool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from openai import OpenAI\n",
    "from openai._streaming import Stream\n",
    "from openai.types.chat.chat_completion import ChatCompletion\n",
    "from openai.types.chat.chat_completion_chunk import ChatCompletionChunk\n",
    "\n",
    "today = datetime.now().strftime(\"%A %Y-%m-%d\")\n",
    "\n",
    "\n",
    "class OpenAIChatCompletion:\n",
    "    clients: Dict = dict()\n",
    "\n",
    "    @classmethod\n",
    "    def _load_client(cls, base_url: Optional[str] = None, api_key: Optional[str] = None) -> OpenAI:\n",
    "        client_key = (base_url, api_key)\n",
    "        if OpenAIChatCompletion.clients.get(client_key) is None:\n",
    "            OpenAIChatCompletion.clients[client_key] = OpenAI(base_url=base_url, api_key=api_key)\n",
    "        return OpenAIChatCompletion.clients[client_key]\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        model: str,\n",
    "        messages: list,\n",
    "        base_url: Optional[str] = None,\n",
    "        api_key: Optional[str] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[ChatCompletion, Stream[ChatCompletionChunk]]:\n",
    "        # https://platform.openai.com/docs/api-reference/chat/create\n",
    "        # https://github.com/openai/openai-python\n",
    "        client = self._load_client(base_url, api_key)\n",
    "        return client.chat.completions.create(model=model, messages=messages, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988707e9830c2a7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Simply use it like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b11b917b0e15430b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T04:21:19.131104Z",
     "start_time": "2024-03-16T04:21:18.387315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-93FpmWcebHwAX2c8nhtUqjjWiofxt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1710562878, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_4f2ebda25a', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAIChatCompletion()\n",
    "print(llm(model=\"gpt-3.5-turbo-0125\", messages=[dict(role=\"user\", content=\"Hello!\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215d8f7ea59119a3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can also use the same code to run inference with `Hermes-2-Pro-Mistral-7B`.\n",
    "Now, you don't need to use an inference endpoint. You could use the transformers library directly\n",
    "and run it locally if you want. I'm choosing to run it as a Hugging Face inference endpoint\n",
    "because it's simple to set up. Remember you have different options for the [prompt format](https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B#prompt-format). I'm using the messages format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ade58c78494bdf39",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T04:21:29.468928Z",
     "start_time": "2024-03-16T04:21:19.133340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open source AI is crucial for several reasons. \n",
      "\n",
      "First, it promotes transparency and collaboration. As the source code is openly available, the AI community can review, critique, and improve the models, leading to better and more reliable results. Collaboration allows diverse perspectives and expertise to contribute to the development of the AI, leading to more innovative solutions.\n",
      "\n",
      "Second, open source AI encourages accessibility. By being available for free, it democratizes access to advanced AI technologies, enabling researchers, students, and small organizations with limited resources to access and utilize cutting-edge AI models.\n",
      "\n",
      "Third, it fosters rapid advancement in the AI field. The shared knowledge and resources in open source AI give developers a solid foundation on which to build new models or tailor existing ones to specific needs. This accelerates innovation and allows the field to progress at a faster rate than if proprietary models were dominant.\n",
      "\n",
      "Finally, open source AI promotes trust in AI systems. With transparent development and peer review, potential users and stakeholders can have confidence in the performance, reliability, and security of these AI models, making them more likely to adopt and integrate AI solutions into their workflows.\n",
      "\n",
      "In summary, open source AI is important because it promotes transparency and collaboration, encourages accessibility, fosters rapid advancement, and builds trust in AI systems, ultimately benefiting society as a whole by democratizing access to the technology and fueling innovation.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    llm(\n",
    "        model=\"tgi\",\n",
    "        api_key=HUGGING_FACE_ACCESS_TOKEN,\n",
    "        base_url=HUGGING_FACE_ENDPOINT_URL,\n",
    "        messages=[\n",
    "            dict(\n",
    "                role=\"system\",\n",
    "                content=\"You are an OpenSource LLM that rivals OpenAI GPT. Your goal in life is to bring open source AI to the masses.\",\n",
    "            ),\n",
    "            dict(role=\"user\", content=\"Explain why open source AI is so important.\"),\n",
    "        ],\n",
    "        max_tokens=2000,\n",
    "        temperature=1,\n",
    "    )\n",
    "    .choices[0]\n",
    "    .message.content\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0553b95f6dccc8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Function Calling Capabilities\n",
    "\n",
    "First we will define some dummy functions/tools which the LLM will have access to.\n",
    "I use `langchain` here to convert the Python functions to the `tools` format used\n",
    "by OpenAI. Note that `Hermes-2-Pro-Mistral-7B` can also use this same format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "694bba7bc801d2f1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T04:21:29.591896Z",
     "start_time": "2024-03-16T04:21:29.468034Z"
    }
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_weather_forecast(location: str, date: str) -> str:\n",
    "    \"\"\"\n",
    "    Provides a weather forecast for a given location and date.\n",
    "\n",
    "    Args:\n",
    "        location (str): The name of the city and state, e.g. 'San Francisco, CA'.\n",
    "        date (str): The date of the forecast in YYYY-MM-DD format, e.g. '2023-07-01'.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the weather forecast, e.g. 'Partly cloudy with a high of 72F (22C).'\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@tool\n",
    "def book_flight(\n",
    "    departure_city: str,\n",
    "    arrival_city: str,\n",
    "    departure_date: str,\n",
    "    return_date: str,\n",
    "    num_passengers: int,\n",
    "    cabin_class: str,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Book a round-trip flight for the given parameters.\n",
    "\n",
    "    Args:\n",
    "        departure_city (str): The full city name with the departure airport, e.g. \"Toronto\".\n",
    "        arrival_city (str): The full city name with the arrival airport, e.g. \"Austin\".\n",
    "        departure_date (str): The departure date in YYYY-MM-DD format.\n",
    "        return_date (str): The return date in YYYY-MM-DD format.\n",
    "        num_passengers (int): The number of passengers.\n",
    "        cabin_class (str): The cabin class, e.g. \"economy\", \"business\", \"first\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dict with the booking details including airline, flight numbers, price and booking confirmation code.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@tool\n",
    "def book_movie_tickets(movie_name: str, theater_name: str, date: str, time: str, num_tickets: int) -> dict:\n",
    "    \"\"\"\n",
    "    Book movie tickets for the given movie, theater, date, time, and number of tickets.\n",
    "    Args:\n",
    "        movie_name (str): The name of the movie.\n",
    "        theater_name (str): The name of the theater.\n",
    "        date (str): The date of the movie showing (YYYY-MM-DD).\n",
    "        time (str): The time of the movie showing (HH:MM).\n",
    "        num_tickets (int): The number of tickets to book.\n",
    "    Returns:\n",
    "        dict: Returns a dictionary with booking details if successful, otherwise returns a dictionary with an error message.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@tool\n",
    "def translate_text(text: str, target_language: str) -> str:\n",
    "    \"\"\"\n",
    "    Translate the given text into the specified target language.\n",
    "    Args:\n",
    "        text (str): The text to be translated.\n",
    "        target_language (str): The target language code (e.g., 'es' for Spanish, 'fr' for French).\n",
    "    Returns:\n",
    "        str: The translated text in the target language.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_recipe(dish_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns a recipe for the given dish name, optionally filtered by cuisine type and dietary restrictions.\n",
    "    Args:\n",
    "        dish_name (str): The name of the dish to get the recipe for.\n",
    "    Returns:\n",
    "        str: A string containing the recipe instructions.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@tool\n",
    "def solve_math_problem(problem: str) -> str:\n",
    "    \"\"\"\n",
    "    Solves a given math problem using a symbolic math library or calculator API.\n",
    "    Args:\n",
    "        problem (str): The math problem to solve, expressed as a string.\n",
    "                       diff(f(x),x) # derivative\n",
    "                       integrate(f(x), x) # integrate\n",
    "                       solve(3x=2,x) # solve for x\n",
    "    Returns:\n",
    "        str: The solution to the math problem.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@tool\n",
    "def send_slack_message(channel_name: str, message: str) -> bool:\n",
    "    \"\"\"\n",
    "    Send a message to a Slack channel.\n",
    "    Args:\n",
    "        channel_name (str): The name of the channel.\n",
    "        message (str): The message to be sent.\n",
    "    Returns:\n",
    "        bool: True if the message was sent successfully, False otherwise.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "functions = [\n",
    "    get_weather_forecast,\n",
    "    book_flight,\n",
    "    book_movie_tickets,\n",
    "    translate_text,\n",
    "    get_recipe,\n",
    "    solve_math_problem,\n",
    "    send_slack_message,\n",
    "]\n",
    "tools = [convert_to_openai_tool(f) for f in functions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76c716840526abc5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T04:21:29.596458Z",
     "start_time": "2024-03-16T04:21:29.593875Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'type': 'function',\n 'function': {'name': 'get_weather_forecast',\n  'description': \"get_weather_forecast(location: str, date: str) -> str - Provides a weather forecast for a given location and date.\\n\\n    Args:\\n        location (str): The name of the city and state, e.g. 'San Francisco, CA'.\\n        date (str): The date of the forecast in YYYY-MM-DD format, e.g. '2023-07-01'.\\n\\n    Returns:\\n        str: A string containing the weather forecast, e.g. 'Partly cloudy with a high of 72F (22C).'\",\n  'parameters': {'type': 'object',\n   'properties': {'location': {'type': 'string'}, 'date': {'type': 'string'}},\n   'required': ['location', 'date']}}}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7ffdba08513d8f6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T04:21:29.598624Z",
     "start_time": "2024-03-16T04:21:29.596400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'type': 'function',\n 'function': {'name': 'send_slack_message',\n  'description': 'send_slack_message(channel_name: str, message: str) -> bool - Send a message to a Slack channel.\\n    Args:\\n        channel_name (str): The name of the channel.\\n        message (str): The message to be sent.\\n    Returns:\\n        bool: True if the message was sent successfully, False otherwise.',\n  'parameters': {'type': 'object',\n   'properties': {'channel_name': {'type': 'string'},\n    'message': {'type': 'string'}},\n   'required': ['channel_name', 'message']}}}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b59066f68f1cf4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here is a list of questions to test out the function calling. For each question\n",
    "we have the text and the ground truth i.e. expected function/tool name and arguments.\n",
    "This way we can have a mini evaluation for how well the function calling works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7d9b009b7e0909b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T04:21:29.606420Z",
     "start_time": "2024-03-16T04:21:29.605299Z"
    }
   },
   "outputs": [],
   "source": [
    "questions = [\n",
    "    {\n",
    "        \"question\": \"What will the weather be like in Seattle, WA tomorrow?\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"get_weather_forecast\",\n",
    "                \"arguments\": {\n",
    "                    \"location\": \"Seattle, WA\",\n",
    "                    \"date\": (datetime.now() + timedelta(days=1)).strftime(\"%Y-%m-%d\"),\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What's the forecast for Miami for today?\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"get_weather_forecast\",\n",
    "                \"arguments\": {\"location\": \"Miami, FL\", \"date\": datetime.now().strftime(\"%Y-%m-%d\")},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Will I need an umbrella in New York City two days from now?\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"get_weather_forecast\",\n",
    "                \"arguments\": {\n",
    "                    \"location\": \"New York City, NY\",\n",
    "                    \"date\": (datetime.now() + timedelta(days=2)).strftime(\"%Y-%m-%d\"),\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Book me a round-trip flight from New York City to Los Angeles departing on June 15th and returning June 22nd for 2 passengers in economy class.\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"book_flight\",\n",
    "                \"arguments\": {\n",
    "                    \"departure_city\": \"NYC\",\n",
    "                    \"arrival_city\": \"LAX\",\n",
    "                    \"departure_date\": datetime(datetime.now().year, 6, 15).strftime(\"%Y-%m-%d\"),\n",
    "                    \"return_date\": datetime(datetime.now().year, 6, 22).strftime(\"%Y-%m-%d\"),\n",
    "                    \"num_passengers\": 2,\n",
    "                    \"cabin_class\": \"economy\",\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"I need to book a first class round-trip flight for 4 people from Chicago to Miami. We want to leave on December 1 and return on December 12.\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"book_flight\",\n",
    "                \"arguments\": {\n",
    "                    \"departure_city\": \"Chicago\",\n",
    "                    \"arrival_city\": \"Miami\",\n",
    "                    \"departure_date\": datetime(datetime.now().year, 12, 1).strftime(\"%Y-%m-%d\"),\n",
    "                    \"return_date\": datetime(datetime.now().year, 12, 12).strftime(\"%Y-%m-%d\"),\n",
    "                    \"num_passengers\": 4,\n",
    "                    \"cabin_class\": \"first\",\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"I want to book 3 tickets for The Super Mario Bros. Movie at AMC Empire 25 on April 7th at 7:30 PM.\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"book_movie_tickets\",\n",
    "                \"arguments\": {\n",
    "                    \"movie_name\": \"The Super Mario Bros. Movie\",\n",
    "                    \"theater_name\": \"AMC Empire 25\",\n",
    "                    \"date\": datetime(datetime.now().year, 4, 7).strftime(\"%Y-%m-%d\"),\n",
    "                    \"time\": \"19:30\",\n",
    "                    \"num_tickets\": 3,\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Book 2 tickets for Guardians of the Galaxy Vol. 3 at Regal Union Square on May 5th for the 9:45 PM show.\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"book_movie_tickets\",\n",
    "                \"arguments\": {\n",
    "                    \"movie_name\": \"Guardians of the Galaxy Vol. 3\",\n",
    "                    \"theater_name\": \"Regal Union Square\",\n",
    "                    \"date\": datetime(datetime.now().year, 5, 5).strftime(\"%Y-%m-%d\"),\n",
    "                    \"time\": \"21:45\",\n",
    "                    \"num_tickets\": 2,\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do you say 'Hello, how are you?' in Spanish?\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"translate_text\",\n",
    "                \"arguments\": {\"text\": \"Hello, how are you?\", \"target_language\": \"es\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Translate 'I love programming' to French.\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"translate_text\",\n",
    "                \"arguments\": {\"text\": \"I love programming\", \"target_language\": \"fr\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I make pesto?\",\n",
    "        \"tool_calls\": [{\"name\": \"get_recipe\", \"arguments\": {\"dish_name\": \"pesto\"}}],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What's a good vegan chili recipe?\",\n",
    "        \"tool_calls\": [{\"name\": \"get_recipe\", \"arguments\": {\"dish_name\": \"vegan chili\"}}],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Can you give me a recipe for chocolate chip cookies?\",\n",
    "        \"tool_calls\": [{\"name\": \"get_recipe\", \"arguments\": {\"dish_name\": \"chocolate chip cookies\"}}],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the result of integrating x^2 + 2x + 1 with respect to x?\",\n",
    "        \"tool_calls\": [{\"name\": \"solve_math_problem\", \"arguments\": {\"problem\": \"integrate(x^2 + 2x + 1, x)\"}}],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Solve the equation: 3x - 7 = 5x + 9\",\n",
    "        \"tool_calls\": [{\"name\": \"solve_math_problem\", \"arguments\": {\"problem\": \"solve(3x - 7 = 5x + 9, x)\"}}],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Calculate the derivative of sin(x^3) with respect to x.\",\n",
    "        \"tool_calls\": [{\"name\": \"solve_math_problem\", \"arguments\": {\"problem\": \"diff(sin(x^3), x)\"}}],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Send a message to the general channel on Slack saying 'Hello, world!'\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"send_slack_message\",\n",
    "                \"arguments\": {\"channel_name\": \"general\", \"message\": \"Hello, world!\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Send a message to the sales-team channel on Slack telling them to check their email\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"send_slack_message\",\n",
    "                \"arguments\": {\n",
    "                    \"channel_name\": \"sales-team\",\n",
    "                    \"message\": \"Please check your email.\",\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Send a message to the office-updates channel on Slack telling them to the food is here. \",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"name\": \"send_slack_message\",\n",
    "                \"arguments\": {\"channel_name\": \"office-updates\", \"message\": \"The food is here!\"},\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e174b428a21e1040",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T04:21:29.610652Z",
     "start_time": "2024-03-16T04:21:29.606526Z"
    }
   },
   "outputs": [],
   "source": [
    "random.shuffle(tools)\n",
    "random.shuffle(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443a7e1015120707",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## gpt-3.5-turbo-0125 Function Calling\n",
    "\n",
    "First we will use `gpt-3.5-turbo-0125` to extract the function name and arguments for each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a849123be9b5533e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T04:21:29.611855Z",
     "start_time": "2024-03-16T04:21:29.609788Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_tool_calls(resp):\n",
    "    resp = resp.choices[0].message\n",
    "    if resp.tool_calls:\n",
    "        final_tools = []\n",
    "        for tool_call in resp.tool_calls:\n",
    "            final_tools.append(\n",
    "                {\n",
    "                    \"name\": tool_call.function.name,\n",
    "                    \"arguments\": json.loads(tool_call.function.arguments),\n",
    "                }\n",
    "            )\n",
    "        return final_tools\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee6f5f7c87358e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "I'm also going to use GPT4 to check the \"correctness\" of the expected\n",
    "function arguments and the predicted/generated function arguments. \n",
    "It's not perfect but good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "811a16cd3f637430",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T04:21:29.628590Z",
     "start_time": "2024-03-16T04:21:29.611215Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_tool_call_arguments(expected, predicted):\n",
    "    # Ask GPT4 if the expected arguments and predicted arguments are \"the same\".\n",
    "    if expected[\"name\"] != predicted[\"name\"]:\n",
    "        return False\n",
    "    prompt = f\"\"\"\n",
    "Are the following queries approx equal. Use fuzzy matching for strings. Just looking for semantic similar. \n",
    "If so then return TRUE and only TRUE with no other explanation. \n",
    "Otherwise FALSE and explain.\n",
    "\n",
    "Expected Arguments: {expected['arguments']}\n",
    "Predicted Arguments: {predicted['arguments']}\n",
    "    \"\"\"\n",
    "    resp = llm(model=\"gpt-4-0125-preview\", messages=[dict(role=\"user\", content=prompt)])\n",
    "    if resp.choices[0].message.content.lower().strip() == \"true\":\n",
    "        return True, None\n",
    "    explanation = resp.choices[0].message.content.lower().strip()\n",
    "    return False, explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f68237b70b4ae25",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Okay, let's loop over the questions and use `gpt-3.5-turbo-0125` to extract the function name and arguments,\n",
    "and keep track of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9840f4e683329e50",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-16T04:21:29.620030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: Send a message to the office-updates channel on Slack telling them to the food is here. \n",
      "EXPECTED Tool Call: {'name': 'send_slack_message', 'arguments': {'channel_name': 'office-updates', 'message': 'The food is here!'}}\n",
      "GENERATED Tool Call: {'name': 'send_slack_message', 'arguments': {'channel_name': 'office-updates', 'message': 'The food is here! Enjoy your meal 🍕🥗'}}\n",
      "EXPLANATION: false \n",
      "\n",
      "the key 'channel_name' matches exactly in both arguments, which is semantically similar. however, the 'message' key, while quite similar, includes the additional phrase \"enjoy your meal 🍕🥗\" in the predicted arguments. this extension changes the semantic content by not only announcing the arrival of food but also extending a wish/enjoyment aspect with emojis suggesting the type of food. therefore, semantically, they are not equivalent due to the added context in the predicted argument.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "total_correct = 0\n",
    "for question in questions:\n",
    "    resp = llm(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        tools=tools,\n",
    "        messages=[\n",
    "            dict(role=\"system\", content=f\"The date today is {today}\"),\n",
    "            dict(role=\"user\", content=question[\"question\"]),\n",
    "        ],\n",
    "    )\n",
    "    tool_calls = extract_tool_calls(resp)\n",
    "    assert len(tool_calls) == len(question[\"tool_calls\"])\n",
    "    for tool_call, expected_call in zip(tool_calls, question[\"tool_calls\"]):\n",
    "        correct_call, explanation = check_tool_call_arguments(expected_call, tool_call)\n",
    "        if not correct_call:\n",
    "            print(f'QUESTION: {question[\"question\"]}')\n",
    "            print(f'EXPECTED Tool Call: {question[\"tool_calls\"][0]}')\n",
    "            print(f\"GENERATED Tool Call: {tool_call}\")\n",
    "            print(f\"EXPLANATION: {explanation}\\n\\n\")\n",
    "        else:\n",
    "            total_correct += 1\n",
    "        total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e75a23bdf4aa16",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Correctly called the proper functions {total_correct} times out of {total}. But check the \"failure\" cases above since they may be correct anyway.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f7132866ee84e0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## NousResearch/Hermes-2-Pro-Mistral-7B Function Calling\n",
    "\n",
    "Now we are going to do the same thing with `NousResearch/Hermes-2-Pro-Mistral-7B`.\n",
    "The format for the function calling is documented \n",
    "on the [model card](https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B#prompt-format-for-function-calling) as well as this [repo](https://github.com/NousResearch/Hermes-Function-Calling). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3759d1b1286d65",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def extract_tool_calls(tool_calls_str):\n",
    "    # Split the string by \"</tool_call>\\n\" to separate each tool call\n",
    "    tool_calls = tool_calls_str.split(\"</tool_call>\\n\")\n",
    "    parsed_results = []\n",
    "    for tool_call in tool_calls:\n",
    "        if tool_call:\n",
    "            dict_str = tool_call.split(\"\\n\")[1]  # Get the line with the dictionary\n",
    "            tool_call_dict = ast.literal_eval(dict_str)\n",
    "            # Extracting the arguments and function name and add to the results list\n",
    "            parsed_results.append({\"arguments\": tool_call_dict[\"arguments\"], \"name\": tool_call_dict[\"name\"]})\n",
    "    return parsed_results\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    f\"The date today is {today}\\n\"\n",
    "    + \"\"\"\n",
    "You are a function calling AI model. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools:\n",
    "<tools> \n",
    "\"\"\"\n",
    "    + str(tools)\n",
    "    + \"\"\"\n",
    "    \n",
    "</tools> Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']} For each function call return a json object with function name and arguments within <tool_call></tool_call> XML tags as follows:\n",
    "<tool_call>\n",
    "{'arguments': <args-dict>, 'name': <function-name>}\n",
    "</tool_call>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "total = 0\n",
    "total_correct = 0\n",
    "for question in questions:\n",
    "    resp = llm(\n",
    "        model=\"tgi\",\n",
    "        base_url=HUGGING_FACE_ENDPOINT_URL,\n",
    "        api_key=HUGGING_FACE_ACCESS_TOKEN,\n",
    "        messages=[\n",
    "            dict(role=\"system\", content=system_prompt),\n",
    "            dict(role=\"user\", content=question[\"question\"]),\n",
    "        ],\n",
    "    )\n",
    "    tool_calls = extract_tool_calls(resp.choices[0].message.content)\n",
    "    assert len(tool_calls) == len(question[\"tool_calls\"])\n",
    "    for tool_call, expected_call in zip(tool_calls, question[\"tool_calls\"]):\n",
    "        correct_call, explanation = check_tool_call_arguments(expected_call, tool_call)\n",
    "        if not correct_call:\n",
    "            print(f'QUESTION: {question[\"question\"]}')\n",
    "            print(f'EXPECTED Tool Call: {question[\"tool_calls\"][0]}')\n",
    "            print(f\"GENERATED Tool Call: {tool_call}\")\n",
    "            print(f\"EXPLANATION: {explanation}\\n\\n\")\n",
    "        else:\n",
    "            total_correct += 1\n",
    "        total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e702a26b80c04",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f'Correctly called the proper functions {total_correct} times out of {total}. But check the \"failure\" cases above since they may be correct anyway.'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
