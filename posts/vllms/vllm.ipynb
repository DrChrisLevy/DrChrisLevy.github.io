{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7908ac58af685d94",
   "metadata": {
    "collapsed": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Multi Modal LLMs WIPS\n",
    "author: Chris Levy\n",
    "draft: false\n",
    "date: '2024-11-07'\n",
    "date-modified: '2024-11-07'\n",
    "image: imgs/intro.png\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: show\n",
    "    page-layout: full\n",
    "include-in-header:\n",
    "  - text: |\n",
    "      <style>\n",
    "      .cell-output-stdout code {\n",
    "        word-break: break-wor !important;\n",
    "        white-space: pre-wrap !important;\n",
    "      }\n",
    "      </style>\n",
    "bibliography: ../../bibliography.bib\n",
    "resources:\n",
    "    - imgs/modal.mp4\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fe2128",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TODO\n",
    "\n",
    "# Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d200ad17",
   "metadata": {},
   "source": [
    "We first need to have an understanding of the transformer architecture used in LLMs.\n",
    "Earlier this year I wrote my first blog post with some notes on the [transformer architecture](https://drchrislevy.github.io/posts/basic_transformer_notes/transformers.html). To get the most out of this post, it would be good to have some familiarity with the transformer architecture. We will give a quick reminder of some basic concepts.\n",
    "\n",
    "\n",
    "We will load one of the [SmolLM2 LLM models](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9) created by\n",
    " the Hugging Face team. This is not the instruction fine tuned model, but rather the base pre-trained model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f947ca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26127465",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e477319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopher/personal_projects/DrChrisLevy.github.io/posts/vllms/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | warning: false\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad35b398",
   "metadata": {},
   "source": [
    "The input to the transformer model is a **sequence of embeddings**. In the case of text inputs, the input first gets converted into a sequence of tokens. Then each token is converted into an embedding vector.\n",
    "\n",
    "Here is the conversion of the input text to tokens ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340b581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  504,  2767, 25437,   690,   260]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 5])\n",
      "tensor([[  504,  2767, 25437,   690,   260]])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer([\"The dog jumped over the\"], return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids\n",
    "print(inputs)\n",
    "print(input_ids.shape)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d16517",
   "metadata": {},
   "source": [
    "Each token id has an associated embedding vector. \n",
    "In the case of this SmolLM2 model, the embedding dimension is 576\n",
    "and there are 49152 tokens in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3772f7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49152, 576])\n"
     ]
    }
   ],
   "source": [
    "embedding_lkp = model.model.embed_tokens\n",
    "print(embedding_lkp.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c8d8c",
   "metadata": {},
   "source": [
    "We can get the token embeddings by passing the token ids to the embedding lookup table.\n",
    "Each row of the returned tensor, ignoring the batch dimension, is a vector representation of a token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ec0fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 576])\n",
      "tensor([[[ 0.1177,  0.0199, -0.0942,  ...,  0.0405,  0.1182,  0.0762],\n",
      "         [-0.0356,  0.1338,  0.0050,  ...,  0.0996,  0.0791,  0.0791],\n",
      "         [-0.0093,  0.0122,  0.0197,  ...,  0.0613, -0.1021, -0.0923],\n",
      "         [-0.0339,  0.0825, -0.1562,  ...,  0.0349,  0.1172, -0.0752],\n",
      "         [-0.1514,  0.0181, -0.0742,  ...,  0.0430,  0.0986,  0.0664]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding_vectors = embedding_lkp(input_ids)\n",
    "print(embedding_vectors.shape)\n",
    "print(embedding_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d3734b",
   "metadata": {},
   "source": [
    "It is this sequence of embedding vectors that flows through the transformer layers.\n",
    "The input shape to the transformer layers is `(batch_size, sequence_length, embedding_dim)`\n",
    "and the output shape is `(batch_size, sequence_length, hidden_size)`.\n",
    "You can get the last hidden state by passing the inputs to the model, excluding\n",
    "the final classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c7df25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 576])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3476,  0.7350,  0.1515,  ..., -0.0168,  0.8690,  1.1515],\n",
       "         [ 0.0334,  0.6300,  0.7636,  ..., -0.6490,  0.0102, -0.2357],\n",
       "         [-1.0193,  0.9439,  0.1579,  ..., -0.3536, -2.4959,  1.6141],\n",
       "         [-2.0151, -0.3402, -0.6598,  ...,  1.7252, -1.6691,  1.4883],\n",
       "         [-0.6080, -0.9785, -0.8922,  ...,  3.4061, -0.1228, -0.6294]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state = model.model(**inputs).last_hidden_state\n",
    "print(last_hidden_state.shape)\n",
    "last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ba968",
   "metadata": {},
   "source": [
    "Then this final transformer output is passed to the classification head.\n",
    "The classification head is a single linear layer that maps the hidden state to the logits for the next token.\n",
    "The output shape of the classification head is `(batch_size, sequence_length, vocab_size)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ebd4a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 49152])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model.lm_head(last_hidden_state)\n",
    "assert torch.allclose(logits, model(**inputs).logits)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc0ef01",
   "metadata": {},
   "source": [
    "Next we convert the logits to probabilities using softmax.\n",
    "While this is useful for visualization and inference, during training we typically\n",
    "use the raw logits directly with CrossEntropyLoss for better numerical stability.\n",
    "Note that we get logits (and after softmax, probabilities) for the next token at **each position** in the sequence.\n",
    "During inference, we typically only care about the last position's values since that's where we'll generate the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7acd1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 49152])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = F.softmax(logits, dim=-1)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83762bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: The dog jumped over the\n",
      "\n",
      "After token: 'The'\n",
      "Top 5 predicted next tokens:\n",
      "   first: 0.022\n",
      "   same: 0.015\n",
      "   most: 0.012\n",
      "   world: 0.011\n",
      "   last: 0.006\n",
      "\n",
      "After token: ' dog'\n",
      "Top 5 predicted next tokens:\n",
      "   was: 0.063\n",
      "   is: 0.062\n",
      "  's: 0.047\n",
      "  â€™: 0.039\n",
      "  ,: 0.031\n",
      "\n",
      "After token: ' jumped'\n",
      "Top 5 predicted next tokens:\n",
      "   up: 0.200\n",
      "   on: 0.135\n",
      "   into: 0.068\n",
      "   over: 0.063\n",
      "   out: 0.062\n",
      "\n",
      "After token: ' over'\n",
      "Top 5 predicted next tokens:\n",
      "   the: 0.793\n",
      "   a: 0.032\n",
      "   it: 0.030\n",
      "   and: 0.017\n",
      "   him: 0.013\n",
      "\n",
      "After token: ' the'\n",
      "Top 5 predicted next tokens:\n",
      "   fence: 0.408\n",
      "   wall: 0.029\n",
      "   top: 0.017\n",
      "   bridge: 0.017\n",
      "   table: 0.013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "K = 5  # Number of top predictions to show\n",
    "top_probs, top_indices = torch.topk(probs[0], k=K, dim=-1)  # Remove batch dim and get top K\n",
    "\n",
    "# Convert token indices to actual tokens and print predictions for each position\n",
    "input_text = tokenizer.decode(input_ids[0])  # Original text\n",
    "print(f\"Original text: {input_text}\\n\")\n",
    "\n",
    "for pos in range(len(input_ids[0])):\n",
    "    token = tokenizer.decode(input_ids[0][pos])\n",
    "    print(f\"After token: '{token}'\")\n",
    "    print(f\"Top {K} predicted next tokens:\")\n",
    "    for prob, idx in zip(top_probs[pos], top_indices[pos]):\n",
    "        predicted_token = tokenizer.decode(idx)\n",
    "        print(f\"  {predicted_token}: {prob:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef37feda",
   "metadata": {},
   "source": [
    "In summary, the input to the transformer layers is a sequence of embeddings, of shape `(batch_size, sequence_length, embedding_dim)`.\n",
    "The transformer layers process this sequence and return a new sequence of hidden states, of shape `(batch_size, sequence_length, hidden_size)`.\n",
    "It is often the case that the hidden size is the same as the embedding dimension, but this is not a requirement. Even if you forget the details of the inner workings of the transformer layers (self attention, etc.), this is a useful mental model to keep in mind. The final classifier layer returns a probability distribution over the next token for each position in the sequence, of shape `(batch_size, sequence_length, vocab_size)`.\n",
    "\n",
    "Now we'll move on to the next topic, which has to with how we can pass images into LLMs.\n",
    "If you remember that the input to the transformer layers is a sequence of embeddings, then passing in images is no different.\n",
    "We just need to convert the images into a sequence of embeddings.\n",
    "\n",
    "**TODO: A SIMPLE DIAGRAM HERE OF (B,T,C) ---> Transformer(B,T,C) ---> (B,T,C)**\n",
    "\n",
    "# VITs (Vision Transformers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6196e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be4d9ede",
   "metadata": {},
   "source": [
    "# Random Notes\n",
    "\n",
    "ZeroShot Models\n",
    "\n",
    "- CLIP - text and image encoder - probs add to 1 - went through softmax\n",
    "- SigLIP (newer/better?) - sigmoid was used instead of softmax\n",
    "- OWLViT/V2 -  CLIP architecture but with object detection\n",
    "- Segmentation Mask - segment anything model\n",
    "- OWLSAM\n",
    "\n",
    "Vision Language Models\n",
    "- LLaVA - one of the first - see https://huggingface.co/blog/vlms for more details. Images there about pre-training and post-training.\n",
    "- PaliGemma - Siglip and Gemma model\n",
    "- Qwen2-VL - and 2.5? - 675M ViT image encoder, MLP projector, Qwen2 LLM decoder\n",
    "- Pixtral 400M vision encoder, 12B Mistral Nemo Text decoder\n",
    "- Molmo - CLIP encoder with different decoders\n",
    "- [OpenVLM Leaderboard](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard)\n",
    "\n",
    "Newer Advancements \n",
    "[See here YT time stamp](https://youtu.be/_TlhKHTgWjY?t=2186)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16684ba2",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "## Multimodal LLMs\n",
    "\n",
    "[Understanding Multimodal LLMs](https://magazine.sebastianraschka.com/p/understanding-multimodal-llms?utm_source=post-email-title&publication_id=1174659&post_id=151078631&utm_campaign=email-post-title&isFreemail=true&r=1urfra&triedRedirect=true&utm_medium=email)\n",
    "\n",
    "[AI Visions Live | Merve Noyan | Open-source Multimodality](https://www.youtube.com/watch?v=_TlhKHTgWjY)\n",
    "\n",
    "[Vision Language Models Explained](https://huggingface.co/blog/vlms)\n",
    "\n",
    "\n",
    "[PaliGemma â€“ Google's Cutting-Edge Open Vision Language Model](https://huggingface.co/blog/paligemma)\n",
    "\n",
    "[Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models?tab=readme-ov-file#awesome-papers)\n",
    "\n",
    "[Vision Arena](https://huggingface.co/spaces/WildVision/vision-arena)\n",
    "\n",
    "[smol-vision](https://github.com/merveenoyan/smol-vision)\n",
    "\n",
    "\n",
    "## Vision Transformer (ViT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/pdf/2010.11929)\n",
    "\n",
    "[Vision Transformer (ViT: Hugging Face)](https://huggingface.co/docs/transformers/en/model_doc/vit)\n",
    "\n",
    "[Fine-Tune ViT for Image Classification with ðŸ¤— Transformers](https://huggingface.co/blog/fine-tune-vit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108bbb31",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
