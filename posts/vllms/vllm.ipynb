{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7908ac58af685d94",
   "metadata": {
    "collapsed": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Passing Images into LLMs\n",
    "author: Chris Levy\n",
    "draft: false\n",
    "date: '2024-11-07'\n",
    "date-modified: '2024-11-07'\n",
    "image: imgs/intro.png\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: show\n",
    "    page-layout: full\n",
    "include-in-header:\n",
    "  - text: |\n",
    "      <style>\n",
    "      .cell-output-stdout code {\n",
    "        word-break: break-wor !important;\n",
    "        white-space: pre-wrap !important;\n",
    "      }\n",
    "      </style>\n",
    "bibliography: ../../bibliography.bib\n",
    "resources:\n",
    "    - imgs/modal.mp4\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fe2128",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "I work with LLMs every day. Mostly with text inputs, but also with image inputs too. I continue to spend time learning the internals of the transformer architecture used in decoder style LLMs. But most of my studying has been with text inputs. I was always a little curious, and confused, on how images were passed into LLMs. I just never took the time to dig into it more. Now I am finally getting around to it. This blog post is some notes I'm taking as I learn more about this topic. I write about things so I can better understand them, and my future self is always grateful.\n",
    "\n",
    "The main motivation for this post was Sebastian Raschka's recent blog post [Understanding Multimodal LLMs](https://magazine.sebastianraschka.com/p/understanding-multimodal-llms?utm_source=post-email-title&publication_id=1174659&post_id=151078631&utm_campaign=email-post-title&isFreemail=true&r=1urfra&triedRedirect=true&utm_medium=email). I highly recommend reading it. I'm not going to regurgitate his content here, but I am going to focus on learning a subset of the topics he discusses. I am starting with focusing on what he refers to as *Method A: Unified Embedding Decoder Architecture*. I may go deeper on other topics in other blog posts, but for now this is a good start for me.\n",
    "\n",
    "# Recap of Transformer Architecture for Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d200ad17",
   "metadata": {},
   "source": [
    "We first need to have an understanding of the transformer architecture used in decoder style LLMs.\n",
    "Earlier this year I wrote my first blog post with some notes on the [transformer architecture](https://drchrislevy.github.io/posts/basic_transformer_notes/transformers.html). To get the most out of this post, it would be good to have some familiarity with the transformer architecture. We will give a quick reminder of some basic concepts.\n",
    "\n",
    "\n",
    "We will load one of the [SmolLM2 LLM models](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9) created by\n",
    " the Hugging Face team. This is not the instruction fine tuned model, but rather the base pre-trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e477319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopher/personal_projects/DrChrisLevy.github.io/posts/vllms/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | warning: false\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad35b398",
   "metadata": {},
   "source": [
    "The input to the transformer model is a **sequence of embeddings**. In the case of text inputs, the input first gets converted into a sequence of tokens. Then each token is converted into an embedding vector.\n",
    "\n",
    "Here is the conversion of the input text to tokens ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340b581a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  504,  2767, 25437,   690,   260]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 5])\n",
      "tensor([[  504,  2767, 25437,   690,   260]])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer([\"The dog jumped over the\"], return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids\n",
    "print(inputs)\n",
    "print(input_ids.shape)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d16517",
   "metadata": {},
   "source": [
    "Each token id has an associated embedding vector. \n",
    "In the case of this SmolLM2 model, the embedding dimension is 576\n",
    "and there are 49152 tokens in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3772f7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49152, 576])\n"
     ]
    }
   ],
   "source": [
    "embedding_lkp = model.model.embed_tokens\n",
    "print(embedding_lkp.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1c8d8c",
   "metadata": {},
   "source": [
    "We can get the token embeddings by passing the token ids to the embedding lookup table.\n",
    "Each row of the returned tensor, ignoring the batch dimension, is a vector representation of a token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ec0fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 576])\n",
      "tensor([[[ 0.1177,  0.0199, -0.0942,  ...,  0.0405,  0.1182,  0.0762],\n",
      "         [-0.0356,  0.1338,  0.0050,  ...,  0.0996,  0.0791,  0.0791],\n",
      "         [-0.0093,  0.0122,  0.0197,  ...,  0.0613, -0.1021, -0.0923],\n",
      "         [-0.0339,  0.0825, -0.1562,  ...,  0.0349,  0.1172, -0.0752],\n",
      "         [-0.1514,  0.0181, -0.0742,  ...,  0.0430,  0.0986,  0.0664]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding_vectors = embedding_lkp(input_ids)\n",
    "print(embedding_vectors.shape)\n",
    "print(embedding_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d3734b",
   "metadata": {},
   "source": [
    "It is this sequence of embedding vectors that flows through the transformer layers.\n",
    "The input shape to the transformer layers is `(batch_size, sequence_length, embedding_dim)`\n",
    "and the output shape is `(batch_size, sequence_length, hidden_size)`.\n",
    "You can get the last hidden state by passing the inputs to the model, excluding\n",
    "the final classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c7df25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 576])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3476,  0.7350,  0.1515,  ..., -0.0168,  0.8690,  1.1515],\n",
       "         [ 0.0334,  0.6300,  0.7636,  ..., -0.6490,  0.0102, -0.2357],\n",
       "         [-1.0193,  0.9439,  0.1579,  ..., -0.3536, -2.4959,  1.6141],\n",
       "         [-2.0151, -0.3402, -0.6598,  ...,  1.7252, -1.6691,  1.4883],\n",
       "         [-0.6080, -0.9785, -0.8922,  ...,  3.4061, -0.1228, -0.6294]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state = model.model(**inputs).last_hidden_state\n",
    "print(last_hidden_state.shape)\n",
    "last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ba968",
   "metadata": {},
   "source": [
    "Then this final transformer output is passed to the classification head.\n",
    "The classification head is a single linear layer that maps the hidden state to the logits for the next token.\n",
    "The output shape of the classification head is `(batch_size, sequence_length, vocab_size)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ebd4a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 49152])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model.lm_head(last_hidden_state)\n",
    "assert torch.allclose(logits, model(**inputs).logits)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc0ef01",
   "metadata": {},
   "source": [
    "Next we convert the logits to probabilities using the softmax function.\n",
    "While this is useful for visualization and inference, during training we typically\n",
    "use the raw logits directly with CrossEntropyLoss for better numerical stability.\n",
    "Note that we get logits (and after softmax, probabilities) for the next token at **each position** in the sequence.\n",
    "During inference, we typically only care about the last position's values since that's where we'll generate the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7acd1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 49152])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = F.softmax(logits, dim=-1)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54108fa1",
   "metadata": {},
   "source": [
    "This shows that at inference time we actually get the probabilities for the next token at **each position** in the sequence.\n",
    "Here we'll just look at the top 5 predictions for each token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83762bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: The dog jumped over the\n",
      "\n",
      "After token: 'The'\n",
      "Top 5 predicted next tokens:\n",
      "   first: 0.022\n",
      "   same: 0.015\n",
      "   most: 0.012\n",
      "   world: 0.011\n",
      "   last: 0.006\n",
      "\n",
      "After token: ' dog'\n",
      "Top 5 predicted next tokens:\n",
      "   was: 0.063\n",
      "   is: 0.062\n",
      "  's: 0.047\n",
      "  ’: 0.039\n",
      "  ,: 0.031\n",
      "\n",
      "After token: ' jumped'\n",
      "Top 5 predicted next tokens:\n",
      "   up: 0.200\n",
      "   on: 0.135\n",
      "   into: 0.068\n",
      "   over: 0.063\n",
      "   out: 0.062\n",
      "\n",
      "After token: ' over'\n",
      "Top 5 predicted next tokens:\n",
      "   the: 0.793\n",
      "   a: 0.032\n",
      "   it: 0.030\n",
      "   and: 0.017\n",
      "   him: 0.013\n",
      "\n",
      "After token: ' the'\n",
      "Top 5 predicted next tokens:\n",
      "   fence: 0.408\n",
      "   wall: 0.029\n",
      "   top: 0.017\n",
      "   bridge: 0.017\n",
      "   table: 0.013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "K = 5  # Number of top predictions to show\n",
    "top_probs, top_indices = torch.topk(probs[0], k=K, dim=-1)  # Remove batch dim and get top K\n",
    "\n",
    "# Convert token indices to actual tokens and print predictions for each position\n",
    "input_text = tokenizer.decode(input_ids[0])  # Original text\n",
    "print(f\"Original text: {input_text}\\n\")\n",
    "\n",
    "for pos in range(len(input_ids[0])):\n",
    "    token = tokenizer.decode(input_ids[0][pos])\n",
    "    print(f\"After token: '{token}'\")\n",
    "    print(f\"Top {K} predicted next tokens:\")\n",
    "    for prob, idx in zip(top_probs[pos], top_indices[pos]):\n",
    "        predicted_token = tokenizer.decode(idx)\n",
    "        print(f\"  {predicted_token}: {prob:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef37feda",
   "metadata": {},
   "source": [
    "In summary, the input to the transformer layers is a sequence of embeddings, of shape `(batch_size, sequence_length, embedding_dim)`.\n",
    "The transformer layers process this sequence and return a new sequence of hidden states, of shape `(batch_size, sequence_length, hidden_size)`.\n",
    "It is often the case that the hidden size is the same as the embedding dimension, but this is not a requirement. Even if you forget the details of the inner workings of the transformer layers (self attention, etc.), this is a useful mental model to keep in mind. The final classifier layer returns a probability distribution over the next token for each position in the sequence, of shape `(batch_size, sequence_length, vocab_size)`.\n",
    "\n",
    "Another thing to mention is that the above explanation was focusing on auto-regressive decoders. Recall that most of the LLMs which generate text are auto-regressive decoders. There are also pure encoder models. Here are some of the key differences between decoder and encoder models:\n",
    "\n",
    "- Attention Masking:\n",
    "    - Decoder: Uses causal (or triangular) masked attention to ensure that each position can only attend to previous positions, enforcing an autoregressive quality. This prevents the model from \"seeing the future,\" which is essential for tasks like text generation.\n",
    "    - Encoder: Doesn't require causal masking because it processes the entire input sequence at once. Each token can attend to every other token in the sequence, providing a comprehensive context.\n",
    "\n",
    "- Purpose and Data Flow:\n",
    "    - Decoder: Designed for autoregressive tasks, where each output token is generated one by one, conditioning on previously generated tokens. This step-by-step generation is central to tasks like text generation, where each token \"builds\" upon the preceding tokens.\n",
    "    - Encoder: Designed to encode the entire input sequence into a contextualized representation in one shot, capturing relationships across the whole sequence. It’s typically used in understanding or embedding tasks and classification tasks, where the model needs a holistic view of the input.\n",
    "\n",
    "Now we'll move on to how we can pass images into decoder style LLMs, along side text, to generate text outputs.\n",
    "If you remember that the input to the transformer layers is a sequence of embeddings, then passing in images is no different.\n",
    "We just need to convert the images into a sequence of embeddings suitable for the LLM decoder model.\n",
    "\n",
    "**TODO: A SIMPLE DIAGRAM HERE OF (B,T,C) ---> Transformer(B,T,C) ---> (B,T,C)**\n",
    "\n",
    "# Introducing Images into LLMs\n",
    "\n",
    "Now that we've revisited how transformers handle text inputs, let's explore how images can be incorporated into LLMs. Remember, the key idea is that transformers process sequences of embeddings. For text, these are token embeddings. For images, we'll need to convert them into embeddings as well.\n",
    "\n",
    "## Image Embeddings with Vision Transformers (ViT)\n",
    "\n",
    "To convert an image into a sequence of embeddings, we can use a Vision Transformer (ViT) image encoder.\n",
    "At this point we are going to treat this image encoder as a black box. We will go into more details on how it works later on. \n",
    "The ViT image encoder converts an input image into a sequence of embeddings. We don't need to know the internal workings of the image encoder at this point; we just need to know that it produces embeddings. \n",
    "\n",
    "Why use an Image Encoder? Just like text needs to be tokenized and embedded before being processed by an LLM, images need to be transformed into a suitable format. The image encoder serves this purpose by translating visual information into a sequence of numerical embeddings.\n",
    "\n",
    "### SigLIP\n",
    "SigLIP is a state-of-the-art model that can understand both images and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b976d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.5% that image 0 is 'a photo of 2 cats'\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import torch\n",
    "\n",
    "model = AutoModel.from_pretrained(\"google/siglip-so400m-patch14-384\")\n",
    "processor = AutoProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "texts = [\"a photo of 2 cats\", \"a photo of 2 dogs\"]\n",
    "inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits_per_image = outputs.logits_per_image\n",
    "probs = torch.sigmoid(logits_per_image) # these are the probabilities\n",
    "print(f\"{probs[0][0]:.1%} that image 0 is '{texts[0]}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5f8cbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiglipModel(\n",
       "  (text_model): SiglipTextTransformer(\n",
       "    (embeddings): SiglipTextEmbeddings(\n",
       "      (token_embedding): Embedding(32000, 1152)\n",
       "      (position_embedding): Embedding(64, 1152)\n",
       "    )\n",
       "    (encoder): SiglipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-26): 27 x SiglipEncoderLayer(\n",
       "          (self_attn): SiglipSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SiglipMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "  )\n",
       "  (vision_model): SiglipVisionTransformer(\n",
       "    (embeddings): SiglipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "      (position_embedding): Embedding(729, 1152)\n",
       "    )\n",
       "    (encoder): SiglipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-26): 27 x SiglipEncoderLayer(\n",
       "          (self_attn): SiglipSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SiglipMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "            (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): SiglipMultiheadAttentionPoolingHead(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1152, out_features=1152, bias=True)\n",
       "      )\n",
       "      (layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): SiglipMLP(\n",
       "        (activation_fn): PytorchGELUTanh()\n",
       "        (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "        (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6196e0e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be4d9ede",
   "metadata": {},
   "source": [
    "# Random Notes\n",
    "\n",
    "- Rough Structure of Blog: https://chatgpt.com/c/673221d7-5b34-8011-b2a3-a7adbc56ab29\n",
    "\n",
    "ZeroShot Models\n",
    "\n",
    "- CLIP - text and image encoder - probs add to 1 - went through softmax\n",
    "- SigLIP (newer/better?) - sigmoid was used instead of softmax\n",
    "- OWLViT/V2 -  CLIP architecture but with object detection\n",
    "- Segmentation Mask - segment anything model\n",
    "- OWLSAM\n",
    "\n",
    "Vision Language Models\n",
    "- LLaVA - one of the first - see https://huggingface.co/blog/vlms for more details. Images there about pre-training and post-training.\n",
    "- PaliGemma - Siglip and Gemma model\n",
    "- Qwen2-VL - and 2.5? - 675M ViT image encoder, MLP projector, Qwen2 LLM decoder\n",
    "- Pixtral 400M vision encoder, 12B Mistral Nemo Text decoder\n",
    "- Molmo - CLIP encoder with different decoders\n",
    "- [OpenVLM Leaderboard](https://huggingface.co/spaces/opencompass/open_vlm_leaderboard)\n",
    "\n",
    "Newer Advancements \n",
    "[See here YT time stamp](https://youtu.be/_TlhKHTgWjY?t=2186)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16684ba2",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "## Multimodal LLMs\n",
    "\n",
    "[Understanding Multimodal LLMs](https://magazine.sebastianraschka.com/p/understanding-multimodal-llms?utm_source=post-email-title&publication_id=1174659&post_id=151078631&utm_campaign=email-post-title&isFreemail=true&r=1urfra&triedRedirect=true&utm_medium=email)\n",
    "\n",
    "[AI Visions Live | Merve Noyan | Open-source Multimodality](https://www.youtube.com/watch?v=_TlhKHTgWjY)\n",
    "\n",
    "[Vision Language Models Explained](https://huggingface.co/blog/vlms)\n",
    "\n",
    "\n",
    "[PaliGemma – Google's Cutting-Edge Open Vision Language Model](https://huggingface.co/blog/paligemma)\n",
    "[PaliGemma: A versatile 3B VLM for transfer: Paper](https://arxiv.org/pdf/2407.07726)\n",
    "[Gemma explained: PaliGemma architecture: Google for Developers](https://developers.googleblog.com/en/gemma-explained-paligemma-architecture/)\n",
    "[Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models?tab=readme-ov-file#awesome-papers)\n",
    "\n",
    "[Vision Arena](https://huggingface.co/spaces/WildVision/vision-arena)\n",
    "\n",
    "[smol-vision](https://github.com/merveenoyan/smol-vision)\n",
    "\n",
    "\n",
    "## Vision Transformer (ViT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "[AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/pdf/2010.11929)\n",
    "\n",
    "[Vision Transformer (ViT: Hugging Face)](https://huggingface.co/docs/transformers/en/model_doc/vit)\n",
    "\n",
    "[Fine-Tune ViT for Image Classification with 🤗 Transformers](https://huggingface.co/blog/fine-tune-vit)\n",
    "\n",
    "[SigLIP Model Card](https://huggingface.co/google/siglip-so400m-patch14-384)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108bbb31",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
