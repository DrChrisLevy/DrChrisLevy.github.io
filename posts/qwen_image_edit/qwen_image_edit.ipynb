{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f728359e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Running Qwen Image Edit on Modal\n",
    "author: Chris Levy\n",
    "draft: false\n",
    "date: '2025-08-19'\n",
    "date-modified: '2025-08-19'\n",
    "image: qwen_edited_20250819_195701_158fca45.jpg\n",
    "toc: true\n",
    "description: Creating an endpoint for Qwen Image Edit inference on Modal.\n",
    "tags:\n",
    "  - Modal\n",
    "  - Qwen Image Edit\n",
    "  - Diffusers\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87da846",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/02NuraPr21g\" title=\"YouTube video\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
    "\n",
    "\n",
    "## Qwen-Image-Edit\n",
    "\n",
    "- The editing version of Qwen Image just came out on August 19, 2025, `Qwen-Image-Edit`.\n",
    "- [blog post](https://qwenlm.github.io/blog/qwen-image-edit/)\n",
    "- [X announcement](https://x.com/Alibaba_Qwen/status/1957500569029079083)\n",
    "- [Hugging Face Model Card](https://huggingface.co/Qwen/Qwen-Image-Edit)\n",
    "- [Github Repo](https://github.com/QwenLM/Qwen-Image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8372aaf0",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "- create an account at [Modal](https://modal.com/docs/guide#getting-started) and install it in your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f14e23d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# The Code\n",
    "\n",
    "## Qwen Image Edit inference\n",
    "\n",
    "- put the code in a file, `qwen_image_edit.py` and deploy the endpoint with `modal deploy qwen_image_edit.py`\n",
    "\n",
    "```python\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "\n",
    "import modal\n",
    "\n",
    "# Modal Volume URL configuration\n",
    "MODAL_WORKSPACE = \"drchrislevy\"  # replace with your modal workspace\n",
    "MODAL_ENVIRONMENT = \"main\"  # replace with your modal environment\n",
    "VOLUME_NAME = \"qwen_edited_images\"\n",
    "\n",
    "# Image with required dependencies\n",
    "image = (\n",
    "    modal.Image.debian_slim()\n",
    "    .apt_install([\"git\"])\n",
    "    .pip_install(\n",
    "        [\n",
    "            \"torch\",\n",
    "            \"torchvision\",\n",
    "            \"git+https://github.com/huggingface/diffusers\",\n",
    "            \"transformers\",\n",
    "            \"accelerate\",\n",
    "            \"pillow\",\n",
    "            \"sentencepiece\",\n",
    "            \"python-dotenv\",\n",
    "            \"requests\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "app = modal.App(\"qwen-image-editor\", image=image)\n",
    "\n",
    "hf_hub_cache = modal.Volume.from_name(\"hf_hub_cache\", create_if_missing=True)\n",
    "images_volume = modal.Volume.from_name(VOLUME_NAME, create_if_missing=True)\n",
    "\n",
    "\n",
    "@app.cls(\n",
    "    image=image,\n",
    "    gpu=\"H100\",\n",
    "    secrets=[\n",
    "        modal.Secret.from_name(\"huggingface-secret\"),\n",
    "    ],\n",
    "    timeout=60 * 5,\n",
    "    volumes={\n",
    "        \"/root/.cache/huggingface/hub/\": hf_hub_cache,\n",
    "        \"/root/edited_images\": images_volume,\n",
    "    },\n",
    "    scaledown_window=10 * 60,\n",
    "    max_containers=2,\n",
    "    # enable_memory_snapshot=True, # in alpha # https://modal.com/blog/gpu-mem-snapshots\n",
    "    # experimental_options={\"enable_gpu_snapshot\": True}\n",
    ")\n",
    "@modal.concurrent(max_inputs=1)\n",
    "class QwenImageEditor:\n",
    "    @modal.enter()  # snap=True to try gpu memory snapshot\n",
    "    def setup(self):\n",
    "        \"\"\"Load Qwen-Image-Edit model once per container\"\"\"\n",
    "        import torch\n",
    "        from diffusers import QwenImageEditPipeline\n",
    "\n",
    "        print(\"Loading Qwen/Qwen-Image-Edit model...\")\n",
    "        self.pipe = QwenImageEditPipeline.from_pretrained(\"Qwen/Qwen-Image-Edit\")\n",
    "        self.pipe.to(torch.bfloat16)\n",
    "        self.pipe.to(\"cuda\")\n",
    "        self.pipe.set_progress_bar_config(disable=None)\n",
    "        print(\"Model loaded successfully!\")\n",
    "        self.images_path = \"/root/edited_images\"\n",
    "\n",
    "    def _download_image_from_url(self, image_url: str):\n",
    "        \"\"\"Download image from URL and convert to PIL Image\"\"\"\n",
    "        import requests\n",
    "        from PIL import Image\n",
    "\n",
    "        response = requests.get(image_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        image = Image.open(io.BytesIO(response.content)).convert(\"RGB\")\n",
    "        return image\n",
    "\n",
    "    def edit_image(\n",
    "        self,\n",
    "        image_url: str,\n",
    "        prompt: str,\n",
    "        negative_prompt: str = \" \",\n",
    "        true_cfg_scale: float = 4.0,\n",
    "        seed: int = 0,\n",
    "        randomize_seed: bool = False,\n",
    "        num_inference_steps: int = 50,\n",
    "    ):\n",
    "        import random\n",
    "        import uuid\n",
    "        from datetime import datetime\n",
    "\n",
    "        import numpy as np\n",
    "        import torch\n",
    "\n",
    "        input_image = self._download_image_from_url(image_url)\n",
    "\n",
    "        MAX_SEED = np.iinfo(np.int32).max\n",
    "        if randomize_seed:\n",
    "            seed = random.randint(0, MAX_SEED)\n",
    "\n",
    "        print(f\"Editing image {image_url} with prompt: {prompt}\")\n",
    "\n",
    "        # Edit image using Qwen-Image-Edit exactly like the original\n",
    "        inputs = {\n",
    "            \"image\": input_image,\n",
    "            \"prompt\": prompt,\n",
    "            \"generator\": torch.manual_seed(seed),\n",
    "            \"true_cfg_scale\": true_cfg_scale,\n",
    "            \"negative_prompt\": negative_prompt,\n",
    "            \"num_inference_steps\": num_inference_steps,\n",
    "        }\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            output = self.pipe(**inputs)\n",
    "            edited_image = output.images[0]\n",
    "\n",
    "        # Create unique filename\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        unique_id = str(uuid.uuid4())[:8]\n",
    "        filename = f\"qwen_edited_{timestamp}_{unique_id}.png\"\n",
    "        file_path = os.path.join(self.images_path, filename)\n",
    "\n",
    "        edited_image.save(file_path, format=\"PNG\")\n",
    "        print(f\"Edited image saved successfully to volume: {file_path}\")\n",
    "\n",
    "        # Generate Modal Volume URL\n",
    "        # Format: https://modal.com/api/volumes/{workspace}/{env}/{volume_name}/files/content?path={filename}\n",
    "        image_url_output = f\"https://modal.com/api/volumes/{MODAL_WORKSPACE}/{MODAL_ENVIRONMENT}/{VOLUME_NAME}/files/content?path={filename}\"\n",
    "\n",
    "        return {\n",
    "            \"original_image_url\": image_url,\n",
    "            \"image_url\": image_url_output,\n",
    "            \"prompt\": prompt,\n",
    "            \"negative_prompt\": negative_prompt,\n",
    "            \"true_cfg_scale\": true_cfg_scale,\n",
    "            \"seed\": seed,\n",
    "            \"num_inference_steps\": num_inference_steps,\n",
    "        }\n",
    "\n",
    "    @modal.fastapi_endpoint(\n",
    "        method=\"POST\",\n",
    "        docs=True,\n",
    "    )\n",
    "    def edit_image_endpoint(\n",
    "        self,\n",
    "        image_url: str,\n",
    "        prompt: str,\n",
    "        negative_prompt: str = \" \",\n",
    "        true_cfg_scale: float = 4.0,\n",
    "        seed: int = 0,\n",
    "        randomize_seed: bool = False,\n",
    "        num_inference_steps: int = 50,\n",
    "    ):\n",
    "        \"\"\"Public FastAPI endpoint for image editing\"\"\"\n",
    "        return self.edit_image(\n",
    "            image_url=image_url,\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            true_cfg_scale=true_cfg_scale,\n",
    "            seed=seed,\n",
    "            randomize_seed=randomize_seed,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "        )\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbcf0ef",
   "metadata": {},
   "source": [
    "## Qwen Image Edit Inference (FAST!)\n",
    "\n",
    "- code is nearly identical to above except it uses [lightx2v/Qwen-Image-Lightning](https://huggingface.co/lightx2v/Qwen-Image-Lightning)\n",
    "- The inference code snippet I got from [here](https://huggingface.co/spaces/multimodalart/Qwen-Image-Edit-Fast/blob/main/app.py)\n",
    "- uses less inference steps, 8 instead of 50\n",
    "- put the code in a file, `qwen_image_edit_fast.py` and deploy the endpoint with `modal deploy qwen_image_edit_fast.py`\n",
    "\n",
    "\n",
    "```python\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "import modal\n",
    "\n",
    "# Modal Volume URL configuration\n",
    "MODAL_WORKSPACE = \"drchrislevy\"  # replace with your modal workspace\n",
    "MODAL_ENVIRONMENT = \"main\"  # replace with your modal environment\n",
    "VOLUME_NAME = \"qwen_edited_images\"\n",
    "\n",
    "# Image with required dependencies\n",
    "image = (\n",
    "    modal.Image.debian_slim()\n",
    "    .apt_install([\"git\"])\n",
    "    .pip_install(\n",
    "        [\n",
    "            \"torch\",\n",
    "            \"torchvision\",\n",
    "            \"git+https://github.com/huggingface/diffusers\",\n",
    "            \"transformers\",\n",
    "            \"accelerate\",\n",
    "            \"pillow\",\n",
    "            \"sentencepiece\",\n",
    "            \"python-dotenv\",\n",
    "            \"requests\",\n",
    "            \"peft\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "app = modal.App(\"qwen-image-editor-fast\", image=image)\n",
    "\n",
    "hf_hub_cache = modal.Volume.from_name(\"hf_hub_cache\", create_if_missing=True)\n",
    "images_volume = modal.Volume.from_name(VOLUME_NAME, create_if_missing=True)\n",
    "\n",
    "\n",
    "@app.cls(\n",
    "    image=image,\n",
    "    gpu=\"H100\",\n",
    "    secrets=[\n",
    "        modal.Secret.from_name(\"huggingface-secret\"),\n",
    "    ],\n",
    "    timeout=60 * 5,\n",
    "    volumes={\n",
    "        \"/root/.cache/huggingface/hub/\": hf_hub_cache,\n",
    "        \"/root/edited_images\": images_volume,\n",
    "    },\n",
    "    scaledown_window=10 * 60,\n",
    "    max_containers=2,\n",
    "    # enable_memory_snapshot=True, # in alpha # https://modal.com/blog/gpu-mem-snapshots\n",
    "    # experimental_options={\"enable_gpu_snapshot\": True}\n",
    ")\n",
    "@modal.concurrent(max_inputs=1)\n",
    "class QwenImageEditor:\n",
    "    @modal.enter()  # snap=True to try gpu memory snapshot\n",
    "    def setup(self):\n",
    "        \"\"\"Load Qwen-Image-Edit model with Lightning acceleration once per container\"\"\"\n",
    "        import torch\n",
    "        from diffusers import FlowMatchEulerDiscreteScheduler, QwenImageEditPipeline\n",
    "\n",
    "        print(\"Loading Qwen/Qwen-Image-Edit model with Lightning acceleration...\")\n",
    "        dtype = torch.bfloat16\n",
    "        device = \"cuda\"\n",
    "        scheduler_config = {\n",
    "            \"base_image_seq_len\": 256,\n",
    "            \"base_shift\": math.log(3),\n",
    "            \"invert_sigmas\": False,\n",
    "            \"max_image_seq_len\": 8192,\n",
    "            \"max_shift\": math.log(3),\n",
    "            \"num_train_timesteps\": 1000,\n",
    "            \"shift\": 1.0,\n",
    "            \"shift_terminal\": None,\n",
    "            \"stochastic_sampling\": False,\n",
    "            \"time_shift_type\": \"exponential\",\n",
    "            \"use_beta_sigmas\": False,\n",
    "            \"use_dynamic_shifting\": True,\n",
    "            \"use_exponential_sigmas\": False,\n",
    "            \"use_karras_sigmas\": False,\n",
    "        }\n",
    "        scheduler = FlowMatchEulerDiscreteScheduler.from_config(scheduler_config)\n",
    "        self.pipe = QwenImageEditPipeline.from_pretrained(\n",
    "            \"Qwen/Qwen-Image-Edit\", scheduler=scheduler, torch_dtype=dtype\n",
    "        ).to(device)\n",
    "        self.pipe.load_lora_weights(\n",
    "            \"lightx2v/Qwen-Image-Lightning\",\n",
    "            weight_name=\"Qwen-Image-Lightning-8steps-V1.1.safetensors\",\n",
    "        )\n",
    "        self.pipe.fuse_lora()\n",
    "        self.pipe.set_progress_bar_config(disable=None)\n",
    "        print(\"Model loaded successfully with Lightning acceleration!\")\n",
    "        self.images_path = \"/root/edited_images\"\n",
    "\n",
    "    def _download_image_from_url(self, image_url: str):\n",
    "        \"\"\"Download image from URL and convert to PIL Image\"\"\"\n",
    "        import requests\n",
    "        from PIL import Image\n",
    "\n",
    "        response = requests.get(image_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        image = Image.open(io.BytesIO(response.content)).convert(\"RGB\")\n",
    "        return image\n",
    "\n",
    "    def edit_image(\n",
    "        self,\n",
    "        image_url: str,\n",
    "        prompt: str,\n",
    "        negative_prompt: str = \" \",\n",
    "        true_cfg_scale: float = 4.0,\n",
    "        seed: int = 0,\n",
    "        randomize_seed: bool = False,\n",
    "        num_inference_steps: int = 8,\n",
    "    ):\n",
    "        import random\n",
    "        import uuid\n",
    "        from datetime import datetime\n",
    "\n",
    "        import numpy as np\n",
    "        import torch\n",
    "\n",
    "        input_image = self._download_image_from_url(image_url)\n",
    "\n",
    "        MAX_SEED = np.iinfo(np.int32).max\n",
    "        if randomize_seed:\n",
    "            seed = random.randint(0, MAX_SEED)\n",
    "\n",
    "        print(f\"Editing image {image_url} with prompt: {prompt}\")\n",
    "\n",
    "        # Edit image using Qwen-Image-Edit exactly like the original\n",
    "        inputs = {\n",
    "            \"image\": input_image,\n",
    "            \"prompt\": prompt,\n",
    "            \"generator\": torch.manual_seed(seed),\n",
    "            \"true_cfg_scale\": true_cfg_scale,\n",
    "            \"negative_prompt\": negative_prompt,\n",
    "            \"num_inference_steps\": num_inference_steps,\n",
    "        }\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            output = self.pipe(**inputs)\n",
    "            edited_image = output.images[0]\n",
    "\n",
    "        # Create unique filename\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        unique_id = str(uuid.uuid4())[:8]\n",
    "        filename = f\"qwen_edited_{timestamp}_{unique_id}.png\"\n",
    "        file_path = os.path.join(self.images_path, filename)\n",
    "\n",
    "        edited_image.save(file_path, format=\"PNG\")\n",
    "        print(f\"Edited image saved successfully to volume: {file_path}\")\n",
    "\n",
    "        # Generate Modal Volume URL\n",
    "        # Format: https://modal.com/api/volumes/{workspace}/{env}/{volume_name}/files/content?path={filename}\n",
    "        image_url_output = f\"https://modal.com/api/volumes/{MODAL_WORKSPACE}/{MODAL_ENVIRONMENT}/{VOLUME_NAME}/files/content?path={filename}\"\n",
    "\n",
    "        return {\n",
    "            \"original_image_url\": image_url,\n",
    "            \"image_url\": image_url_output,\n",
    "            \"prompt\": prompt,\n",
    "            \"negative_prompt\": negative_prompt,\n",
    "            \"true_cfg_scale\": true_cfg_scale,\n",
    "            \"seed\": seed,\n",
    "            \"num_inference_steps\": num_inference_steps,\n",
    "        }\n",
    "\n",
    "    @modal.fastapi_endpoint(\n",
    "        method=\"POST\",\n",
    "        docs=True,\n",
    "    )\n",
    "    def edit_image_endpoint(\n",
    "        self,\n",
    "        image_url: str,\n",
    "        prompt: str,\n",
    "        negative_prompt: str = \" \",\n",
    "        true_cfg_scale: float = 4.0,\n",
    "        seed: int = 0,\n",
    "        randomize_seed: bool = False,\n",
    "        num_inference_steps: int = 8,\n",
    "    ):\n",
    "        \"\"\"Public FastAPI endpoint for image editing\"\"\"\n",
    "        return self.edit_image(\n",
    "            image_url=image_url,\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            true_cfg_scale=true_cfg_scale,\n",
    "            seed=seed,\n",
    "            randomize_seed=randomize_seed,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce367dc",
   "metadata": {},
   "source": [
    "# Testing the Endpoints - Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ad5303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "endpoint_url = \"https://drchrislevy--qwen-image-editor-qwenimageeditor-edit-imag-10fda9.modal.run/\"\n",
    "endpoint_url_fast = \"https://drchrislevy--qwen-image-editor-fast-qwenimageeditor-edit-516e26.modal.run/\"\n",
    "\n",
    "\n",
    "def download_generated_image(result):\n",
    "    time.sleep(5)  # wait for volume sync\n",
    "    filename = result[\"image_url\"].split(\"path=\")[-1]\n",
    "    download_command = f\"modal volume get qwen_edited_images {filename} ../static_blog_imgs/{filename} --force\"\n",
    "    subprocess.run(download_command, shell=True, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    p = f\"../static_blog_imgs/{filename}\"\n",
    "    return Image.open(p)\n",
    "\n",
    "\n",
    "def generate_image(endpoint, image_url, prompt, num_inference_steps=None, negative_prompt=\" \", true_cfg_scale=4.0, seed=0, randomize_seed=False):\n",
    "    if num_inference_steps is None:\n",
    "        if endpoint == endpoint_url:\n",
    "            num_inference_steps = 50\n",
    "        elif endpoint == endpoint_url_fast:\n",
    "            num_inference_steps = 8\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid endpoint: {endpoint}\")\n",
    "\n",
    "    response = requests.post(\n",
    "        endpoint,\n",
    "        params={\n",
    "            \"image_url\": image_url,\n",
    "            \"prompt\": prompt,\n",
    "            \"negative_prompt\": negative_prompt,\n",
    "            \"true_cfg_scale\": true_cfg_scale,\n",
    "            \"seed\": seed,\n",
    "            \"randomize_seed\": randomize_seed,\n",
    "            \"num_inference_steps\": num_inference_steps,\n",
    "        },\n",
    "        timeout=180,\n",
    "    )\n",
    "    res = response.json()\n",
    "    # image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "    # display(image)\n",
    "    download_generated_image(res)\n",
    "    # display(download_generated_image(res))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36923815",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = (\n",
    "    \"https://img.alicdn.com/imgextra/i3/O1CN01XfJ71c1qokTchToKf_!!6000000005543-2-tps-1248-832.png?x-oss-process=image/resize,m_mfit,w_320,h_320\"\n",
    ")\n",
    "res = generate_image(endpoint_url_fast, image_url, \"transform this image into Ghibli style\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d32dcd",
   "metadata": {},
   "source": [
    "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01XfJ71c1qokTchToKf_!!6000000005543-2-tps-1248-832.png?x-oss-process=image/resize,m_mfit,w_320,h_320\" width=\"50%\">\n",
    "\n",
    "<img src=\"static_blog_imgs/qwen_edited_20250819_195701_158fca45.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = (\n",
    "    \"https://img.alicdn.com/imgextra/i3/O1CN01m3Jkqd1UoMb4edofx_!!6000000002564-2-tps-832-1248.png?x-oss-process=image/resize,m_mfit,w_320,h_320\"\n",
    ")\n",
    "res = generate_image(endpoint_url, image_url, \"change the color of the purse to the same color of the jacket\", num_inference_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fcbeb8",
   "metadata": {},
   "source": [
    "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01m3Jkqd1UoMb4edofx_!!6000000002564-2-tps-832-1248.png?x-oss-process=image/resize,m_mfit,w_320,h_320\" width=\"50%\">\n",
    "\n",
    "<img src=\"static_blog_imgs/qwen_edited_20250819_195903_9d311654.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c2ce5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = (\n",
    "    \"https://img.alicdn.com/imgextra/i3/O1CN01m3Jkqd1UoMb4edofx_!!6000000002564-2-tps-832-1248.png?x-oss-process=image/resize,m_mfit,w_320,h_320\"\n",
    ")\n",
    "res = generate_image(endpoint_url_fast, image_url, \"change the color of the purse to the same color of the jacket\", num_inference_steps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a7e98e",
   "metadata": {},
   "source": [
    "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01m3Jkqd1UoMb4edofx_!!6000000002564-2-tps-832-1248.png?x-oss-process=image/resize,m_mfit,w_320,h_3200\" width=\"50%\">\n",
    "\n",
    "<img src=\"static_blog_imgs/qwen_edited_20250819_195958_8ae72cbd.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f49ccef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = (\n",
    "    \"https://img.alicdn.com/imgextra/i3/O1CN01m3Jkqd1UoMb4edofx_!!6000000002564-2-tps-832-1248.png?x-oss-process=image/resize,m_mfit,w_320,h_320\"\n",
    ")\n",
    "res = generate_image(endpoint_url_fast, image_url, \"change the color of the hair to blonde\", num_inference_steps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f8b1ca",
   "metadata": {},
   "source": [
    "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01m3Jkqd1UoMb4edofx_!!6000000002564-2-tps-832-1248.png?x-oss-process=image/resize,m_mfit,w_320,h_3200\" width=\"50%\">\n",
    "\n",
    "<img src=\"static_blog_imgs/qwen_edited_20250819_201014_a413bab5.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6ed24ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = (\n",
    "    \"https://img.alicdn.com/imgextra/i3/O1CN01m3Jkqd1UoMb4edofx_!!6000000002564-2-tps-832-1248.png?x-oss-process=image/resize,m_mfit,w_320,h_320\"\n",
    ")\n",
    "res = generate_image(endpoint_url, image_url, \"change the color of the hair to blonde\", num_inference_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb57fc9",
   "metadata": {},
   "source": [
    "<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01m3Jkqd1UoMb4edofx_!!6000000002564-2-tps-832-1248.png?x-oss-process=image/resize,m_mfit,w_320,h_3200\" width=\"50%\">\n",
    "\n",
    "<img src=\"static_blog_imgs/qwen_edited_20250819_201222_36e07535.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53ba4b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = (\n",
    "    \"https://img.alicdn.com/imgextra/i4/O1CN01hZNlck1mYwLJKmEaI_!!6000000004967-2-tps-1024-1024.png?x-oss-process=image/resize,m_mfit,w_320,h_320\"\n",
    ")\n",
    "res = generate_image(endpoint_url_fast, image_url, \"face to the right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83955d3",
   "metadata": {},
   "source": [
    "<img src=\"https://img.alicdn.com/imgextra/i4/O1CN01hZNlck1mYwLJKmEaI_!!6000000004967-2-tps-1024-1024.png?x-oss-process=image/resize,m_mfit,w_320,h_320\" width=\"50%\">\n",
    "\n",
    "<img src=\"static_blog_imgs/qwen_edited_20250819_200111_0e68ba18.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa16e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = (\n",
    "    \"https://img.alicdn.com/imgextra/i4/O1CN01hZNlck1mYwLJKmEaI_!!6000000004967-2-tps-1024-1024.png?x-oss-process=image/resize,m_mfit,w_320,h_320\"\n",
    ")\n",
    "res = generate_image(endpoint_url_fast, image_url, \"change the color of the door to yellow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbe513e",
   "metadata": {},
   "source": [
    "<img src=\"https://img.alicdn.com/imgextra/i4/O1CN01hZNlck1mYwLJKmEaI_!!6000000004967-2-tps-1024-1024.png?x-oss-process=image/resize,m_mfit,w_320,h_320\" width=\"50%\">\n",
    "\n",
    "<img src=\"static_blog_imgs/qwen_edited_20250819_200151_c9665a26.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0fba6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = (\n",
    "    \"https://img.alicdn.com/imgextra/i4/O1CN01hZNlck1mYwLJKmEaI_!!6000000004967-2-tps-1024-1024.png?x-oss-process=image/resize,m_mfit,w_320,h_320\"\n",
    ")\n",
    "res = generate_image(endpoint_url_fast, image_url, \"turn the dog around\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8f7b09",
   "metadata": {},
   "source": [
    "<img src=\"https://img.alicdn.com/imgextra/i4/O1CN01hZNlck1mYwLJKmEaI_!!6000000004967-2-tps-1024-1024.png?x-oss-process=image/resize,m_mfit,w_320,h_320\" width=\"50%\">\n",
    "\n",
    "<img src=\"static_blog_imgs/qwen_edited_20250819_200227_da26e28f.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b86fbb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = (\n",
    "    \"https://img.alicdn.com/imgextra/i4/O1CN01hZNlck1mYwLJKmEaI_!!6000000004967-2-tps-1024-1024.png?x-oss-process=image/resize,m_mfit,w_320,h_320\"\n",
    ")\n",
    "res = generate_image(endpoint_url_fast, image_url, \"change the dog to sitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0a6258",
   "metadata": {},
   "source": [
    "<img src=\"https://img.alicdn.com/imgextra/i4/O1CN01hZNlck1mYwLJKmEaI_!!6000000004967-2-tps-1024-1024.png?x-oss-process=image/resize,m_mfit,w_320,h_320\" width=\"50%\">\n",
    "\n",
    "<img src=\"static_blog_imgs/qwen_edited_20250819_200326_30d6e1b6.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cbcc133",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = (\n",
    "    \"https://img.alicdn.com/imgextra/i4/O1CN01GzeRec1ji9SfxtvRm_!!6000000004581-2-tps-1184-896.png?x-oss-process=image/resize,m_mfit,w_320,h_320\"\n",
    ")\n",
    "prompt = \"Restore old photograph, remove scratches, reduce noise, enhance details, high resolution, realistic, natural skin tones, clear facial features, no distortion, vintage photo restoration.\"\n",
    "res = generate_image(endpoint_url_fast, image_url, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9473c0",
   "metadata": {},
   "source": [
    "<img src=\"https://img.alicdn.com/imgextra/i4/O1CN01GzeRec1ji9SfxtvRm_!!6000000004581-2-tps-1184-896.png?x-oss-process=image/resize,m_mfit,w_320,h_320\" width=\"50%\">\n",
    "\n",
    "<img src=\"static_blog_imgs/qwen_edited_20250819_200400_d8a97e73.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "221f26e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://travelbynatasha.com/wp-content/uploads/2023/09/dsc_2272.jpg\"\n",
    "prompt = \"Add a sunset to this image\"\n",
    "res = generate_image(endpoint_url_fast, image_url, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec110b3",
   "metadata": {},
   "source": [
    "<img src=\"https://travelbynatasha.com/wp-content/uploads/2023/09/dsc_2272.jpg\" width=\"50%\">\n",
    "\n",
    "<img src=\"static_blog_imgs/qwen_edited_20250819_200438_d6dc4f42.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "356cc8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://travelbynatasha.com/wp-content/uploads/2023/09/dsc_2272.jpg\"\n",
    "prompt = \"Add a fishing boat to the image\"\n",
    "res = generate_image(endpoint_url_fast, image_url, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ef82f5",
   "metadata": {},
   "source": [
    "<img src=\"https://travelbynatasha.com/wp-content/uploads/2023/09/dsc_2272.jpg\" width=\"50%\">\n",
    "\n",
    "<img src=\"static_blog_imgs/qwen_edited_20250819_200531_a9c39225.jpg\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76a190ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://travelbynatasha.com/wp-content/uploads/2023/09/dsc_2272.jpg\"\n",
    "prompt = \"Add a pod of killer whales swimming out of the cove\"\n",
    "res = generate_image(endpoint_url_fast, image_url, prompt, num_inference_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15bb083",
   "metadata": {},
   "source": [
    "<img src=\"https://travelbynatasha.com/wp-content/uploads/2023/09/dsc_2272.jpg\" width=\"50%\">\n",
    "\n",
    "<img src=\"static_blog_imgs/qwen_edited_20250819_201650_93bd87b4.jpg\" width=\"50%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
