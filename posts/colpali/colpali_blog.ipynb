{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7908ac58af685d94",
   "metadata": {
    "collapsed": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Chat with PDF App using ColPali, Modal, and FastHTML\n",
    "author: Chris Levy\n",
    "date: '2024-10-26'\n",
    "date-modified: '2024-10-24'\n",
    "image: imgs/intro.png\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: show\n",
    "    page-layout: full\n",
    "include-in-header:\n",
    "  - text: |\n",
    "      <style>\n",
    "      .cell-output-stdout code {\n",
    "        word-break: break-wor !important;\n",
    "        white-space: pre-wrap !important;\n",
    "      }\n",
    "      </style>\n",
    "bibliography: ../../bibliography.bib\n",
    "resources:\n",
    "    - imgs/modal.mp4\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a088ff1f",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "\n",
    "Lately, I've been following [FastHTML](https://about.fastht.ml/) from a distance. As someone who sticks to backend Python development, frontend development has always been a bit foreign to me, but I'm interested in giving it a shot. FastHTML feels like a good way to get started with some basics by building small apps.\n",
    "\n",
    "I've also noticed a lot of chatter on X about [Colpali](https://github.com/illuin-tech/colpali) and document retrieval with vision language models, which caught my attention. I like exploring new stuff so I want to see what that is all about.\n",
    "\n",
    "On top of that, I'm still enjoying [Modal](https://modal.com/), which Iâ€™ve written about before [here](https://drchrislevy.github.io/posts/modal_fun/modal_blog.html) and [here](https://drchrislevy.github.io/posts/intro_modal/intro_modal.html). I thought it would be fun to combine these tools into a simple app and see what I can learn from it.\n",
    "\n",
    "\n",
    "# ColPali\n",
    "\n",
    "There are already so many great resources out there about ColPali. Checkout the resources [below](#resources) for more information.\n",
    "I will give a quick overview.\n",
    "\n",
    "I have already deployed ColPali to Modal as a remote function I can call, running on an A10 GPU.\n",
    "\n",
    "```\n",
    "modal deploy pdf_retriever.py\n",
    "```\n",
    "\n",
    "Remember that with Modal, you only pay for compute when running requests in active containers. My deployed app\n",
    "can sit there idle without costing me anything!\n",
    "\n",
    "![ColPali Model Deployed on Modal](imgs/colpali_modal.png)\n",
    "\n",
    "\n",
    "There are a couple functions I have decorated with `@modal.method()` within the `PDFRetriever` class:\n",
    "\n",
    "**TODO: add link to the code**\n",
    "\n",
    "-  `def forward(self, inputs)`\n",
    "-  `def top_pages(self, pdf_url, queries, use_cache=True, top_k=1)`\n",
    "\n",
    "\n",
    "Let's look at the `forward` function first as it can be used to run inference on a list of strings or images to get the embeddings.\n",
    "\n",
    "First we will pass in text inputs to ColPali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da2ab8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopher/personal_projects/DrChrisLevy.github.io/posts/colpali/env/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:258: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1680, -0.0111,  0.0957,  ..., -0.0272, -0.0762, -0.0249],\n",
       "        [-0.0737, -0.0918,  0.0698,  ...,  0.0195, -0.1162,  0.0593],\n",
       "        [ 0.1357, -0.0054, -0.0986,  ...,  0.1279, -0.0537, -0.0309],\n",
       "        ...,\n",
       "        [ 0.1270, -0.0297,  0.0503,  ...,  0.0908, -0.1523,  0.0200],\n",
       "        [ 0.1152, -0.0654,  0.0454,  ...,  0.0996, -0.1079, -0.0101],\n",
       "        [ 0.1533,  0.0125, -0.0135,  ...,  0.1553, -0.0972, -0.0305]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | warning: false\n",
    "import modal\n",
    "\n",
    "get_embeddings = modal.Function.lookup(\"pdf-retriever\", \"PDFRetriever.forward\")\n",
    "embeddings_batch = get_embeddings.remote([\"What days in October had anomalies in the sales data?\"])\n",
    "assert len(embeddings_batch) == 1  # we passed in one document i.e. batch size of 1\n",
    "embeddings = embeddings_batch[0]\n",
    "print(embeddings.shape)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a792913a",
   "metadata": {},
   "source": [
    "The first thing to note is that we don't get a single dense embedding vector.\n",
    "Traditionally that is the case where a single vector is used to represent one input.\n",
    "But ColPali is generating ColBERT-style multi-vector representations of the input.\n",
    "With the late interaction paradigm you get back multiple embeddings, one per input **token**.\n",
    "Each embedding is 128-dimensional. \n",
    "\n",
    "\n",
    "ColPali is trained to take image documents as input.\n",
    "It was trained on query-document pairs where each document is a page of a PDF.\n",
    "Each PDF page (\"document\") is treated as an image. It uses a vision language model to create \n",
    "multi-vector embeddings purely from visual document features.\n",
    "\n",
    "Consider the following image of a PDF page from the ColPali paper:\n",
    "\n",
    "![ColPali Paper PDF Page 2](imgs/colpali_paper_page_sample.png)\n",
    "We can pass this image to the `forward` function and get the embeddings back.\n",
    "The ColPali model divides each page image into a 32 x 32 = 1024 patches.\n",
    "In addition to the image grid patches, ColPali includes 6 instruction text tokens that are prepended to the image input. \n",
    "These tokens represent the text: \"Describe the image.\" Combining the image grid patches and the instruction tokens, we get:\n",
    "1024 (image patches) + 6 (instruction tokens) = 1030 total patches/embeddings.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1855bd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1030, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1543, -0.0332, -0.1001,  ...,  0.1436, -0.0928,  0.1108],\n",
       "        [-0.1152,  0.0593,  0.0972,  ..., -0.0347, -0.0114,  0.0815],\n",
       "        [-0.1533,  0.0422,  0.0776,  ..., -0.0248, -0.0116,  0.0518],\n",
       "        ...,\n",
       "        [ 0.0884,  0.0054,  0.0698,  ..., -0.0850, -0.1206, -0.0461],\n",
       "        [-0.0116,  0.0908,  0.0752,  ..., -0.0248, -0.0449, -0.1016],\n",
       "        [ 0.0116,  0.0442,  0.1416,  ...,  0.0087, -0.1602, -0.1611]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"imgs/colpali_paper_page_sample.png\")\n",
    "embeddings = get_embeddings.remote([img])[0]\n",
    "print(embeddings.shape)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c24ac8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "413d1b17",
   "metadata": {},
   "source": [
    "Each PDF page/image (document) can be indexed with the ColPali model to get the multi-vector embeddings per page.\n",
    "At query time, we use the same model to generate multi-vector embeddings for the query. \n",
    "So both queries and documents are represented as sets of vectors rather than single vector.\n",
    "\n",
    "The MaxSim (Maximum Similarity) scoring function is used to compute the similarity between query embeddings and document embeddings.\n",
    "The scoring function performs the following steps:\n",
    "\n",
    "- Computes dot products between all query token embeddings and all document page patch embeddings\n",
    "- Applies a max reduce operation over the patch dimension\n",
    "- Performs a sum reduce operation over the query tokens\n",
    "\n",
    "There is a great and simple explanation in this [blog post](Both queries and documents are represented as sets of vectors rather than single vector.)\n",
    "\n",
    "I have wrapped the logic for a given pdf url and query/question within the deployed Modal function `def top_pages(self, pdf_url, queries, use_cache=True, top_k=1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "155965b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 4, 5, 13]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_pages = modal.Function.lookup(\"pdf-retriever\", \"PDFRetriever.top_pages\")\n",
    "pdf_url = \"https://arxiv.org/pdf/2407.01449\"\n",
    "top_pages = get_top_pages.remote(pdf_url, queries=[\"How does the latency between ColPali and standard retrieval methods compare?\"], top_k=5)[0]\n",
    "top_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca2cba0",
   "metadata": {},
   "source": [
    "The function takes a `pdf_url` and a list of `queries` (questions) and returns the top `top_k` pages for each query/question.\n",
    "ColPali is used to retrieve the most relevant pages from the PDF. \n",
    "\n",
    "# Generating the Final Answer with a Vision Language Model\n",
    "\n",
    "Once we have the top pages/images as context, we can pass them along with the query/question to a vision language model to generate an answer.\n",
    "The images are passed as the context and the question/query is passed as text. I have this logic deployed in a Modal Application as well\n",
    "running on CPU. It communicates with the deployed ColPali Modal app running on the GPU when it needs to compute the embeddings.\n",
    "\n",
    "```\n",
    "modal deploy multi_modal_rag.py\n",
    "```\n",
    "\n",
    "The deployed Modal function here is \n",
    "\n",
    "```\n",
    "def answer_question_with_image_context(pdf_url, query, top_k=1, use_cache=True, max_new_tokens=2000, additional_instructions=\"\"):\n",
    "```\n",
    "\n",
    "**TODO: add link to the code**\n",
    "\n",
    "I will explain all the arguments in the function later when we look at the FastHTML App.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ba667dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latency comparison between ColPali and standard retrieval methods shows a significant improvement. \n",
      "\n",
      "- **Standard Retrieval**: The latency is approximately **7.22 seconds per page** for processing.\n",
      "- **ColPali**: The latency is reduced to about **0.39 seconds per page**.\n",
      "\n",
      "Additionally, when querying, ColPali has a latency of **22 milliseconds per query**, while standard methods require longer processing times. This indicates that ColPali is considerably faster than traditional methods for both indexing and querying.\n"
     ]
    }
   ],
   "source": [
    "answer_question_with_image_context = modal.Function.lookup(\"multi-modal-rag\", \"answer_question_with_image_context\")\n",
    "res = answer_question_with_image_context.remote_gen(\n",
    "    pdf_url=\"https://arxiv.org/pdf/2407.01449\", query=\"How does the latency between ColPali and standard retrieval methods compare?\", top_k=5\n",
    ")\n",
    "answer = \"\".join([chunk for chunk in res if type(chunk) == str])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2385ce",
   "metadata": {},
   "source": [
    "I am simply using OpenAI's `gpt-4o-mini` as the vision language model here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6e3a99",
   "metadata": {},
   "source": [
    "# FastHTML App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0821f02",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3b28ff1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e545913",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Resources\n",
    "\n",
    "In no particular order:\n",
    "\n",
    "- [Colpali paper](https://arxiv.org/pdf/2407.01449v2)\n",
    "- [Colbert paper](https://arxiv.org/pdf/2004.12832)\n",
    "- [Colbert V2 paper](https://arxiv.org/pdf/2112.01488)\n",
    "- [PaliGemma](https://arxiv.org/pdf/2407.07726)\n",
    "- [A little pooling goes a long way for multi-vector representations: Blog answer.ai](https://www.answer.ai/posts/colbert-pooling.html)\n",
    "    - [Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling, Paper](https://arxiv.org/pdf/2409.14683)\n",
    "- [PLAID paper](https://arxiv.org/pdf/2205.09707)\n",
    "- [Beyond the Basics of Retrieval for Augmenting Generation (w/ Ben ClaviÃ©), Youtube Talk](https://www.youtube.com/watch?v=0nA5QG3087g)\n",
    "- [RAG is more than dense embedding, Google Slides, Ben ClaviÃ©](https://docs.google.com/presentation/d/1Zczs5Sk3FsCO06ZLDznqkOOhbTe96PwJa4_7FwyMBrA/edit#slide=id.p)\n",
    "- The quick start in the README [Original ColPali Repo](https://github.com/illuin-tech/colpali) as well as the sample [inference code](https://github.com/illuin-tech/colpali/blob/main/scripts/infer/run_inference_with_python.py)\n",
    "- [Hugging Face Model Cards](https://huggingface.co/vidore/colpali-v1.2)\n",
    "- [The Future of Search: Vision Models and the Rise of Multi-Model Retrieval](https://mcplusa.com/the-future-of-search-vision-models-and-the-rise-of-multi-model-retrieval/)\n",
    "- [Scaling ColPali to billions of PDFs with Vespa](https://blog.vespa.ai/scaling-colpali-to-billions/)\n",
    "- [Beyond Text: The Rise of Vision-Driven Document Retrieval for RAG](https://blog.vespa.ai/the-rise-of-vision-driven-document-retrieval-for-rag/)\n",
    "- [Vision Language Models Explained](https://huggingface.co/blog/vlms)\n",
    "- [Document Similarity Search with ColPali](https://huggingface.co/blog/fsommers/document-similarity-colpali)\n",
    "- [Jo Kristian Bergum: X](https://x.com/jobergum)\n",
    "- [Manuel Faysse: X](https://x.com/ManuelFaysse)\n",
    "- [Tony Wu: X](https://x.com/tonywu_71)\n",
    "- [Omar Khattab: X](https://x.com/lateinteraction?lang=en)\n",
    "- [fastHTML](https://about.fastht.ml/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9db69f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
