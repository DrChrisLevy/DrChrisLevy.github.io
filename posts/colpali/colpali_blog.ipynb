{
 "cells": [
  {
   "cell_type": "raw",
   "id": "7908ac58af685d94",
   "metadata": {
    "collapsed": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Chat with PDF App using ColPali, Modal, and FastHTML\n",
    "author: Chris Levy\n",
    "date: '2024-10-30'\n",
    "date-modified: '2024-10-30'\n",
    "image: imgs/intro.png\n",
    "toc: true\n",
    "format:\n",
    "  html:\n",
    "    code-fold: show\n",
    "    page-layout: full\n",
    "include-in-header:\n",
    "  - text: |\n",
    "      <style>\n",
    "      .cell-output-stdout code {\n",
    "        word-break: break-wor !important;\n",
    "        white-space: pre-wrap !important;\n",
    "      }\n",
    "      </style>\n",
    "bibliography: ../../bibliography.bib\n",
    "resources:\n",
    "    - imgs/modal.mp4\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a088ff1f",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "\n",
    "Lately, I've been following [FastHTML](https://about.fastht.ml/) from a distance. As someone who sticks to backend Python development, frontend development has always been a bit foreign to me, but I'm interested in giving it a shot. FastHTML feels like a good way to get started with some basics by building small apps.\n",
    "\n",
    "I've also noticed a lot of chatter on X about [Colpali](https://github.com/illuin-tech/colpali) and document retrieval with vision language models, which caught my attention. I like exploring new stuff so I want to see what that is all about.\n",
    "\n",
    "On top of that, I'm still enjoying [Modal](https://modal.com/), which I’ve written about before [here](https://drchrislevy.github.io/posts/modal_fun/modal_blog.html) and [here](https://drchrislevy.github.io/posts/intro_modal/intro_modal.html). I thought it would be fun to combine these tools into a simple app and see what I can learn from it.\n",
    "\n",
    "All the code for this project is in this [folder](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/colpali/README.md).\n",
    "The main code is the following:\n",
    "\n",
    "- [multi_modal_rag.py](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/colpali/multi_modal_rag.py) - A Modal app running on CPU that runs the multimodal retrieval logic.\n",
    "- [pdf_retriever.py](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/colpali/pdf_retriever.py) - A Modal app running on GPU which processes and caches images/embeddings for each PDF and runs inference for ColPali.\n",
    "- [utils.py](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/colpali/utils.py) - some simple utility functions for logging and generating unique folder names in the Modal Volumes.\n",
    "- [main.py](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/colpali/main.py) - the FastHTML app that runs the frontend.\n",
    "- [colpali_blog.ipynb](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/colpali/colpali_blog.ipynb) - a notebook that I used to generate the blog post for this project.\n",
    "\n",
    "See the [README](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/colpali/README.md) for more details.\n",
    "\n",
    "# ColPali\n",
    "\n",
    "There are already so many great resources out there about ColPali. Checkout the resources [below](#resources) for more information.\n",
    "I will give a quick overview.\n",
    "\n",
    "I have already deployed ColPali to Modal as a remote function I can call, running on an A10 GPU.\n",
    "\n",
    "```\n",
    "modal deploy pdf_retriever.py\n",
    "```\n",
    "\n",
    "Remember that with Modal, you only pay for compute when running requests in active containers. My deployed app\n",
    "can sit there idle without costing me anything!\n",
    "\n",
    "![ColPali Model Deployed on Modal](imgs/colpali_modal.png)\n",
    "\n",
    "\n",
    "There are a couple functions I have decorated with `@modal.method()` within the `PDFRetriever` class:\n",
    "\n",
    "-  `def forward(self, inputs)` --> [here](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/colpali/pdf_retriever.py#L72)\n",
    "-  `def top_pages(self, pdf_url, queries, use_cache=True, top_k=1)` --> [here](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/colpali/pdf_retriever.py#L103)\n",
    "\n",
    "\n",
    "Let's look at the `forward` function first as it can be used to run inference on a list of strings or images to get the embeddings.\n",
    "\n",
    "First we will pass in text inputs to ColPali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da2ab8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopher/personal_projects/DrChrisLevy.github.io/posts/colpali/env/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:258: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1572, -0.0240,  0.0942,  ..., -0.0278, -0.0791, -0.0129],\n",
       "        [-0.0688, -0.1260,  0.0038,  ..., -0.0073, -0.1162,  0.0962],\n",
       "        [ 0.0413, -0.1055, -0.1055,  ..., -0.0055, -0.2178,  0.1406],\n",
       "        ...,\n",
       "        [-0.0825, -0.0444, -0.0674,  ..., -0.0327, -0.1504,  0.1670],\n",
       "        [ 0.1465,  0.0016, -0.1338,  ...,  0.0127, -0.2119,  0.1191],\n",
       "        [ 0.1641, -0.0405, -0.1338,  ...,  0.0175, -0.2080,  0.1177]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | warning: false\n",
    "import modal\n",
    "\n",
    "forward = modal.Function.lookup(\"pdf-retriever\", \"PDFRetriever.forward\")\n",
    "embeddings_batch = forward.remote([\"How does the latency between ColPali and standard retrieval methods compare?\"])\n",
    "assert len(embeddings_batch) == 1  # we passed in one document i.e. batch size of 1\n",
    "embeddings = embeddings_batch[0]\n",
    "print(embeddings.shape)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a792913a",
   "metadata": {},
   "source": [
    "The first thing to note is that we don't get a single dense embedding vector.\n",
    "Traditionally that is the case where a single vector is used to represent one input.\n",
    "But ColPali is generating ColBERT-style multi-vector representations of the input.\n",
    "With the late interaction paradigm you get back multiple embeddings, one per input **token**.\n",
    "Each embedding is 128-dimensional. \n",
    "\n",
    "\n",
    "ColPali is trained to take image documents as input.\n",
    "It was trained on query-document pairs where each document is a page of a PDF.\n",
    "Each PDF page (\"document\") is treated as an image. It uses a vision language model to create \n",
    "multi-vector embeddings purely from visual document features.\n",
    "\n",
    "Consider the following image of a PDF page from the ColPali paper:\n",
    "\n",
    "![ColPali Paper PDF Page 2](imgs/colpali_paper_page_sample.png)\n",
    "We can pass this image to the `forward` function and get the embeddings back.\n",
    "The ColPali model divides each page image into a 32 x 32 = 1024 patches.\n",
    "In addition to the image grid patches, ColPali includes 6 instruction text tokens that are prepended to the image input. \n",
    "These tokens represent the text: \"Describe the image.\" Combining the image grid patches and the instruction tokens, we get:\n",
    "1024 (image patches) + 6 (instruction tokens) = 1030 total patches/embeddings.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1855bd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1030, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1562, -0.0396, -0.0908,  ...,  0.1426, -0.1113,  0.1079],\n",
       "        [-0.1260,  0.0427,  0.0991,  ..., -0.0286, -0.0170,  0.0786],\n",
       "        [-0.1621,  0.0297,  0.0874,  ..., -0.0255, -0.0168,  0.0625],\n",
       "        ...,\n",
       "        [ 0.1045, -0.0178,  0.0522,  ..., -0.0986, -0.1011, -0.0366],\n",
       "        [ 0.0078,  0.0674,  0.0674,  ..., -0.0226, -0.0479, -0.0908],\n",
       "        [ 0.0062,  0.0623,  0.1396,  ...,  0.0264, -0.1699, -0.1533]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"imgs/colpali_paper_page_sample.png\")\n",
    "embeddings = forward.remote([img])[0]\n",
    "print(embeddings.shape)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c24ac8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "413d1b17",
   "metadata": {},
   "source": [
    "Using the ColPali model we produce multi-vector embeddings per page which can be indexed.\n",
    "At query time, we use the same model to generate multi-vector embeddings for the query. \n",
    "So both queries and documents are represented as sets of vectors rather than single vector.\n",
    "\n",
    "The MaxSim (Maximum Similarity) scoring function is used to compute the similarity between query embeddings and document embeddings.\n",
    "The scoring function performs the following steps:\n",
    "\n",
    "- Computes dot products between all query token embeddings and all document page patch embeddings\n",
    "- Applies a max reduce operation over the patch dimension\n",
    "- Performs a sum reduce operation over the query tokens\n",
    "\n",
    "There is a great and simple explanation in this [blog post](Both queries and documents are represented as sets of vectors rather than single vector.)\n",
    "\n",
    "I have wrapped the logic for a given PDF url and query/question within the deployed Modal function \n",
    "\n",
    "`def top_pages(self, pdf_url, queries, use_cache=True, top_k=1)`.\n",
    "\n",
    "The function takes a `pdf_url` and a list of `queries` (questions) and returns the top `top_k` pages for each query/question.\n",
    "The use of ColPali and the MaxSim scoring function allows us to retrieve the most relevant pages from the PDF\n",
    "that will assist in answering the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "155965b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 4]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_pages = modal.Function.lookup(\"pdf-retriever\", \"PDFRetriever.top_pages\")\n",
    "pdf_url = \"https://arxiv.org/pdf/2407.01449\"\n",
    "top_pages = get_top_pages.remote(pdf_url, queries=[\"How does the latency between ColPali and standard retrieval methods compare?\"], top_k=3)[0]\n",
    "top_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e758fc3",
   "metadata": {},
   "source": [
    "This first returned index page `1` is actually the second page of the PDF since we start counting from `0`.\n",
    "And that page being returned is the image we saw earlier from the ColPali paper. It's really cool\n",
    "because the answer is found in the figure on that page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca2cba0",
   "metadata": {},
   "source": [
    "# Generating the Answer\n",
    "\n",
    "Once we have the top pages/images as context, we can pass them along with the query/question to a vision language model to generate an answer.\n",
    "The images are passed as the context and the question/query is passed as text. I have this logic deployed in a Modal Application as well\n",
    "running on CPU. It communicates with the other deployed ColPali Modal app running on the GPU when it needs to compute the embeddings.\n",
    "I am using OpenAI's `gpt-4o-mini` for the vision language model to generate the answer with the provided image context and question.\n",
    "\n",
    "```\n",
    "modal deploy multi_modal_rag.py\n",
    "```\n",
    "\n",
    "The deployed Modal function [here](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/colpali/multi_modal_rag.py#L62) is \n",
    "\n",
    "```\n",
    "def answer_question_with_image_context(pdf_url, query, top_k=1, use_cache=True, max_new_tokens=2000, additional_instructions=\"\"):\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ba667dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latency comparison between ColPali and standard retrieval methods indicates that ColPali is significantly faster. Specifically:\n",
      "\n",
      "- **ColPali**: 0.39 seconds per page.\n",
      "- **Standard Retrieval**: 7.22 seconds per page.\n",
      "\n",
      "This demonstrates that ColPali achieves better performance in terms of latency while maintaining a stronger relevance score in document retrieval tasks.\n"
     ]
    }
   ],
   "source": [
    "answer_question_with_image_context = modal.Function.lookup(\"multi-modal-rag\", \"answer_question_with_image_context\")\n",
    "res = answer_question_with_image_context.remote_gen(\n",
    "    pdf_url=\"https://arxiv.org/pdf/2407.01449\", query=\"How does the latency between ColPali and standard retrieval methods compare?\", top_k=5\n",
    ")\n",
    "answer = \"\".join([chunk for chunk in res if type(chunk) == str])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2385ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f6e3a99",
   "metadata": {},
   "source": [
    "# FastHTML App\n",
    "\n",
    "To demo the FastHTML App I created, I will share images and videos of running it locally.\n",
    "The entire app is in the code [main.py](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/colpali/main.py).\n",
    "\n",
    "```\n",
    "python main.py\n",
    "```\n",
    "\n",
    "Here is what the app looks like when you first load it up:\n",
    "\n",
    "![](imgs/fasthtml_demo1.png)\n",
    "\n",
    "\n",
    "Here are two videos of running the app and asking questions about the ColPali paper.\n",
    "\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=YoXkFCA0qC8 >}}\n",
    "\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=AR7h95IppMU >}}\n",
    "\n",
    "\n",
    "This PDF url of the ColPali paper was already processed and cached which means I already stored the embeddings and images\n",
    "inside volumes on Modal. So it loads the document embeddings and images very quickly. Also, the Modal container was warm\n",
    "and running so there were no cold start delays.\n",
    "\n",
    "In this next video I will demo the app with a new PDF url that was not processed and cached yet.\n",
    "I will also send the requests to the backend when the Modal containers are idle.\n",
    "These requests will trigger the Modal containers to start up and run the inference.\n",
    "It will take longer but you will see how everything is logged from the backend in the terminal window I created.\n",
    "It uses server-sent events (SSE) to stream the logs to the frontend so you can see what is happening in the backend.\n",
    "This example will use a longer PDF from Meta, [Movie Gen: A Cast of Media Foundation Models](https://ai.meta.com/static-resource/movie-gen-research-paper),\n",
    "which is 92 pages.\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=Eu6QJjD73N0&list=PLSF4aA8KYOVwRv4I6hBgdlfw4uy5JLE0V&index=3 >}}\n",
    "\n",
    "This next video runs the same PDF and question a second time. Now that all the images and document embeddings are cached\n",
    "in a volume on Modal, everything is much faster. This is also using a warm Modal container so there were no cold start delays.\n",
    "Most of the time is spent in the OpenAI API call which takes five images as input and streams back the text response.\n",
    "\n",
    "{{< video https://www.youtube.com/watch?v=Z-EOqVBibSY&list=PLSF4aA8KYOVwRv4I6hBgdlfw4uy5JLE0V&index=4 >}}\n",
    "\n",
    "# Highlights\n",
    "\n",
    "There are a few highlights I want to call out.\n",
    "The first is the use of server-sent events (SSE) to stream the logs to the frontend.\n",
    "The backend code is running in the cloud on Modal's infrastructure.\n",
    "In the frontend code I created the terminal looking window with this [code](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/colpali/main.py#L54-L60).\n",
    "It continually calls the `/poll-queue` endpoint to get the latest logs from Modal and streams them via SSE. \n",
    "In Modal I am using a Queue to collect the logs. Throughout my Modal application code I use these [functions](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/colpali/utils.py#L18-L29). Anytime I want to log a message I just call `log_to_queue`. It gets placed on the queue and then\n",
    "`read_from_queue` is used to pop the message off the queue and display it. It's a fun and neat way to provide more visibility\n",
    "to the frontend about what the backend is doing.\n",
    "\n",
    "![](imgs/fasthtml_demo2.png)\n",
    "\n",
    "\n",
    "Another highlight is the use of Modal's volume functionality. \n",
    "I use a volume to store the images and document embeddings for each PDF that is processed.\n",
    "This way if the PDF is used a second time, the images and embeddings are stored to \n",
    "the Volume for fast retrieval. This avoids having to call ColPali processing and PDF\n",
    "processing for each question/query related to the same document.\n",
    "\n",
    "One final highlight was streaming the OpenAI response back to the frontend in markdown format via SSE.\n",
    "This took me a while to figure out how to do. On the frontend I did [this](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/colpali/main.py#L131).\n",
    "There could be better ways to do this but it works for now. Big shout out to `@Frax` and `@Phi` from the [FastHTML Discord channel](https://discord.com/channels/689892369998676007/1296050761414742127) for helping me out with that. Streaming from Modal was really easy. I just made used of `yield` [here](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/colpali/multi_modal_rag.py#L78-82) and `remote_gen` [here](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/colpali/main.py#L84).\n",
    "\n",
    "![](imgs/modal_volumes1.png)\n",
    "\n",
    "There is a folder for each PDF processed (for images and embeddings).\n",
    "\n",
    "![](imgs/modal_volumes2.png)\n",
    "\n",
    "![](imgs/modal_volumes3.png)\n",
    "\n",
    "Each image for each page is stored in the volume like this:\n",
    "![](imgs/modal_volumes4.png)\n",
    "\n",
    "And all the document embeddings are stored in Pickle format in a file called `embeddings.pkl`.\n",
    "![](imgs/modal_volumes5.png)\n",
    "\n",
    "Since I am only allowing to ask questions about a single PDF at a time, there is no need for fancy vector DBs etc.\n",
    "The embeddings for a specific PDF are cached and can be loaded into memory very quickly when needed.\n",
    "When a new PDF comes along that is not cached, we process it, and then store the images and embeddings in the volume.\n",
    "You can see all the details about PDF processing and ColPali inference in the [PDFRetriever class](https://github.com/DrChrisLevy/DrChrisLevy.github.io/blob/main/posts/colpali/pdf_retriever.py).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0821f02",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e545913",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Resources\n",
    "\n",
    "In no particular order:\n",
    "\n",
    "- [Colpali paper](https://arxiv.org/pdf/2407.01449v2)\n",
    "- [Colbert paper](https://arxiv.org/pdf/2004.12832)\n",
    "- [Colbert V2 paper](https://arxiv.org/pdf/2112.01488)\n",
    "- [PaliGemma](https://arxiv.org/pdf/2407.07726)\n",
    "- [A little pooling goes a long way for multi-vector representations: Blog answer.ai](https://www.answer.ai/posts/colbert-pooling.html)\n",
    "    - [Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling, Paper](https://arxiv.org/pdf/2409.14683)\n",
    "- [PLAID paper](https://arxiv.org/pdf/2205.09707)\n",
    "- [Beyond the Basics of Retrieval for Augmenting Generation (w/ Ben Clavié), Youtube Talk](https://www.youtube.com/watch?v=0nA5QG3087g)\n",
    "- [RAG is more than dense embedding, Google Slides, Ben Clavié](https://docs.google.com/presentation/d/1Zczs5Sk3FsCO06ZLDznqkOOhbTe96PwJa4_7FwyMBrA/edit#slide=id.p)\n",
    "- The quick start in the README [Original ColPali Repo](https://github.com/illuin-tech/colpali) as well as the sample [inference code](https://github.com/illuin-tech/colpali/blob/main/scripts/infer/run_inference_with_python.py)\n",
    "- [Hugging Face Model Cards](https://huggingface.co/vidore/colpali-v1.2)\n",
    "- [The Future of Search: Vision Models and the Rise of Multi-Model Retrieval](https://mcplusa.com/the-future-of-search-vision-models-and-the-rise-of-multi-model-retrieval/)\n",
    "- [Scaling ColPali to billions of PDFs with Vespa](https://blog.vespa.ai/scaling-colpali-to-billions/)\n",
    "- [Beyond Text: The Rise of Vision-Driven Document Retrieval for RAG](https://blog.vespa.ai/the-rise-of-vision-driven-document-retrieval-for-rag/)\n",
    "- [Vision Language Models Explained](https://huggingface.co/blog/vlms)\n",
    "- [Document Similarity Search with ColPali](https://huggingface.co/blog/fsommers/document-similarity-colpali)\n",
    "- [Jo Kristian Bergum: X](https://x.com/jobergum)\n",
    "- [Manuel Faysse: X](https://x.com/ManuelFaysse)\n",
    "- [Tony Wu: X](https://x.com/tonywu_71)\n",
    "- [Omar Khattab: X](https://x.com/lateinteraction?lang=en)\n",
    "- [fastHTML](https://about.fastht.ml/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9db69f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
