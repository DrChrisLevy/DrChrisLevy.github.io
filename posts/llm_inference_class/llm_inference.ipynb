{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6cebaaabc77c5364",
   "metadata": {
    "collapsed": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: OpenAI Compatible LLM Inference\n",
    "subtitle: A Single Inference Wrapper for OpenAI, Together AI, Hugging Face Inference TGI, Ollama, etc.\n",
    "author: Chris Levy\n",
    "date: '2024-03-08'\n",
    "date-modified: '2024-03-08'\n",
    "image: llm_inference.png\n",
    "toc: true\n",
    "description: In this blog post, I explore OpenAI-compatible LLM inference, demonstrating how to use the OpenAI Python client with various services like OpenAI, Together AI, Hugging Face Inference Endpoints, and Ollama, and I also introduce the instructor library for structured output, providing a unified approach to interact with different LLMs.\n",
    "tags:\n",
    "  - OpenAI\n",
    "  - TogetherAI\n",
    "  - huggingface\n",
    "  - ollama\n",
    "  - instructor\n",
    "format:\n",
    "  html: \n",
    "    code-fold: show\n",
    "    page-layout: full\n",
    "include-in-header:\n",
    "  - text: |\n",
    "      <style>\n",
    "      .cell-output-stdout code {\n",
    "        word-break: break-wor !important;\n",
    "        white-space: pre-wrap !important;\n",
    "      }\n",
    "      </style>\n",
    "bibliography: ../../bibliography.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5846853ac7bbda76",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Until recently I thought that the `openai` library was only for connecting to OpenAI endpoints. It was not until I was testing out LLM inference with [together.ai](https://www.together.ai/) that I came across a section in their documentation on [OpenAI API compatibility](https://docs.together.ai/docs/openai-api-compatibility). The idea of using the `openai` client to do inference with open source models was completely new to me. In the together.ai documentation example they use the `openai` library to connect to an open source model.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import openai\n",
    "\n",
    "system_content = \"You are a travel agent. Be descriptive and helpful.\"\n",
    "user_content = \"Tell me about San Francisco\"\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n",
    "    base_url=\"https://api.together.xyz/v1\",\n",
    "    )\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "response = chat_completion.choices[0].message.content\n",
    "print(\"Together response:\\n\", response)\n",
    "```\n",
    "\n",
    "Then a week later I saw that Hugging Face had also released support for [OpenAI compatibility with Text Generation Inference (TGI) and Inference Endpoints](https://huggingface.co/blog/tgi-messages-api). Again, you simply modify the `base_url`, `api_key`, and `model` as seen is this example from their blog post announcement.\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "# initialize the client but point it to TGI\n",
    "client = OpenAI(\n",
    "    base_url=\"<ENDPOINT_URL>\" + \"/v1/\",  # replace with your endpoint url\n",
    "    api_key=\"<HF_API_TOKEN>\",  # replace with your token\n",
    ")\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"tgi\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Why is open-source software important?\"},\n",
    "    ],\n",
    "    stream=True,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# iterate and print stream\n",
    "for message in chat_completion:\n",
    "    print(message.choices[0].delta.content, end=\"\")\n",
    "\n",
    "```\n",
    "\n",
    "What about working with LLMs locally? Two such options are [Ollama](https://ollama.com/) and  [LM Studio](https://lmstudio.ai/).  [Ollama](https://ollama.com/blog/openai-compatibility) recently added support for  the `openai` client\n",
    "and LM Studio supports it too. For example, here is how one can use `mistral-7b` locally with Ollama to run inference with the\n",
    "`openai` client:\n",
    "\n",
    "```\n",
    "ollama pull mistral\n",
    "```\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"mistral\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant and always talk like a pirate.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a haiku.\"},\n",
    "  ])\n",
    "print(response.choices[0].message.content)\n",
    "```\n",
    "\n",
    "There are other services and libraries for running LLM inference that are compatible with  the `openai` library too. I find it all very exciting because it is less code I have to write and maintain for running inference with LLMs. All I need to change is a `base_url`, an `api_key`, and the name of the `model`.\n",
    "\n",
    "At the same time that I was learning about `openai` client compatibility, I was also looking into the [instructor](https://github.com/jxnl/instructor) library. Since it patches in some additional functionality into the `openai` client, I thought it would be fun to discuss here too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8897c8aacb4e36",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ENV Setup\n",
    "\n",
    "Start by creating a virtual environment: \n",
    "\n",
    "```\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "```\n",
    "\n",
    "Then install:\n",
    "\n",
    "```\n",
    "pip install openai\n",
    "pip install instructor # only if you want to try out instructor library\n",
    "pip install python-dotenv # or define your environment variables differently\n",
    "```\n",
    "\n",
    "I also have:\n",
    "\n",
    "- an [OpenAI account](https://platform.openai.com/api-keys) with an API key.\n",
    "- a [together.ai account](https://api.together.xyz/settings/api-keys) with an API key.\n",
    "- Hugging Face Account, Access Token, and created [inference endpoint](https://huggingface.co/inference-endpoints/dedicated)\n",
    "- [installed Ollama](https://ollama.com/download) and `ollama pull gemma:2b-instruct` and `ollama pull llama2`\n",
    "\n",
    "In my `.env` file I have the following:\n",
    "```\n",
    "OPENAI_API_KEY=your_key\n",
    "HUGGING_FACE_ACCESS_TOKEN=your_key\n",
    "TOGETHER_API_KEY=your_key\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df37c9456aacee9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T05:28:41.505990Z",
     "start_time": "2024-03-08T05:28:41.490307Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# | output: false\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bbaa9d96c2153a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# LLM Inference Class\n",
    "\n",
    "You could go ahead and just start using `client.chat.completions.create` directly as in the examples from the introduction.\n",
    "However, I do like wrapping third party services into classes for reusability, maintainability, etc.\n",
    "\n",
    "The class below, `OpenAIChatCompletion`, does several things:\n",
    "\n",
    " - manages the different client connections in the `clients` dict\n",
    " - exposes `client.chat.completions.create` in the `__call__` method\n",
    " - provides functionality for making multiple calls in parallel. I know alternatively that one could use the `AsyncOpenAI` client, but sometimes I prefer simply using\n",
    "`futures.ThreadPoolExecutor` as seen in the function `create_chat_completions_async`.\n",
    " - patches the `OpenAI` client with the instructor library. If you don't want to play around with instructor library then simply remove the `instructor.patch` code.\n",
    "\n",
    "I also added some logging functionality which keeps track of every outgoing LLM request. This was inspired by the awesome blog post by Hamel Husain, [Fuck You, Show Me The Prompt.](https://hamel.dev/blog/posts/prompt/). In that post, Hamel writes about how various LLM tools can often hide the prompts, making it tricky to see what requests are actually sent to the LLM behind the scenes. I created a simple logger class `OpenAIMessagesLogger` which keeps track of all the requests sent to the `openai` client.  Later when we try out the instructor library for getting structured output, we will utilize this debugging logger to see some additional messages that were sent to the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c5c7b941c5c43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T05:28:45.022046Z",
     "start_time": "2024-03-08T05:28:44.693939Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import logging\n",
    "import re\n",
    "from concurrent import futures\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import instructor\n",
    "from openai import APITimeoutError, OpenAI\n",
    "from openai._streaming import Stream\n",
    "from openai.types.chat.chat_completion import ChatCompletion\n",
    "from openai.types.chat.chat_completion_chunk import ChatCompletionChunk\n",
    "\n",
    "\n",
    "class OpenAIChatCompletion:\n",
    "    clients: Dict = dict()\n",
    "\n",
    "    @classmethod\n",
    "    def _load_client(cls, base_url: Optional[str] = None, api_key: Optional[str] = None) -> OpenAI:\n",
    "        client_key = (base_url, api_key)\n",
    "        if OpenAIChatCompletion.clients.get(client_key) is None:\n",
    "            OpenAIChatCompletion.clients[client_key] = instructor.patch(OpenAI(base_url=base_url, api_key=api_key))\n",
    "        return OpenAIChatCompletion.clients[client_key]\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        model: str,\n",
    "        messages: list,\n",
    "        base_url: Optional[str] = None,\n",
    "        api_key: Optional[str] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Union[ChatCompletion, Stream[ChatCompletionChunk]]:\n",
    "        # https://platform.openai.com/docs/api-reference/chat/create\n",
    "        # https://github.com/openai/openai-python\n",
    "        client = self._load_client(base_url, api_key)\n",
    "        return client.chat.completions.create(model=model, messages=messages, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def create_chat_completions_async(\n",
    "        cls, task_args_list: List[Dict], concurrency: int = 10\n",
    "    ) -> List[Union[ChatCompletion, Stream[ChatCompletionChunk]]]:\n",
    "        \"\"\"\n",
    "        Make a series of calls to chat.completions.create endpoint in parallel and collect back\n",
    "        the results.\n",
    "        :param task_args_list: A list of dictionaries where each dictionary contains the keyword\n",
    "            arguments required for __call__ method.\n",
    "        :param concurrency: the max number of workers\n",
    "        \"\"\"\n",
    "\n",
    "        def create_chat_task(\n",
    "            task_args: Dict,\n",
    "        ) -> Union[None, ChatCompletion, Stream[ChatCompletionChunk]]:\n",
    "            try:\n",
    "                return cls().__call__(**task_args)\n",
    "            except APITimeoutError:\n",
    "                return None\n",
    "\n",
    "        with futures.ThreadPoolExecutor(max_workers=concurrency) as executor:\n",
    "            results = list(executor.map(create_chat_task, task_args_list))\n",
    "        return results\n",
    "\n",
    "\n",
    "class OpenAIMessagesLogger(logging.Handler):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.log_messages = []\n",
    "\n",
    "    def emit(self, record):\n",
    "        # Append the log message to the list\n",
    "        log_record_str = self.format(record)\n",
    "        match = re.search(r\"Request options: (.+)\", log_record_str, re.DOTALL)\n",
    "        if match:\n",
    "            text = match[1].replace(\"\\n\", \"\")\n",
    "            log_obj = ast.literal_eval(text)\n",
    "            self.log_messages.append(log_obj)\n",
    "\n",
    "\n",
    "def debug_messages():\n",
    "    msg = OpenAIMessagesLogger()\n",
    "    openai_logger = logging.getLogger(\"openai\")\n",
    "    openai_logger.setLevel(logging.DEBUG)\n",
    "    openai_logger.addHandler(msg)\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1988707e9830c2a7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here is how you use the inference class to call the LLM. If you have ever used the `openai` client you will be familiar with the input and output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b11b917b0e15430b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T05:28:47.516282Z",
     "start_time": "2024-03-08T05:28:46.919940Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-90N4hSh3AG1Sz68zjUnfcEtAjvFn5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1709875727, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_2b778c6b35', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAIChatCompletion()\n",
    "message_logger = debug_messages()  # optional for keeping track of all outgoing requests\n",
    "print(llm(model=\"gpt-3.5-turbo-0125\", messages=[dict(role=\"user\", content=\"Hello!\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae84f4d64b4a69e0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And our logger is keeping track of all the outgoing requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ade58c78494bdf39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T05:28:48.727831Z",
     "start_time": "2024-03-08T05:28:48.714274Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'method': 'post',\n",
       "  'url': '/chat/completions',\n",
       "  'files': None,\n",
       "  'json_data': {'messages': [{'role': 'user', 'content': 'Hello!'}],\n",
       "   'model': 'gpt-3.5-turbo-0125'}}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_logger.log_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fbca562dae01f0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we can define some different models that can all be accessed through the same inference class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e7c602c665686d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T05:28:57.484996Z",
     "start_time": "2024-03-08T05:28:57.469470Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Models:\n",
    "    # OpenAI GPT Models\n",
    "    GPT4 = dict(model=\"gpt-4-0125-preview\", base_url=None, api_key=None)\n",
    "    GPT3 = dict(model=\"gpt-3.5-turbo-0125\", base_url=None, api_key=None)\n",
    "    # Hugging Face Inference Endpoints\n",
    "    OPENHERMES2_5_MISTRAL_7B = dict(\n",
    "        model=\"tgi\",\n",
    "        base_url=\"https://xofunqxk66baupmf.us-east-1.aws.endpoints.huggingface.cloud\" + \"/v1/\",\n",
    "        api_key=os.environ[\"HUGGING_FACE_ACCESS_TOKEN\"],\n",
    "    )\n",
    "    # Ollama Models\n",
    "    LLAMA2 = dict(\n",
    "        model=\"llama2\",\n",
    "        base_url=\"http://localhost:11434/v1\",\n",
    "        api_key=\"ollama\",\n",
    "    )\n",
    "    GEMMA2B = dict(\n",
    "        model=\"gemma:2b-instruct\",\n",
    "        base_url=\"http://localhost:11434/v1\",\n",
    "        api_key=\"ollama\",\n",
    "    )\n",
    "    # together AI endpoints\n",
    "    GEMMA7B = dict(model=\"google/gemma-7b-it\", base_url=\"https://api.together.xyz/v1\", api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n",
    "    MISTRAL7B = dict(model=\"mistralai/Mistral-7B-Instruct-v0.1\", base_url=\"https://api.together.xyz/v1\", api_key=os.environ.get(\"TOGETHER_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d865a2b88b7d383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T05:28:58.088833Z",
     "start_time": "2024-03-08T05:28:58.082485Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_models = [(model_name, model_config) for model_name, model_config in Models.__dict__.items() if not model_name.startswith(\"__\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37eae23f2c788bfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T05:28:59.668374Z",
     "start_time": "2024-03-08T05:28:59.665399Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Your replies are short, brief and to the point.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who was the first person to walk on the Moon, and in what year did it happen?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae893a245fe7e432",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T05:29:14.939708Z",
     "start_time": "2024-03-08T05:29:01.682267Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GPT4\n",
      "Response: Neil Armstrong, 1969.\n",
      "Model: GPT3\n",
      "Response: The first person to walk on the Moon was Neil Armstrong in 1969.\n",
      "Model: OPENHERMES2_5_MISTRAL_7B\n",
      "Response: Neil Armstrong was the first person to walk on the Moon. It happened on July 20, 1969.\n",
      "Model: LLAMA2\n",
      "Response: The first person to walk on the Moon was Neil Armstrong, who stepped onto the lunar surface on July 20, 1969 as part of the Apollo 11 mission.\n",
      "Model: GEMMA2B\n",
      "Response: There is no evidence to support the claim that a person walked on the Moon in any year.\n",
      "Model: GEMMA7B\n",
      "Response: Sure, here is the answer:\n",
      "\n",
      "Neil Armstrong was the first person to walk on the Moon in 1969.\n",
      "Model: MISTRAL7B\n",
      "Response:  The first person to walk on the Moon was Neil Armstrong, and it happened on July 20, 1969.\n"
     ]
    }
   ],
   "source": [
    "for model_name, model_config in all_models:\n",
    "    resp = llm(messages=messages, **model_config)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Response: {resp.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131acedd0dc70bbd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can also send the same requests in parallel like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44cca2ef0f460418",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T05:29:26.302016Z",
     "start_time": "2024-03-08T05:29:18.966198Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GPT4\n",
      "Response: Neil Armstrong, 1969.\n",
      "Model: GPT3\n",
      "Response: The first person to walk on the Moon was Neil Armstrong in 1969.\n",
      "Model: OPENHERMES2_5_MISTRAL_7B\n",
      "Response: The first person to walk on the Moon was Neil Armstrong, and it happened in 1969.\n",
      "Model: LLAMA2\n",
      "Response: Nice question! The first person to walk on the Moon was Neil Armstrong, and it happened in 1969 during the Apollo 11 mission. Armstrong stepped onto the lunar surface on July 20, 1969, famously declaring \"That's one small step for man, one giant leap for mankind\" as he took his first steps.\n",
      "Model: GEMMA2B\n",
      "Response: There is no evidence or record of any person walking on the Moon.\n",
      "Model: GEMMA7B\n",
      "Response: Sure, here is the answer:\n",
      "\n",
      "Neil Armstrong was the first person to walk on the Moon in 1969.\n",
      "Model: MISTRAL7B\n",
      "Response:  The first person to walk on the Moon was Neil Armstrong, and it happened on July 20, 1969.\n"
     ]
    }
   ],
   "source": [
    "task_args_list = []\n",
    "for model_name, model_config in all_models:\n",
    "    task_args_list.append(dict(messages=messages, **model_config))\n",
    "\n",
    "# execute the same calls in parallel\n",
    "model_names = [m[0] for m in all_models]\n",
    "resps = llm.create_chat_completions_async(task_args_list)\n",
    "for model_name, resp in zip(model_names, resps):\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Response: {resp.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "193f6070fdbcdb2b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "I love that! The ability to use various models (open source and OpenAI GPT) all through the same interface.\n",
    "And we have all our outgoing requests logged for debugging if needed. We have made 15 requests up to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e941ba3e76e4200",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T05:29:26.305166Z",
     "start_time": "2024-03-08T05:29:26.299508Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert len(message_logger.log_messages) == 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dd2d113558a6a22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T05:29:26.307405Z",
     "start_time": "2024-03-08T05:29:26.304495Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'post',\n",
       " 'url': '/chat/completions',\n",
       " 'files': None,\n",
       " 'json_data': {'messages': [{'role': 'system',\n",
       "    'content': 'You are a helpful assistant. Your replies are short, brief and to the point.'},\n",
       "   {'role': 'user',\n",
       "    'content': 'Who was the first person to walk on the Moon, and in what year did it happen?'}],\n",
       "  'model': 'mistralai/Mistral-7B-Instruct-v0.1'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_logger.log_messages[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f90d9f01f81299a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Structured Output\n",
    "\n",
    "There are various approaches to getting structured output from LLMs. For example see [JSON mode](https://platform.openai.com/docs/guides/text-generation/json-mode) and [Function calling](https://platform.openai.com/docs/guides/function-calling). Some open source models and inference providers are also starting to offer these capabilities. For example see the [together.ai docs](https://docs.together.ai/docs/json-mode). The [instructor blog](https://jxnl.github.io/instructor/blog/#advanced-topics) also has lots of examples and tips for getting structured output from LLMs. See this recent [blog post](https://jxnl.github.io/instructor/blog/2024/03/07/open-source-local-structured-output-pydantic-json-openai/?h=together) for getting structured output from open source and Local LLMs.\n",
    "\n",
    "One thing that is neat about the instructor library is you can define a Pydantic schema and then pass it to the patched `openai` client.\n",
    "It also adds in schema validation and retry logic. \n",
    "\n",
    "First we will clear out our debugging log messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f013dd91feaf8f51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T05:35:44.035916Z",
     "start_time": "2024-03-08T05:35:44.031495Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "message_logger.log_messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c3ebf113d1e6656",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T05:35:44.330062Z",
     "start_time": "2024-03-08T05:35:44.323313Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from pydantic import BaseModel, field_validator\n",
    "\n",
    "\n",
    "class Character(BaseModel):\n",
    "    name: str\n",
    "    race: str\n",
    "    fun_fact: str\n",
    "    favorite_food: str\n",
    "    skills: List[str]\n",
    "    weapons: List[str]\n",
    "\n",
    "\n",
    "class Characters(BaseModel):\n",
    "    characters: List[Character]\n",
    "\n",
    "    @field_validator(\"characters\")\n",
    "    @classmethod\n",
    "    def validate_characters(cls, v):\n",
    "        if len(v) < 20:\n",
    "            raise ValueError(f\"The number of characters must be at least 20, but it is {len(v)}\")\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ed2e224f75efa17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T05:37:08.895531Z",
     "start_time": "2024-03-08T05:35:45.747427Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = llm(\n",
    "    messages=[dict(role=\"user\", content=\"Who are the main characters from Lord of the Rings?.\")],\n",
    "    response_model=Characters,\n",
    "    max_retries=4,\n",
    "    **Models.GPT4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10ad842226e1d1b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T05:37:08.920878Z",
     "start_time": "2024-03-08T05:37:08.898677Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Frodo Baggins\n",
      "race: Hobbit\n",
      "fun_fact: Bearer of the One Ring\n",
      "favorite_food: Mushrooms\n",
      "skills: ['Courage', 'Stealth']\n",
      "weapons: ['Sting', 'Elven Dagger']\n",
      "\n",
      "name: Samwise Gamgee\n",
      "race: Hobbit\n",
      "fun_fact: Frodo's gardener and friend\n",
      "favorite_food: Potatoes\n",
      "skills: ['Loyalty', 'Cooking']\n",
      "weapons: ['Barrow-blade']\n",
      "\n",
      "name: Gandalf\n",
      "race: Maia\n",
      "fun_fact: Known as Gandalf the Grey and later as Gandalf the White\n",
      "favorite_food: N/A\n",
      "skills: ['Wisdom', 'Magic']\n",
      "weapons: ['Glamdring', 'Staff']\n",
      "\n",
      "name: Aragorn\n",
      "race: Human\n",
      "fun_fact: Heir of Isildur and rightful king of Gondor\n",
      "favorite_food: Elvish waybread\n",
      "skills: ['Swordsmanship', 'Leadership']\n",
      "weapons: ['Andúril', 'Bow']\n",
      "\n",
      "name: Legolas\n",
      "race: Elf\n",
      "fun_fact: Prince of the Woodland Realm\n",
      "favorite_food: Lembas bread\n",
      "skills: ['Archery', 'Agility']\n",
      "weapons: ['Elven bow', 'Daggers']\n",
      "\n",
      "name: Gimli\n",
      "race: Dwarf\n",
      "fun_fact: Son of Glóin\n",
      "favorite_food: Meat\n",
      "skills: ['Axe fighting', 'Stout-heartedness']\n",
      "weapons: ['Battle axe', 'Throwing axes']\n",
      "\n",
      "name: Boromir\n",
      "race: Human\n",
      "fun_fact: Son of Denethor, Steward of Gondor\n",
      "favorite_food: Stew\n",
      "skills: ['Swordsmanship', 'Leadership']\n",
      "weapons: ['Sword', 'Shield']\n",
      "\n",
      "name: Meriadoc Brandybuck\n",
      "race: Hobbit\n",
      "fun_fact: Member of the Fellowship\n",
      "favorite_food: Ale\n",
      "skills: ['Stealth', 'Strategy']\n",
      "weapons: ['Elven dagger']\n",
      "\n",
      "name: Peregrin Took\n",
      "race: Hobbit\n",
      "fun_fact: Often known simply as Pippin\n",
      "favorite_food: Cakes\n",
      "skills: ['Curiosity', 'Bravery']\n",
      "weapons: ['Sword']\n",
      "\n",
      "name: Galadriel\n",
      "race: Elf\n",
      "fun_fact: Lady of Lothlórien\n",
      "favorite_food: N/A\n",
      "skills: ['Wisdom', 'Telepathy']\n",
      "weapons: ['Nenya (Ring of Power)']\n",
      "\n",
      "name: Elrond\n",
      "race: Elf\n",
      "fun_fact: Lord of Rivendell\n",
      "favorite_food: N/A\n",
      "skills: ['Wisdom', 'Healing']\n",
      "weapons: ['Sword']\n",
      "\n",
      "name: Eowyn\n",
      "race: Human\n",
      "fun_fact: Niece of King Théoden of Rohan; slayer of the Witch-king\n",
      "favorite_food: Bread\n",
      "skills: ['Swordsmanship', 'Courage']\n",
      "weapons: ['Sword', 'Shield']\n",
      "\n",
      "name: Faramir\n",
      "race: Human\n",
      "fun_fact: Brother of Boromir\n",
      "favorite_food: Bread\n",
      "skills: ['Archery', 'Strategy']\n",
      "weapons: ['Bow', 'Sword']\n",
      "\n",
      "name: Gollum\n",
      "race: Hobbit-like creature\n",
      "fun_fact: Once the bearer of the One Ring, known as Sméagol\n",
      "favorite_food: Raw fish\n",
      "skills: ['Stealth', 'Persuasion']\n",
      "weapons: ['Teeth and claws']\n",
      "\n",
      "name: Saruman\n",
      "race: Maia\n",
      "fun_fact: Head of the White Council before being corrupted\n",
      "favorite_food: N/A\n",
      "skills: ['Magic', 'Persuasion']\n",
      "weapons: ['Staff']\n",
      "\n",
      "name: Sauron\n",
      "race: Maia\n",
      "fun_fact: The Dark Lord and creator of the One Ring\n",
      "favorite_food: N/A\n",
      "skills: ['Necromancy', 'Deception']\n",
      "weapons: ['One Ring', 'Mace']\n",
      "\n",
      "name: Bilbo Baggins\n",
      "race: Hobbit\n",
      "fun_fact: Original discoverer of the One Ring\n",
      "favorite_food: Everything\n",
      "skills: ['Stealth', 'Story-telling']\n",
      "weapons: ['Sting']\n",
      "\n",
      "name: Théoden\n",
      "race: Human\n",
      "fun_fact: King of Rohan\n",
      "favorite_food: Meat\n",
      "skills: ['Leadership', 'Horsemanship']\n",
      "weapons: ['Herugrim', 'Sword']\n",
      "\n",
      "name: Treebeard\n",
      "race: Ent\n",
      "fun_fact: Oldest of the Ents, protectors of Fangorn Forest\n",
      "favorite_food: Water\n",
      "skills: ['Strength', 'Wisdom']\n",
      "weapons: ['None']\n",
      "\n",
      "name: Witch-king of Angmar\n",
      "race: Undead/Nazgûl\n",
      "fun_fact: Leader of the Nazgûl\n",
      "favorite_food: N/A\n",
      "skills: ['Fear-induction', 'Swordsmanship']\n",
      "weapons: ['Morgul-blade', 'Flail']\n",
      "\n",
      "name: Gríma Wormtongue\n",
      "race: Human\n",
      "fun_fact: Advisor to King Théoden under Saruman's influence\n",
      "favorite_food: N/A\n",
      "skills: ['Deception', 'Speechcraft']\n",
      "weapons: ['Knife']\n",
      "\n",
      "name: Éomer\n",
      "race: Human\n",
      "fun_fact: Nephew of King Théoden; later king of Rohan\n",
      "favorite_food: Meat\n",
      "skills: ['Swordsmanship', 'Horsemanship']\n",
      "weapons: ['Sword', 'Spear']\n"
     ]
    }
   ],
   "source": [
    "for character in res.characters:\n",
    "    for k, v in character.model_dump().items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0653be1bc9d61ae",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It is probably likely that GPT would not return 20 characters in the first request.\n",
    "If `max_retries=0` then it would likely raise a Pydantic validation error.\n",
    "But since we have `max_retries=4` then the `instructor` library sends back\n",
    "the validation error as a message and asks again. How exactly does it do that?\n",
    "We can look at the messages that we have logged for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d50b5d2e7afcf4d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T05:37:08.958175Z",
     "start_time": "2024-03-08T05:37:08.919344Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(message_logger.log_messages) > 1\n",
    "len(message_logger.log_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "205594ada666a833",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T05:37:08.958834Z",
     "start_time": "2024-03-08T05:37:08.925280Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'method': 'post',\n",
       "  'url': '/chat/completions',\n",
       "  'files': None,\n",
       "  'json_data': {'messages': [{'role': 'user',\n",
       "     'content': 'Who are the main characters from Lord of the Rings?.'}],\n",
       "   'model': 'gpt-4-0125-preview',\n",
       "   'tool_choice': {'type': 'function', 'function': {'name': 'Characters'}},\n",
       "   'tools': [{'type': 'function',\n",
       "     'function': {'name': 'Characters',\n",
       "      'description': 'Correctly extracted `Characters` with all the required parameters with correct types',\n",
       "      'parameters': {'$defs': {'Character': {'properties': {'name': {'title': 'Name',\n",
       "           'type': 'string'},\n",
       "          'race': {'title': 'Race', 'type': 'string'},\n",
       "          'fun_fact': {'title': 'Fun Fact', 'type': 'string'},\n",
       "          'favorite_food': {'title': 'Favorite Food', 'type': 'string'},\n",
       "          'skills': {'items': {'type': 'string'},\n",
       "           'title': 'Skills',\n",
       "           'type': 'array'},\n",
       "          'weapons': {'items': {'type': 'string'},\n",
       "           'title': 'Weapons',\n",
       "           'type': 'array'}},\n",
       "         'required': ['name',\n",
       "          'race',\n",
       "          'fun_fact',\n",
       "          'favorite_food',\n",
       "          'skills',\n",
       "          'weapons'],\n",
       "         'title': 'Character',\n",
       "         'type': 'object'}},\n",
       "       'properties': {'characters': {'items': {'$ref': '#/$defs/Character'},\n",
       "         'title': 'Characters',\n",
       "         'type': 'array'}},\n",
       "       'required': ['characters'],\n",
       "       'type': 'object'}}}]}},\n",
       " {'method': 'post',\n",
       "  'url': '/chat/completions',\n",
       "  'files': None,\n",
       "  'json_data': {'messages': [{'role': 'user',\n",
       "     'content': 'Who are the main characters from Lord of the Rings?.'},\n",
       "    {'role': 'assistant',\n",
       "     'content': '',\n",
       "     'tool_calls': [{'id': 'call_kjUg9ogoR1OdRr0OkmTzabue',\n",
       "       'function': {'arguments': '{\"characters\":[{\"name\":\"Frodo Baggins\",\"race\":\"Hobbit\",\"fun_fact\":\"Bearer of the One Ring\",\"favorite_food\":\"Mushrooms\",\"skills\":[\"Courage\",\"Stealth\"],\"weapons\":[\"Sting\",\"Elven Dagger\"]},{\"name\":\"Samwise Gamgee\",\"race\":\"Hobbit\",\"fun_fact\":\"Frodo\\'s gardener and friend\",\"favorite_food\":\"Potatoes\",\"skills\":[\"Loyalty\",\"Cooking\"],\"weapons\":[\"Barrow-blade\"]},{\"name\":\"Gandalf\",\"race\":\"Maia\",\"fun_fact\":\"Known as Gandalf the Grey and later as Gandalf the White\",\"favorite_food\":\"N/A\",\"skills\":[\"Wisdom\",\"Magic\"],\"weapons\":[\"Glamdring\",\"Staff\"]},{\"name\":\"Aragorn\",\"race\":\"Human\",\"fun_fact\":\"Heir of Isildur and rightful king of Gondor\",\"favorite_food\":\"Elvish waybread\",\"skills\":[\"Swordsmanship\",\"Leadership\"],\"weapons\":[\"Andúril\",\"Bow\"]},{\"name\":\"Legolas\",\"race\":\"Elf\",\"fun_fact\":\"Prince of the Woodland Realm\",\"favorite_food\":\"Lembas bread\",\"skills\":[\"Archery\",\"Agility\"],\"weapons\":[\"Elven bow\",\"Daggers\"]},{\"name\":\"Gimli\",\"race\":\"Dwarf\",\"fun_fact\":\"Son of Glóin\",\"favorite_food\":\"Meat\",\"skills\":[\"Axe fighting\",\"Stout-heartedness\"],\"weapons\":[\"Battle axe\",\"Throwing axes\"]}]}',\n",
       "        'name': 'Characters'},\n",
       "       'type': 'function'}]},\n",
       "    {'role': 'tool',\n",
       "     'tool_call_id': 'call_kjUg9ogoR1OdRr0OkmTzabue',\n",
       "     'name': 'Characters',\n",
       "     'content': \"Recall the function correctly, fix the errors and exceptions found\\n1 validation error for Characters\\ncharacters\\n  Value error, The number of characters must be at least 20, but it is 6 [type=value_error, input_value=[{'name': 'Frodo Baggins'...axe', 'Throwing axes']}], input_type=list]\\n    For further information visit https://errors.pydantic.dev/2.6/v/value_error\"}],\n",
       "   'model': 'gpt-4-0125-preview',\n",
       "   'tool_choice': {'type': 'function', 'function': {'name': 'Characters'}},\n",
       "   'tools': [{'type': 'function',\n",
       "     'function': {'name': 'Characters',\n",
       "      'description': 'Correctly extracted `Characters` with all the required parameters with correct types',\n",
       "      'parameters': {'$defs': {'Character': {'properties': {'name': {'title': 'Name',\n",
       "           'type': 'string'},\n",
       "          'race': {'title': 'Race', 'type': 'string'},\n",
       "          'fun_fact': {'title': 'Fun Fact', 'type': 'string'},\n",
       "          'favorite_food': {'title': 'Favorite Food', 'type': 'string'},\n",
       "          'skills': {'items': {'type': 'string'},\n",
       "           'title': 'Skills',\n",
       "           'type': 'array'},\n",
       "          'weapons': {'items': {'type': 'string'},\n",
       "           'title': 'Weapons',\n",
       "           'type': 'array'}},\n",
       "         'required': ['name',\n",
       "          'race',\n",
       "          'fun_fact',\n",
       "          'favorite_food',\n",
       "          'skills',\n",
       "          'weapons'],\n",
       "         'title': 'Character',\n",
       "         'type': 'object'}},\n",
       "       'properties': {'characters': {'items': {'$ref': '#/$defs/Character'},\n",
       "         'title': 'Characters',\n",
       "         'type': 'array'}},\n",
       "       'required': ['characters'],\n",
       "       'type': 'object'}}}]}}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_logger.log_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728f996beef661f8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If you look through the above messages carefully you can see the retry asking logic.\n",
    "\n",
    "*Recall the function correctly, fix the errors and exceptions found\\n1 validation error for Characters\\ncharacters\\n  Value error, The number of characters must be at least 20, ...*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df52cc07845fa36",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "You can even use the structured output with some of the open source models. I would refer to the [instructor blog](https://jxnl.github.io/instructor/blog/2024/03/07/open-source-local-structured-output-pydantic-json-openai/) or documentation for further information on that. I have not fully looked into the different [patching](https://jxnl.github.io/instructor/concepts/patching/) modes yet. But here is a simple example of using `MISTRAL7B` through together.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46924fbf1cc01b9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T05:37:11.475833Z",
     "start_time": "2024-03-08T05:37:08.932295Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Superman', 'race': 'Kryptonian', 'fun_fact': 'Can fly', 'favorite_food': 'Pizza', 'skills': ['Super strength', 'Flight', 'Heat vision', 'X-ray vision'], 'weapons': ['Laser vision', 'Heat vision', 'X-ray vision']}\n"
     ]
    }
   ],
   "source": [
    "res = llm(\n",
    "    messages=[dict(role=\"user\", content=\"Give me a character from a movie or book.\")],\n",
    "    response_model=Character,\n",
    "    max_retries=2,\n",
    "    **Models.MISTRAL7B,\n",
    ")\n",
    "print(res.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4018cbd3dd1c73d8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "Again, I really like the idea of using a single interface for interacting with multiple LLMs.\n",
    "I hope the space continues to mature so that more open source models and services support JSON mode and function calling.\n",
    "I think instructor is a cool library and the corresponding blog is interesting too. I also like the idea of logging all the outgoing prompts/messages just to make sure I fully understand what is happening under the hood."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
