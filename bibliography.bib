@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017},
  url = {https://www.oreilly.com/library/view/natural-language-processing/9781098136789/}
}

@book{tunstall2022natural,
  title={Natural language processing with transformers},
  author={Tunstall, Lewis and Von Werra, Leandro and Wolf, Thomas},
  year={2022},
  publisher={" O'Reilly Media, Inc."},
  url = {https://www.oreilly.com/library/view/natural-language-processing/9781098136789/}
}


@misc{karpathy_youtube_2023_gpt,
  author = {Karpathy, Andrej},
  title = {Let's build GPT: from scratch, in code, spelled out},
  year = {2023},
  howpublished = {YouTube},
  url = {https://www.youtube.com/watch?v=kCc8FmEb1nY},
}


@misc{OmarSansevieroBlogRandomTransformer,
  author = {Omar Sanseviero},
  title = {The Random Transformer},
  year = {2024},
  url = {https://osanseviero.github.io/hackerllama/blog/posts/random_transformer/},
}

@misc{TheIllustratedTransformerGlob,
  author = {Jay Alammar},
  title = {The Illustrated Transformer},
  year = {2018},
  url = {https://jalammar.github.io/illustrated-transformer/},
}

@misc{SebastianRaschkaUnderstandingAttention,
  author = {Sebastian Raschka},
  title = {Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs},
  year = {2024},
  url = {https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention},
}

@article{eldan2023tinystories,
  title={TinyStories: How Small Can Language Models Be and Still Speak Coherent English?},
  author={Eldan, Ronen and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2305.07759},
  year={2023},
  url = {https://arxiv.org/abs/2305.07759},
}