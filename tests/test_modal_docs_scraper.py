from scripts.modal_docs_scraper import scrape_modal_docs


def test_scrape_modal_docs_live():
    # Test with a specific URL that should be stable

    markdown = scrape_modal_docs("https://modal.com/docs/examples/hello_world")
    assert (
        markdown
        == '# Hello, world!\n\nThis tutorial demonstrates some core features of Modal: \n\n- You can run functions on Modal just as easily as you run them locally.\n\n- Running functions in parallel on Modal is simple and fast.\n- Logs and errors show up immediately, even for functions running on Modal.\n\n## Importing Modal and setting up\n\nWe start by importing `modal`and creating a `App`.\nWe build up this `App`to [define our application](https://modal.com/docs/guide/apps). \n\n[ [! ]\n```\nimport sys\n\nimport modal\n\napp = modal.App("example-hello-world")\n```\n] [ [ [ ] [ Copy ] ] ] \n\n## Defining a function\n\nModal takes code and runs it in the cloud. \n\nSo first we’ve got to write some code. \n\nLet’s write a simple function that takes in an input,\nprints a log or an error to the console,\nand then returns an output. \n\nTo make this function work with Modal, we just wrap it in a decorator, [@app.function](https://modal.com/docs/reference/modal.App# function). \n\n[ [! ]\n```\n@app.function()\ndef f(i):\n if i % 2 == 0:\n print("hello", i)\n else:\n print("world", i, file=sys.stderr)\n\n return i * i\n```\n] [ [ [ ] [ Copy ] ] ] \n\n## Running our function locally, remotely, and in parallel\n\nNow let’s see three different ways we can call that function: \n\n1. As a regular call on your `local`machine, with `f.local`\n2. As a `remote`call that runs in the cloud, with `f.remote`\n3. By `map`ping many copies of `f`in the cloud over many inputs, with `f.map`\n\nWe call `f`in each of these ways inside the `main`function below. \n\n[ [! ]\n```\n@app.local_entrypoint()\ndef main():\n # run the function locally\n print(f.local(1000))\n\n # run the function remotely on Modal\n print(f.remote(1000))\n\n # run the function in parallel and remotely on Modal\n total = 0\n for ret in f.map(range(200)):\n total += ret\n\n print(total)\n```\n] [ [ [ ] [ Copy ] ] ] \n\nEnter `modal run hello_world.py`in a shell, and you’ll see a Modal app initialize.\nYou’ll then see the `print`ed logs of\nthe `main`function and, mixed in with them, all the logs of `f`as it is run\nlocally, then remotely, and then remotely and in parallel. \n\nThat’s all triggered by adding the [@app.local_entrypoint](https://modal.com/docs/reference/modal.App# local_entrypoint)decorator on `main`, which defines it as the function to start from locally when we invoke `modal run`. \n\n## What just happened?\n\nWhen we called `.remote`on `f`, the function was executed *in the cloud *, on Modal’s infrastructure, not on the local machine. \n\nIn short, we took the function `f`, put it inside a container,\nsent it the inputs, and streamed back the logs and outputs. \n\n## But why does this matter?\n\nTry one of these things next to start seeing the full power of Modal! \n\n### You can change the code and run it again\n\nFor instance, change the `print`statement in the function `f`to print `"spam"`and `"eggs"`instead and run the app again.\nYou’ll see that that your new code is run with no extra work from you —\nand it should even run faster! \n\nModal’s goal is to make running code in the cloud feel like you’re\nrunning code locally. That means no waiting for long image builds when you’ve just moved a comma,\nno fiddling with container image pushes, and no context-switching to a web UI to inspect logs. \n\n### You can map over more data\n\nChange the `map`range from `200`to some large number, like `1170`. You’ll see\nModal create and run even more containers in parallel this time. \n\nAnd it’ll happen lightning fast! \n\n### You can run a more interesting function\n\nThe function `f`is a bit silly and doesn’t do much, but in its place\nimagine something that matters to you, like: \n\n- Running [language model inference](https://modal.com/docs/examples/vllm_inference)or [fine-tuning](https://modal.com/docs/examples/slack-finetune)\n\n- Manipulating [audio](https://modal.com/docs/examples/musicgen)or [images](https://modal.com/docs/examples/diffusers_lora_finetune)\n- [Embedding huge text datasets](https://modal.com/docs/examples/amazon_embeddings)at lightning fast speeds\n\nModal lets you parallelize that operation effortlessly by running hundreds or\nthousands of containers in the cloud.'
    )

    markdown = scrape_modal_docs("https://modal.com/docs/guide/gpu")
    assert (
        markdown
        == '# GPU acceleration\n\nModal makes it easy to run your code on [GPUs](https://modal.com/gpu-glossary/readme). \n\n## Quickstart\n\nHere’s a simple example of a Function running on an A100 in Modal: \n\n[ [! ]\n```\nimport modal\n\nimage = modal.Image.debian_slim().pip_install("torch")\napp = modal.App(image=image)\n\n@app.function(gpu="A100")\ndef run():\n import torch\n\n assert torch.cuda.is_available()\n```\n] [ [ [ ] [ Copy ] ] ] \n\n## Specifying GPU type\n\nYou can pick a specific GPU type for your Function via the `gpu`argument.\nModal supports the following values for this parameter: \n\n- `T4`\n\n- `L4`\n- `A10`\n\n- `A100`\n- `A100-40GB`\n\n- `A100-80GB`\n- `L40S`\n\n- `H100`/ `H100!`\n- `H200`\n\n- `B200`\n\nFor instance, to use a B200, you can use `@app.function(gpu="B200")`. \n\nRefer to our [pricing page](https://modal.com/pricing)for the latest pricing on each GPU type. \n\n## Specifying GPU count\n\nYou can specify more than 1 GPU per container by appending `:n`to the GPU\nargument. For instance, to run a Function with eight H100s: \n\n[ [! ]\n```\n@app.function(gpu="H100:8")\ndef run_llama_405b_fp8():\n ...\n```\n] [ [ [ ] [ Copy ] ] ] \n\nCurrently B200, H200, H100, A100, L4, T4 and L40S instances support up to 8 GPUs (up to 1,536 GB GPU RAM),\nand A10 instances support up to 4 GPUs (up to 96 GB GPU RAM). Note that requesting\nmore than 2 GPUs per container will usually result in larger wait times. These\nGPUs are always attached to the same physical machine. \n\n## Picking a GPU\n\nFor running, rather than training, neural networks, we recommend starting off\nwith the [L40S](https://resources.nvidia.com/en-us-l40s/l40s-datasheet-28413),\nwhich offers an excellent trade-off of cost and performance and 48 GB of GPU\nRAM for storing model weights and activations. \n\nFor more on how to pick a GPU for use with neural networks like LLaMA or Stable\nDiffusion, and for tips on how to make that GPU go brrr, check out [Tim Dettemers’ blog post](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/)or the [Full Stack Deep Learning page on Cloud GPUs](https://fullstackdeeplearning.com/cloud-gpus/). \n\n## B200 GPUs\n\nModal’s most powerful GPUs are the [B200s](https://www.nvidia.com/en-us/data-center/dgx-b200/),\nNVIDIA’s flagship data center chip for the Blackwell [architecture](https://modal.com/gpu-glossary/device-hardware/streaming-multiprocessor-architecture). \n\nTo request a B200, set the `gpu`argument to `"B200"`\n\n[ [! ]\n```\n@app.function(gpu="B200:8")\ndef run_deepseek():\n ...\n```\n] [ [ [ ] [ Copy ] ] ] \n\nCheck out [this example](https://modal.com/docs/examples/vllm_inference)to see how you can use B200s to max out vLLM serving performance for LLaMA 3.1-8B. \n\nBefore you jump for the most powerful (and so most expensive) GPU, make sure you\nunderstand where the bottlenecks are in your computations. For example, running\nlanguage models with small batch sizes (e.g. one prompt at a time) results in a [bottleneck on memory, not arithmetic](https://kipp.ly/transformer-inference-arithmetic/).\nSince arithmetic throughput has risen faster than memory throughput in recent\nhardware generations, speedups for memory-bound GPU jobs are not as extreme and\nmay not be worth the extra cost. \n\n## H200 and H100 GPUs\n\n[H200s](https://www.nvidia.com/en-us/data-center/h200/)and [H100s](https://www.nvidia.com/en-us/data-center/h100/)are the previous\ngeneration of top-of-the-line data center chips from NVIDIA, based on the Hopper [architecture](https://modal.com/gpu-glossary/device-hardware/streaming-multiprocessor-architecture).\nThese GPUs have better software support than do Blackwell GPUs (e.g. popular libraries include pre-compiled kernels for Hopper, but not Blackwell),\nand they often get the job done at a competitive cost, so they are a common choice of accelerator, on and off Modal. \n\nAll H100 GPUs on the Modal platform are of the SXM variant, as can be verified by examining the [power draw](https://modal.com/docs/guide/gpu-metrics)in the dashboard or with `nvidia-smi`. \n\n### Automatic upgrades to H200s\n\nModal may automatically upgrade a `gpu="H100"`request to run on an H200.\nThis automatic upgrade does *not *change the cost of the GPU. \n\nKernels [compatible](https://modal.com/gpu-glossary/device-software/compute-capability)with H200s are also compatible with H100s,\nso your code will still run, just faster, so long as it doesn’t make strict assumptions about memory capacity.\nAn H200’s [HBM3e memory](https://modal.com/gpu-glossary/device-hardware/gpu-ram)has a capacity of 141 GB and a bandwidth of 4.8TB/s, 1.75x larger and 1.4x faster than an NVIDIA H100 with HBM3. \n\nIn cases where an automatic upgrade to H200 would not be helpful (for instance, benchmarking) you can pass `gpu=H100!`to avoid it. \n\n## A100 GPUs\n\n[A100s](https://www.nvidia.com/en-us/data-center/a100/)are based on NVIDIA’s Ampere [architecture](https://modal.com/gpu-glossary/device-hardware/streaming-multiprocessor-architecture).\nModal offers two versions of the A100: one with 40 GB of RAM and another with 80 GB of RAM. \n\nTo request an A100 with 40 GB of [GPU memory](https://modal.com/gpu-glossary/device-hardware/gpu-ram), use `gpu="A100"`: \n\n[ [! ]\n```\n@app.function(gpu="A100")\ndef qwen_7b():\n ...\n```\n] [ [ [ ] [ Copy ] ] ] \n\nModal may automatically upgrade a `gpu="A100"`request to run on an 80 GB A100.\nThis automatic upgrade does *not *change the cost of the GPU. \n\nYou can specifically request a 40GB A100 with the string `A100-40GB`.\nTo specifically request an 80 GB A100, use the string `A100-80GB`: \n\n[ [! ]\n```\n@app.function(gpu="A100-80GB")\ndef llama_70b_fp8():\n ...\n```\n] [ [ [ ] [ Copy ] ] ] \n\n## GPU fallbacks\n\nModal allows specifying a list of possible GPU types, suitable for Functions that are\ncompatible with multiple options. Modal respects the ordering of this list and\nwill try to allocate the most preferred GPU type before falling back to less\npreferred ones. \n\n[ [! ]\n```\n@app.function(gpu=["H100", "A100-40GB:2"])\ndef run_on_80gb():\n ...\n```\n] [ [ [ ] [ Copy ] ] ] \n\nSee [this example](https://modal.com/docs/examples/gpu_fallbacks)for more detail. \n\n## Multi GPU training\n\nModal currently supports multi-GPU training on a single node, with multi-node training in closed beta ( [contact us](https://modal.com/slack)for access).\nDepending on which framework you are using, you may need to use different techniques to train on multiple GPUs. \n\nIf the framework re-executes the entrypoint of the Python process (like [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/index.html)) you need to either set the strategy to `ddp_spawn`or `ddp_notebook`if you wish to invoke the training directly. Another option is to run the training script as a subprocess instead. \n\n[ [! ]\n```\n@app.function(gpu="A100:2")\ndef run():\n import subprocess\n import sys\n subprocess.run(\n ["python", "train.py"],\n stdout=sys.stdout, stderr=sys.stderr,\n check=True,\n )\n```\n] [ [ [ ] [ Copy ] ] ] \n\n## Examples and more resources\n\nFor more information about GPUs in general, check out our [GPU Glossary](https://modal.com/gpu-glossary/readme). \n\nOr take a look some examples of Modal apps using GPUs: \n\n- [Fine-tune a character LoRA for your pet](https://modal.com/docs/examples/dreambooth_app)\n\n- [Fast LLM inference with vLLM](https://modal.com/docs/examples/vllm_inference)\n- [Stable Diffusion with a CLI, API, and web UI](https://modal.com/docs/examples/stable_diffusion_cli)\n\n- [Rendering Blender videos](https://modal.com/docs/examples/blender_video)'
    )

    markdown = scrape_modal_docs("https://modal.com/docs/guide/apps")
    assert (
        markdown
        == '# Apps, Functions, and entrypoints\n\nAn `App`is the object that represents an application running on Modal.\nAll functions and classes are associated with an [App](https://modal.com/docs/reference/modal.App# modalapp). \n\nWhen you [run](https://modal.com/docs/reference/cli/run)or [deploy](https://modal.com/docs/reference/cli/deploy)an `App`, it creates an ephemeral or a\ndeployed `App`, respectively. \n\nYou can view a list of all currently running Apps on the [apps](https://modal.com/apps)page. \n\n## Ephemeral Apps\n\nAn ephemeral App is created when you use the [modal run](https://modal.com/docs/reference/cli/run)CLI command, or the [app.run](https://modal.com/docs/reference/modal.App# run)method. This creates a temporary\nApp that only exists for the duration of your script. \n\nEphemeral Apps are stopped automatically when the calling program exits, or when\nthe server detects that the client is no longer connected.\nYou can use [--detach](https://modal.com/docs/reference/cli/run)in order to keep an ephemeral App running even\nafter the client exits. \n\nBy using `app.run`you can run your Modal apps from within your Python scripts: \n\n[ [! ]\n```\ndef main():\n ...\n with app.run():\n some_modal_function.remote()\n```\n] [ [ [ ] [ Copy ] ] ] \n\nBy default, running your app in this way won’t propagate Modal logs and progress bar messages. To enable output, use the [modal.enable_output](https://modal.com/docs/reference/modal.enable_output)context manager: \n\n[ [! ]\n```\ndef main():\n ...\n with modal.enable_output():\n with app.run():\n some_modal_function.remote()\n```\n] [ [ [ ] [ Copy ] ] ] \n\n## Deployed Apps\n\nA deployed App is created using the [modal deploy](https://modal.com/docs/reference/cli/deploy)CLI command. The App is persisted indefinitely until you delete it via the [web UI](https://modal.com/apps). Functions in a deployed App that have an attached [schedule](https://modal.com/docs/guide/cron)will be run on a schedule. Otherwise, you can\ninvoke them manually using [web endpoints or Python](https://modal.com/docs/guide/trigger-deployed-functions). \n\nDeployed Apps are named via the [App](https://modal.com/docs/reference/modal.App# modalapp)constructor. Re-deploying an existing `App`(based on the name) will update it\nin place. \n\n## Entrypoints for ephemeral Apps\n\nThe code that runs first when you `modal run`an App is called the “entrypoint”. \n\nYou can register a local entrypoint using the [@app.local_entrypoint()](https://modal.com/docs/reference/modal.App# local_entrypoint)decorator. You can also use a regular Modal function as an entrypoint, in which\ncase only the code in global scope is executed locally. \n\n### Argument parsing\n\nIf your entrypoint function takes arguments with primitive types, `modal run`automatically parses them as CLI options. For example, the following function\ncan be called with `modal run script.py --foo 1 --bar "hello"`: \n\n[ [! ]\n```\n# script.py\n\n@app.local_entrypoint()\ndef main(foo: int, bar: str):\n some_modal_function.remote(foo, bar)\n```\n] [ [ [ ] [ Copy ] ] ] \n\nIf you wish to use your own argument parsing library, such as `argparse`, you can instead accept a variable-length argument list for your entrypoint or your function. In this case, Modal skips CLI parsing and forwards CLI arguments as a tuple of strings. For example, the following function can be invoked with `modal run my_file.py --foo=42 --bar="baz"`: \n\n[ [! ]\n```\nimport argparse\n\n@app.function()\ndef train(*arglist):\n parser = argparse.ArgumentParser()\n parser.add_argument("--foo", type=int)\n parser.add_argument("--bar", type=str)\n args = parser.parse_args(args = arglist)\n```\n] [ [ [ ] [ Copy ] ] ] \n\n### Manually specifying an entrypoint\n\nIf there is only one `local_entrypoint`registered, [modal run script.py](https://modal.com/docs/reference/cli/run)will automatically use it. If\nyou have no entrypoint specified, and just one decorated Modal function, that\nwill be used as a remote entrypoint instead. Otherwise, you can direct `modal run`to use a specific entrypoint. \n\nFor example, if you have a function decorated with [@app.function()](https://modal.com/docs/reference/modal.App# function)in your file: \n\n[ [! ]\n```\n# script.py\n\n@app.function()\ndef f():\n print("Hello world!")\n\n@app.function()\ndef g():\n print("Goodbye world!")\n\n@app.local_entrypoint()\ndef main():\n f.remote()\n```\n] [ [ [ ] [ Copy ] ] ] \n\nRunning [modal run script.py](https://modal.com/docs/reference/cli/run)will execute the `main`function locally, which would call the `f`function remotely. However you can\ninstead run `modal run script.py::app.f`or `modal run script.py::app.g`to\nexecute `f`or `g`directly. \n\n## Apps were once Stubs\n\nThe `modal.App`class in the client was previously called `modal.Stub`. The\nold name was kept as an alias for some time, but from Modal 1.0.0 onwards,\nusing `modal.Stub`will result in an error.'
    )

    markdown = scrape_modal_docs(
        "https://modal.com/docs/reference/cli/container#modal-container-list"
    )
    assert (
        markdown
        == "# modal container\n\nManage and connect to running containers. \n\n**Usage **: \n\n[ [! ]\n```\nmodal container [OPTIONS] COMMAND [ARGS]...\n```\n] [ [ [ ] [ Copy ] ] ] \n\n**Options **: \n\n- `--help`: Show this message and exit.\n\n**Commands **: \n\n- `list`: List all containers that are currently running.\n\n- `logs`: Show logs for a specific container, streaming while active.\n- `exec`: Execute a command in a container.\n\n- `stop`: Stop a currently-running container and reassign its in-progress inputs.\n\n## modal container list\n\nList all containers that are currently running. \n\n**Usage **: \n\n[ [! ]\n```\nmodal container list [OPTIONS]\n```\n] [ [ [ ] [ Copy ] ] ] \n\n**Options **: \n\n- `-e, --env TEXT`: Environment to interact with.\n\nIf not specified, Modal will use the default environment of your current profile, or the `MODAL_ENVIRONMENT`variable.\nOtherwise, raises an error if the workspace has multiple environments. \n\n- `--json / --no-json`: [default: no-json]\n\n- `--help`: Show this message and exit.\n\n## modal container logs\n\nShow logs for a specific container, streaming while active. \n\n**Usage **: \n\n[ [! ]\n```\nmodal container logs [OPTIONS] CONTAINER_ID\n```\n] [ [ [ ] [ Copy ] ] ] \n\n**Arguments **: \n\n- `CONTAINER_ID`: Container ID [required]\n\n**Options **: \n\n- `--help`: Show this message and exit.\n\n## modal container exec\n\nExecute a command in a container. \n\n**Usage **: \n\n[ [! ]\n```\nmodal container exec [OPTIONS] CONTAINER_ID COMMAND...\n```\n] [ [ [ ] [ Copy ] ] ] \n\n**Arguments **: \n\n- `CONTAINER_ID`: Container ID [required]\n\n- `COMMAND...`: A command to run inside the container.\n\nTo pass command-line flags or options, add `--`before the start of your commands. For example: `modal container exec <id> -- /bin/bash -c 'echo hi'`[required] \n\n**Options **: \n\n- `--pty / --no-pty`: Run the command using a PTY.\n\n- `--help`: Show this message and exit.\n\n## modal container stop\n\nStop a currently-running container and reassign its in-progress inputs. \n\nThis will send the container a SIGINT signal that Modal will handle. \n\n**Usage **: \n\n[ [! ]\n```\nmodal container stop [OPTIONS] CONTAINER_ID\n```\n] [ [ [ ] [ Copy ] ] ] \n\n**Arguments **: \n\n- `CONTAINER_ID`: Container ID [required]\n\n**Options **: \n\n- `--help`: Show this message and exit."
    )

    markdown = scrape_modal_docs("https://modal.com/docs/guide/sandbox")
    assert (
        markdown
        == '# Sandboxes\n\nIn addition to the Function interface, Modal has a direct\ninterface for defining containers *at runtime *and securely running arbitrary code\ninside them. \n\nThis can be useful if, for example, you want to: \n\n- Execute code generated by a language model.\n\n- Create isolated environments for running untrusted code.\n- Check out a git repository and run a command against it, like a test suite, or `npm lint`.\n\n- Run containers with arbitrary dependencies and setup scripts.\n\nEach individual job is called a **Sandbox **and can be created using the [Sandbox.create](https://modal.com/docs/reference/modal.Sandbox# create)constructor: \n\n[ [! ]\n```\nimport modal\n\napp = modal.App.lookup("my-app", create_if_missing=True)\n\nsb = modal.Sandbox.create(app=app)\n\np = sb.exec("python", "-c", "print(\'hello\')", timeout=3)\nprint(p.stdout.read())\n\np = sb.exec("bash", "-c", "for i in {1..10}; do date +%T; sleep 0.5; done", timeout=5)\nfor line in p.stdout:\n # Avoid double newlines by using end="".\n print(line, end="")\n\nsb.terminate()\n```\n] [ [ [ ] [ Copy ] ] ] \n\n**Note: **you can run the above example as a script directly with `python my_script.py`. `modal run`is not needed here since there is no [entrypoint](https://modal.com/docs/guide/apps# entrypoints-for-ephemeral-apps). \n\nSandboxes require an [App](https://modal.com/docs/guide/apps)to be passed when spawned from outside\nof a Modal container. You may pass in a regular `App`object or look one up by name with [App.lookup](https://modal.com/docs/reference/modal.App# lookup). The `create_if_missing`flag on `App.lookup`will create an `App`with the given name if it doesn’t exist. \n\n## Running a Sandbox with an entrypoint\n\nIn most cases, Sandboxes are treated as a generic container that can run arbitrary\ncommands. However, in some cases, you may want to run a single command or script\nas the entrypoint of the Sandbox. You can do this by passing string arguments to the\nSandbox constructor: \n\n[ [! ]\n```\nsb = modal.Sandbox.create("python", "-m", "http.server", "8080", app=my_app, timeout=10)\nfor line in sb.stdout:\n print(line, end="")\n```\n] [ [ [ ] [ Copy ] ] ] \n\nThis functionality is most useful for running long-lived services that you want\nto keep running in the background. See our [Jupyter notebook example](https://modal.com/docs/examples/jupyter_sandbox)for a more concrete example of this. \n\n## Referencing Sandboxes from other code\n\nIf you have a running Sandbox, you can retrieve it using the [Sandbox.from_id](https://modal.com/docs/reference/modal.Sandbox# from_id)method. \n\n[ [! ]\n```\nsb = modal.Sandbox.create(app=my_app)\nsb_id = sb.object_id\n\n# ... later in the program ...\n\nsb2 = modal.Sandbox.from_id(sb_id)\n\np = sb2.exec("echo", "hello")\nprint(p.stdout.read())\nsb2.terminate()\n```\n] [ [ [ ] [ Copy ] ] ] \n\nA common use case for this is keeping a pool of Sandboxes available for executing tasks\nas they come in. You can keep a list of `object_id`s of Sandboxes that are “open” and\nreuse them, closing over the `object_id`in whatever function is using them. \n\n## Parameters\n\nSandboxes support nearly all configuration options found in regular `modal.Function`s.\nRefer to [Sandbox.create](https://modal.com/docs/reference/modal.Sandbox# create)for further documentation\non Sandbox parametrization. \n\nFor example, Images and Volumes can be used just as with functions: \n\n[ [! ]\n```\nsb = modal.Sandbox.create(\n image=modal.Image.debian_slim().pip_install("pandas"),\n volumes={"/data": modal.Volume.from_name("my-volume")},\n workdir="/repo",\n app=my_app,\n)\n```\n] [ [ [ ] [ Copy ] ] ] \n\n### Timeouts\n\nSandboxes have a default timeout of 5 minutes. You can increase this by setting the `timeout`parameter. \n\n[ [! ]\n```\nsb = modal.Sandbox.create(app=my_app, timeout=600) # 10 minutes\n```\n] [ [ [ ] [ Copy ] ] ] \n\nIf you need a to pause or extend a Sandbox timeout, we recommend using [Filesystem Snapshots](https://modal.com/docs/guide/sandbox-snapshots)to preserve and restore container state. \n\n### Using custom images\n\nSandboxes support custom images just as Functions do. However, while you’ll typically\ninvoke a Modal Function with the `modal run`cli, you typically spawn a Sandbox\nwith a simple `python`call. As such, you need to manually enable output streaming\nto see your image build logs: \n\n[ [! ]\n```\nimage = modal.Image.debian_slim().pip_install("pandas", "numpy")\n\nwith modal.enable_output():\n sb = modal.Sandbox.create(image=image, app=my_app)\n```\n] [ [ [ ] [ Copy ] ] ] \n\n### Dynamically defined environments\n\nNote that any valid `Image`or `Mount`can be used with a Sandbox, even if those\nimages or mounts have not previously been defined. This also means that Images and\nMounts can be built from requirements at **runtime **. For example, you could\nuse a language model to write some code and define your image, and then spawn a\nSandbox with it. Check out [devlooper](https://github.com/modal-labs/devlooper)for a concrete example of this. \n\n### Environment variables\n\nYou can set environment variables using inline secrets: \n\n[ [! ]\n```\nsecret = modal.Secret.from_dict({"MY_SECRET": "hello"})\n\nsb = modal.Sandbox.create(\n secrets=[secret],\n app=my_app,\n)\np = sb.exec("bash", "-c", "echo $MY_SECRET")\nprint(p.stdout.read())\n```\n] [ [ [ ] [ Copy ] ] ] \n\n### Verbose logging\n\nYou can see Sandbox execution logs using `verbose=True`. For example: \n\n[ [! ]\n```\nsb = modal.Sandbox.create(app=my_app, verbose=True)\n\np = sb.exec("python", "-c", "print(\'hello\')")\nprint(p.stdout.read())\n\nwith sb.open("test.txt", "w") as f:\n f.write("Hello World\\n")\n```\n] [ [ [ ] [ Copy ] ] ] \n\nshows Sandbox logs: \n\n[ [! ]\n```\nSandbox exec started: python -c print(\'hello\')\nOpened file \'test.txt\': fd-yErSQzGL9sig6WAjyNgTPR\nWrote to file: fd-yErSQzGL9sig6WAjyNgTPR\nClosed file: fd-yErSQzGL9sig6WAjyNgTPR\n```\n] [ [ [ ] [ Copy ] ] ] \n\n## Named Sandboxes\n\nYou can assign a name to a Sandbox when creating it. Each name must be unique within an app\n\n- only one *running *Sandbox can use a given name at a time. Note that the associated app must be\na deployed app. Once a Sandbox completely stops running, its name becomes available for reuse.\nSome applications find Sandbox Names to be useful for ensuring that no more than one Sandbox is\nrunning per resource or project. If a Sandbox with the given name is already running, `create()`will raise a `modal.exception.AlreadyExistsError`. \n\n[ [! ]\n```\nsb1 = modal.Sandbox.create(app=my_app, name="my-name")\n# this will raise a modal.exception.AlreadyExistsError\nsb2 = modal.Sandbox.create(app=my_app, name="my-name")\n```\n] [ [ [ ] [ Copy ] ] ] \n\nA named Sandbox may be fetched from a deployed app using `modal.Sandbox.from_name()`*but only\nif the Sandbox is currently running *. If no running Sandbox is found, `from_name()`will raise\na `modal.exception.NotFoundError`. \n\n[ [! ]\n```\nmy_app = modal.App.lookup("my-app", create_if_missing=True)\nsb1 = modal.Sandbox.create(app=my_app, name="my-name")\n# returns the currently running Sandbox with the name "my-name" from the\n# deployed app named "my-app".\nsb2 = modal.Sandbox.from_name("my-app", "my-name")\nassert sb1.object_id == sb2.object_id # sb1 and sb2 refer to the same Sandbox\n```\n] [ [ [ ] [ Copy ] ] ] \n\n## Tagging\n\nSandboxes can also be tagged with arbitrary key-value pairs. These tags can be used\nto filter results in [Sandbox.list](https://modal.com/docs/reference/modal.Sandbox# list). \n\n[ [! ]\n```\nsandbox_v1_1 = modal.Sandbox.create("sleep", "10", app=my_app)\nsandbox_v1_2 = modal.Sandbox.create("sleep", "20", app=my_app)\n\nsandbox_v1_1.set_tags({"major_version": "1", "minor_version": "1"})\nsandbox_v1_2.set_tags({"major_version": "1", "minor_version": "2"})\n\nfor sandbox in modal.Sandbox.list(app_id=my_app.app_id): # All sandboxes.\n print(sandbox.object_id)\n\nfor sandbox in modal.Sandbox.list(\n app_id=my_app.app_id,\n tags={"major_version": "1"},\n): # Also all sandboxes.\n print(sandbox.object_id)\n\nfor sandbox in modal.Sandbox.list(\n app_id=app.app_id,\n tags={"major_version": "1", "minor_version": "2"},\n): # Just the latest sandbox.\n print(sandbox.object_id)\n```\n] [ [ [ ] [ Copy ] ] ]'
    )
