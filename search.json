[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "PDF Q&A App using ColPali, Modal, and FastHTML\n\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2024\n\n\nChris Levy\n\n\n\n\n\n\n  \n\n\n\n\nüöÄ Building with Modal üöÄ\n\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2024\n\n\nChris Levy\n\n\n\n\n\n\n  \n\n\n\n\nLLM Tool Loops with OpenAI and Anthropic\n\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2024\n\n\nChris Levy\n\n\n\n\n\n\n  \n\n\n\n\nMemory Usage for Quantized LLMS\n\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2024\n\n\nChris Levy\n\n\n\n\n\n\n  \n\n\n\n\nFine-Tuning LLMs with Axolotl on JarvisLabs\n\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2024\n\n\nChris Levy\n\n\n\n\n\n\n  \n\n\n\n\nIntro to LLMs\n\n\nLunch and Learn Talk\n\n\n\n\n\n\n\n\n\nMay 1, 2024\n\n\nChris Levy\n\n\n\n\n\n\n  \n\n\n\n\nUsing Modal to Transcribe YouTube Videos with Whisper\n\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2024\n\n\nChris Levy\n\n\n\n\n\n\n  \n\n\n\n\nGetting Started with Axolotl for Fine-Tuning LLMs\n\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2024\n\n\nChris Levy\n\n\n\n\n\n\n  \n\n\n\n\nFunction Calling with Hermes-2-Pro-Mistral-7B\n\n\n\n\n\n\n\n\n\n\n\n\nMar 16, 2024\n\n\nChris Levy\n\n\n\n\n\n\n  \n\n\n\n\nOpenAI Compatible LLM Inference\n\n\nA Single Inference Wrapper for OpenAI, Together AI, Hugging Face Inference TGI, Ollama, etc.\n\n\n\n\n\n\n\n\n\nMar 8, 2024\n\n\nChris Levy\n\n\n\n\n\n\n  \n\n\n\n\nDSPy\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nChris Levy\n\n\n\n\n\n\n  \n\n\n\n\nBasic Transformer Architecture Notes\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2024\n\n\nChris Levy\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#nlp-through-the-years",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#nlp-through-the-years",
    "title": "Intro to LLMs",
    "section": "NLP Through The Years",
    "text": "NLP Through The Years\n\nELIZA (MIT),1964-1967, CS25 V4: Lecture 1 (Spring 2024)"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#word-embeddings",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#word-embeddings",
    "title": "Intro to LLMs",
    "section": "Word Embeddings",
    "text": "Word Embeddings\n\nrepresent each word as an embedding (vector of numbers)\nuseful computations such as distance (cosine/euclidean)\nmapping of words onto a semantic space\nexample: Word2Vec (2013), GloVe, BERT, ELMo"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#attention-and-transformers",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#attention-and-transformers",
    "title": "Intro to LLMs",
    "section": "Attention and Transformers",
    "text": "Attention and Transformers\n\nImage Source: nlp-with-transformers book"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#transformer-multi-head-attention",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#transformer-multi-head-attention",
    "title": "Intro to LLMs",
    "section": "Transformer & Multi-Head Attention",
    "text": "Transformer & Multi-Head Attention\n\nAttention Is All you Need: Paper"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#what-is-a-llm-large-language-model",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#what-is-a-llm-large-language-model",
    "title": "Intro to LLMs",
    "section": "What is a LLM (large language model)?",
    "text": "What is a LLM (large language model)?\n\nLLMs are scaled up versions of the Transformer architecture (millions/billions of parameters)\nMost modern LLMs are decoder only transformers\nTrained on massive amounts of ‚Äúgeneral‚Äù textual data\nTraining objective is typically ‚Äúnext token prediction‚Äù: P(Wt+1|Wt,Wt-1,‚Ä¶,W1)"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#next-token-prediction",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#next-token-prediction",
    "title": "Intro to LLMs",
    "section": "Next Token Prediction",
    "text": "Next Token Prediction\n\nLLMs are next token predictors\n‚ÄúIt is raining today, so I will take my _______.‚Äù"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#tokenization-with-tiktoken-library",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#tokenization-with-tiktoken-library",
    "title": "Intro to LLMs",
    "section": "Tokenization with tiktoken library",
    "text": "Tokenization with tiktoken library\n\nThe first step is to convert the input text into tokens\nEach token has an id in the vocabulary\n\n\nimport tiktoken\n\nenc = tiktoken.encoding_for_model(\"gpt-4-0125\")\nencoded_text = enc.encode(\"tiktoken is great!\")\nencoded_text\n\n[83, 1609, 5963, 374, 2294, 0]\n\n\n\n[enc.decode([token]) for token in encoded_text]\n\n['t', 'ik', 'token', ' is', ' great', '!']\n\n\n\nenc.decode([83, 1609, 5963, 374, 2294, 0])\n\n'tiktoken is great!'"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#tokenization-with-transformers-library",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#tokenization-with-transformers-library",
    "title": "Intro to LLMs",
    "section": "Tokenization with transformers library",
    "text": "Tokenization with transformers library\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ntexts = [\n    \"I love summer\",\n    \"I love tacos\",\n]\ninputs = tokenizer(\n    texts,\n    return_tensors=\"pt\",\n    padding=\"max_length\",\n    max_length=16,\n    truncation=True,\n).input_ids\nprint(inputs)\n\nprint(inputs.shape)  # (B, T)\nprint(tokenizer.vocab_size)\nfor row in inputs:\n    print(tokenizer.convert_ids_to_tokens(row))\n\ntensor([[  101,  1045,  2293,  2621,   102,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [  101,  1045,  2293, 11937, 13186,   102,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0]])\ntorch.Size([2, 16])\n30522\n['[CLS]', 'i', 'love', 'summer', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n['[CLS]', 'i', 'love', 'ta', '##cos', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#tokenization-is-the-first-step",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#tokenization-is-the-first-step",
    "title": "Intro to LLMs",
    "section": "Tokenization is the First Step",
    "text": "Tokenization is the First Step"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#llms-are-not-great-at-math.-why",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#llms-are-not-great-at-math.-why",
    "title": "Intro to LLMs",
    "section": "LLMS are not great at math. Why?",
    "text": "LLMS are not great at math. Why?\n\nbecause of tokenization and next token prediction\n\nWhat is the average of: 2009 1746 4824 8439\n\n\nencoded_text = enc.encode(\"What is the average of:  2009 1746 4824 8439\")\nprint(encoded_text)\n\n[3923, 374, 279, 5578, 315, 25, 220, 220, 1049, 24, 220, 11771, 21, 220, 21984, 19, 220, 23996, 24]\n\n\n\nprint([enc.decode([token]) for token in encoded_text])\n\n['What', ' is', ' the', ' average', ' of', ':', ' ', ' ', '200', '9', ' ', '174', '6', ' ', '482', '4', ' ', '843', '9']"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#basic-transformer-architecture---futher-reading",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#basic-transformer-architecture---futher-reading",
    "title": "Intro to LLMs",
    "section": "Basic Transformer Architecture - Futher Reading",
    "text": "Basic Transformer Architecture - Futher Reading\n\nLots of resources online\nSome of the ones I enjoyed while learning:\n\nChapter 3 of the book Natural Language Processing With Transformers\nAndrej Karpathy‚Äôs video Let‚Äôs build GPT: from scratch, in code, spelled out\nSebastian Raschka‚Äôs Blog Post Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs\nOmar Sanseviero‚Äôs Blog Post The Random Transformer\nThe Illustrated Transformer\nThe original paper: Attention Is All You Need"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#base-models-vs-instruct-models",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#base-models-vs-instruct-models",
    "title": "Intro to LLMs",
    "section": "Base Models VS Instruct Models",
    "text": "Base Models VS Instruct Models\n\nmeta-llama/Meta-Llama-3-8B (base model)"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#base-models-vs-instruct-models-1",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#base-models-vs-instruct-models-1",
    "title": "Intro to LLMs",
    "section": "Base Models VS Instruct Models",
    "text": "Base Models VS Instruct Models\n\nmeta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#popular-instruction-fine-tuned-llms",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#popular-instruction-fine-tuned-llms",
    "title": "Intro to LLMs",
    "section": "Popular Instruction Fine-Tuned LLMs",
    "text": "Popular Instruction Fine-Tuned LLMs\n\nclosed\n\nOpen AI: gpt-4-turbo-2024-04-09, gpt-3.5-turbo-0125, etc.\nAnthropic: opus, sonnet, haiku\nGoogle, Gemini 1.5\n\nopen\n\nMeta: Llama-3-8B-Instruct, Llama-3-70B-Instruct\nMistral: Mistral 7B, Mixtral 8x7B, Mixtral 8x22B\nQwen: Qwen-1.8B, Qwen-7B, Qwen-14B, Qwen-72B\nHuggingFace: Zephyr-ORPO-141b-A35b-v0.1\nDatabricks: DBRX-Instruct-Preview\nNousResearch: Hermes-2-Pro-Mistral-7B,\nCohere: Command R+"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#the-gap-is-closing",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#the-gap-is-closing",
    "title": "Intro to LLMs",
    "section": "The Gap is closing",
    "text": "The Gap is closing\n\nimage source - Maxime Labonne, üèÜ LMSYS Chatbot Arena\nanother fun animation"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#aligning-language-models",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#aligning-language-models",
    "title": "Intro to LLMs",
    "section": "Aligning language models",
    "text": "Aligning language models\n\nThere is so much theory/research behind creating instruction models\nNot going to cover that here\nCheckout this recent talk, Aligning open language models, from Nathan Lambert\nState of GPT Keynote By Andrej Karpathy\nLarge Language Model Course by Maxime Labonne"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#openai-compatible-llm-inference-1",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#openai-compatible-llm-inference-1",
    "title": "Intro to LLMs",
    "section": "OpenAI Compatible LLM Inference",
    "text": "OpenAI Compatible LLM Inference\n\nimport openai\n\nclient = openai.OpenAI()\nchat_completion = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-0125\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Who are the main characters from Lord of the Rings?.\"},\n    ],\n)\nresponse = chat_completion.choices[0].message.content\nprint(response)\n\nThe main characters from Lord of the Rings are Frodo Baggins, Samwise Gamgee, Aragorn, Legolas, Gimli, Gandalf, Boromir, Merry and Pippin, and Gollum."
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#openai-compatible-llm-inference-2",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#openai-compatible-llm-inference-2",
    "title": "Intro to LLMs",
    "section": "OpenAI Compatible LLM Inference",
    "text": "OpenAI Compatible LLM Inference\n\ntogether.ai\n\nimport openai\n\nclient = openai.OpenAI(api_key=os.environ.get(\"TOGETHER_API_KEY\"), base_url=\"https://api.together.xyz/v1\")\nchat_completion = client.chat.completions.create(\n    model=\"META-LLAMA/LLAMA-3-70B-CHAT-HF\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Who are the main characters from Lord of the Rings?.\"},\n    ],\n)\nresponse = chat_completion.choices[0].message.content\nprint(response)"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#openai-compatible-llm-inference-3",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#openai-compatible-llm-inference-3",
    "title": "Intro to LLMs",
    "section": "OpenAI Compatible LLM Inference",
    "text": "OpenAI Compatible LLM Inference\n\ntogether.ai\n\n\nimport openai\n\nclient = openai.OpenAI(api_key=os.environ.get(\"TOGETHER_API_KEY\"), base_url=\"https://api.together.xyz/v1\")\nchat_completion = client.chat.completions.create(\n    model=\"META-LLAMA/LLAMA-3-70B-CHAT-HF\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Who are the main characters from Lord of the Rings?.\"},\n    ],\n)\nresponse = chat_completion.choices[0].message.content\nprint(response)\n\nThe main characters from J.R.R. Tolkien's epic fantasy novel \"The Lord of the Rings\" are:\n\n1. **Frodo Baggins**: The hobbit who inherits the One Ring from Bilbo Baggins and undertakes the perilous journey to destroy it in the fires of Mount Doom.\n2. **Samwise Gamgee** (Sam): Frodo's loyal hobbit servant and friend, who accompanies him on his quest.\n3. **Aragorn (Strider)**: A human warrior who becomes the leader of the Fellowship of the Ring and helps guide Frodo on his journey. He is the rightful King of Gondor.\n4. **Legolas**: An elf archer who joins the Fellowship and provides skilled marksmanship and agility.\n5. **Gimli**: A dwarf warrior who joins the Fellowship and provides strength and combat skills.\n6. **Gandalf the Grey**: A powerful wizard who helps guide Frodo on his quest and provides wisdom and magical assistance.\n7. **Boromir**: A human warrior from the land of Gondor, who joins the Fellowship but ultimately tries to take the Ring from Frodo.\n8. **Merry Brandybuck** and **Pippin Took**: Frodo's hobbit cousins, who join the Fellowship and provide comic relief and bravery in the face of danger.\n9. **Sauron**: The primary antagonist, a dark lord who created the One Ring and seeks to conquer Middle-earth.\n10. **Saruman**: A wizard who betrays Gandalf and allies himself with Sauron, seeking to gain power and control over Middle-earth.\n\nThese characters form the core of the story, and their interactions and relationships drive the plot of \"The Lord of the Rings\"."
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#openai-compatible-llm-inference-4",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#openai-compatible-llm-inference-4",
    "title": "Intro to LLMs",
    "section": "OpenAI Compatible LLM Inference",
    "text": "OpenAI Compatible LLM Inference\n\nlocal inference with ollama\n\n\nimport openai\n\nclient = openai.OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\nchat_completion = client.chat.completions.create(\n    model=\"llama3\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Who are the main characters from Lord of the Rings?.\"},\n    ],\n)\nresponse = chat_completion.choices[0].message.content\nprint(response)\n\nThe main characters in J.R.R. Tolkien's \"Lord of the Rings\" trilogy, which includes \"The Fellowship of the Ring\", \"The Two Towers\", and \"The Return of the King\", are:\n\n1. Frodo Baggins: The hobbit who inherits the One Ring from Bilbo and sets out on a quest to destroy it in the fires of Mount Doom.\n2. Samwise Gamgee (Sam): Frodo's loyal hobbit servant and friend, who accompanies him on his journey to Mordor.\n3. Aragorn (Strider): A human warrior who leads the Fellowship and helps them navigate the perilous lands of Middle-earth.\n4. Legolas: An elf archer who joins the Fellowship and fights alongside them against Sauron's armies.\n5. Gimli: A dwarf warrior who also joins the Fellowship, seeking to avenge his father's death at the hands of orcs.\n6. Boromir: The human son of the Steward of Gondor, who tries to take the One Ring from Frodo for the benefit of his own people.\n7. Meriadoc Brandybuck (Merry) and Peregrin Took (Pippin): Two hobbit friends of Frodo's who accompany him on his journey and become embroiled in the quest to destroy the Ring.\n\nThese characters, along with Gandalf the Grey, a powerful wizard, and other supporting characters, drive the story and its themes of friendship, sacrifice, and the struggle against evil."
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#chat-templates",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#chat-templates",
    "title": "Intro to LLMs",
    "section": "Chat Templates",
    "text": "Chat Templates\n\nfrom transformers import AutoTokenizer\n\ncheckpoint = \"meta-llama/Meta-Llama-3-8B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n\nEach model has its own expected input format. For Llama3 it‚Äôs this:\n\n\"\"\"\n&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\nYou are a friendly chatbot who always responds in the style of a pirate&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\nHow many helicopters can a human eat in one sitting?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\"\"\"\n\nWith chat templates we can use this familiar standard:\n\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\ntokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\nprint(tokenizer.decode(tokenized_chat[0]))\n\n&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\nYou are a friendly chatbot who always responds in the style of a pirate&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\nHow many helicopters can a human eat in one sitting?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#structured-output-1",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#structured-output-1",
    "title": "Intro to LLMs",
    "section": "Structured Output",
    "text": "Structured Output\n\nimport openai\n\nclient = openai.OpenAI()\nchat_completion = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-0125\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Who are the main characters from Lord of the Rings?. \"\n            \"For each character give the name, race, \"\n            \"favorite food, skills, weapons, and a fun fact.\",\n        },\n    ],\n)\nresponse = chat_completion.choices[0].message.content\nprint(response)\n\n1. Frodo Baggins\n- Race: Hobbit\n- Favorite food: Mushrooms\n- Skills: Determination, stealth, resilience\n- Weapons: Sting (his sword)\n- Fun fact: Frodo is the only character to have directly interacted with the One Ring and survived its corrupting influence.\n\n2. Aragorn (also known as Strider)\n- Race: Human (Dunedain)\n- Favorite food: Lembas bread\n- Skills: Swordsmanship, tracking, leadership\n- Weapons: Anduril (his sword), bow and arrows\n- Fun fact: Aragorn is the heir to the throne of Gondor and the rightful King of Arnor.\n\n3. Gandalf\n- Race: Maia (wizard)\n- Favorite food: Pipe-weed\n- Skills: Magic, wisdom, leadership\n- Weapons: Glamdring (his sword), staff\n- Fun fact: Gandalf is actually one of the Maiar, a group of powerful beings who serve the Valar (gods) in the world of Middle-earth.\n\n4. Legolas\n- Race: Elf\n- Favorite food: Waybread (Lembas)\n- Skills: Archery, agility, keen eyesight\n- Weapons: Bow and arrows, knives\n- Fun fact: Legolas is the son of Thranduil, the Elven King of the Woodland Realm in Mirkwood.\n\n5. Gimli\n- Race: Dwarf\n- Favorite food: Roast meats\n- Skills: Axe-fighting, mining, loyalty\n- Weapons: Axe, throwing axes\n- Fun fact: Gimli is a member of the Fellowship representing the Dwarves, who are known for their craftsmanship and love of gold and jewels."
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#structured-output-2",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#structured-output-2",
    "title": "Intro to LLMs",
    "section": "Structured Output",
    "text": "Structured Output\n\nJSON mode and Function Calling give us structured output\ninstructor - library - ‚ÄúPydantic is all you need‚Äù\n\n\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\n# Define your desired output structure\nclass UserInfo(BaseModel):\n    name: str\n    age: int\n\n\n# Extract structured data from natural language\nuser_info = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-0125\",\n    response_model=UserInfo,\n    messages=[{\"role\": \"user\", \"content\": \"Chris is 38 years old.\"}],\n)\nprint(user_info.model_dump())\nprint(user_info.name)\nprint(user_info.age)\n\n{'name': 'Chris', 'age': 38}\nChris\n38"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#structured-output-3",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#structured-output-3",
    "title": "Intro to LLMs",
    "section": "Structured Output",
    "text": "Structured Output\n\nimport openai\nimport instructor\nfrom typing import List\n\nfrom pydantic import BaseModel, field_validator\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass Character(BaseModel):\n    name: str\n    race: str\n    fun_fact: str\n    favorite_food: str\n    skills: List[str]\n    weapons: List[str]\n\n\nclass Characters(BaseModel):\n    characters: List[Character]\n\n    @field_validator(\"characters\")\n    @classmethod\n    def validate_characters(cls, v):\n        if len(v) &lt; 10:\n            raise ValueError(f\"The number of characters must be at least 10, but it is {len(v)}\")\n        return v\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-0125\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Who are the main characters from Lord of the Rings?. \"\n            \"For each character give the name, race, \"\n            \"favorite food, skills, weapons, and a fun fact. Give me at least 10 different characters.\",\n        },\n    ],\n    response_model=Characters,\n    max_retries=4,\n)\n\nfrom pprint import pprint\n\npprint(response.model_dump())\n\n{'characters': [{'favorite_food': 'Mushrooms',\n                 'fun_fact': 'Frodo is the nephew of Bilbo Baggins.',\n                 'name': 'Frodo Baggins',\n                 'race': 'Hobbit',\n                 'skills': ['Ringbearer', 'Stealth', 'Courage'],\n                 'weapons': ['Sting', 'Phial of Galadriel']},\n                {'favorite_food': 'Lembas bread',\n                 'fun_fact': 'Aragorn is the rightful heir to the throne of '\n                             'Gondor.',\n                 'name': 'Aragorn',\n                 'race': 'Man',\n                 'skills': ['Swordsmanship', 'Leadership', 'Tracking'],\n                 'weapons': ['Anduril', 'Bow and Arrow']},\n                {'favorite_food': 'Roast Pork',\n                 'fun_fact': 'Gimli is the son of Gloin, one of the Dwarves in '\n                             \"'The Hobbit'.\",\n                 'name': 'Gimli',\n                 'race': 'Dwarf',\n                 'skills': ['Axe throwing', 'Smithing', 'Courage'],\n                 'weapons': ['Axe', 'Throwing Axe']},\n                {'favorite_food': 'Lembas bread',\n                 'fun_fact': 'Legolas has keen eyesight and can spot enemies '\n                             'from great distances.',\n                 'name': 'Legolas',\n                 'race': 'Elf',\n                 'skills': ['Archery', 'Agility', 'Sight'],\n                 'weapons': ['Bow', 'Arrow']},\n                {'favorite_food': 'Pipe-weed',\n                 'fun_fact': 'Gandalf is also known as Mithrandir in Elvish.',\n                 'name': 'Gandalf',\n                 'race': 'Maia',\n                 'skills': ['Wizardry', 'Wisdom', 'Combat'],\n                 'weapons': ['Glamdring', 'Staff']},\n                {'favorite_food': 'Venison',\n                 'fun_fact': 'Boromir hails from the realm of Gondor.',\n                 'name': 'Boromir',\n                 'race': 'Man',\n                 'skills': ['Swordsmanship', 'Leadership', 'Athletics'],\n                 'weapons': ['Sword', 'Shield']},\n                {'favorite_food': 'Potatoes',\n                 'fun_fact': 'Sam is known for his unwavering loyalty to '\n                             'Frodo.',\n                 'name': 'Samwise Gamgee',\n                 'race': 'Hobbit',\n                 'skills': ['Gardening', 'Loyalty', 'Cooking'],\n                 'weapons': ['Cooking pot', 'Gardening tools']},\n                {'favorite_food': 'Berry tarts',\n                 'fun_fact': 'Arwen is the daughter of Elrond, Lord of '\n                             'Rivendell.',\n                 'name': 'Arwen',\n                 'race': 'Half-Elf',\n                 'skills': ['Horseback riding', 'Healing', 'Sword fighting'],\n                 'weapons': ['Sword']},\n                {'favorite_food': 'Apple pie',\n                 'fun_fact': \"Merry is one of Frodo's close friends and part \"\n                             'of the Fellowship of the Ring.',\n                 'name': 'Merry Brandybuck',\n                 'race': 'Hobbit',\n                 'skills': ['Stealth', 'Swordsmanship', 'Cooking'],\n                 'weapons': ['Dagger', 'Sword']},\n                {'favorite_food': 'Mushrooms',\n                 'fun_fact': 'Pippin becomes a Knight of Gondor for his '\n                             'bravery in battle.',\n                 'name': 'Pippin Took',\n                 'race': 'Hobbit',\n                 'skills': ['Loyalty', 'Entertainment', 'Courage'],\n                 'weapons': ['Dagger', 'Sword']}]}"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#function-calling",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#function-calling",
    "title": "Intro to LLMs",
    "section": "Function Calling",
    "text": "Function Calling\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather_forecast\",\n            \"description\": \"Provides a weather forecast for a given location and date.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\"location\": {\"type\": \"string\"}, \"date\": {\"type\": \"string\"}},\n                \"required\": [\"location\", \"date\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"book_flight\",\n            \"description\": \"Book a flight.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"departure_city\": {\"type\": \"string\"},\n                    \"arrival_city\": {\"type\": \"string\"},\n                    \"departure_date\": {\"type\": \"string\"},\n                    \"return_date\": {\"type\": \"string\"},\n                    \"num_passengers\": {\"type\": \"integer\"},\n                    \"cabin_class\": {\"type\": \"string\"},\n                },\n                \"required\": [\n                    \"departure_city\",\n                    \"arrival_city\",\n                    \"departure_date\",\n                    \"return_date\",\n                    \"num_passengers\",\n                    \"cabin_class\",\n                ],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"send_slack_message\",\n            \"description\": \"Send a slack message to specific channel.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\"channel_name\": {\"type\": \"string\"}, \"message\": {\"type\": \"string\"}},\n                \"required\": [\"channel_name\", \"message\"],\n            },\n        },\n    },\n]\n\nimport openai\nfrom datetime import date\nimport json\n\nclient = openai.OpenAI()\nchat_completion = client.chat.completions.create(\n    model=\"gpt-4-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": f\"Today's date is {date.today()}\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"This coming Friday I need to book a flight from Halifax, NS to Austin, Texas. \n                                    It will be me and my friend and we need first class seats. \n                                    We will come back on Sunday. Let me know what I should pack for clothes \n                                    according to the weather there each day. Also please remind my team on \n                                    the DEV slack channel that I will be out of office on Friday. \n                                    1. Book the flight. \n                                    2. Let me know the weather. \n                                    3. Send the slack message.\"\"\",\n        },\n    ],\n    tools=tools,\n)\n\nfor tool in chat_completion.choices[0].message.tool_calls:\n    print(f\"function name: {tool.function.name}\")\n    print(f\"function arguments: {json.loads(tool.function.arguments)}\")\n    print()\n\nfunction name: book_flight\nfunction arguments: {'departure_city': 'Halifax', 'arrival_city': 'Austin', 'departure_date': '2024-05-03', 'return_date': '2024-05-05', 'num_passengers': 2, 'cabin_class': 'First'}\n\nfunction name: get_weather_forecast\nfunction arguments: {'location': 'Austin, Texas', 'date': '2024-05-03'}\n\nfunction name: get_weather_forecast\nfunction arguments: {'location': 'Austin, Texas', 'date': '2024-05-04'}\n\nfunction name: get_weather_forecast\nfunction arguments: {'location': 'Austin, Texas', 'date': '2024-05-05'}\n\nfunction name: send_slack_message\nfunction arguments: {'channel_name': 'DEV', 'message': 'I will be out of office this Friday, May 3, 2024. Please reach out via email if urgent.'}"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#rag-step-1---index-your-documents",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#rag-step-1---index-your-documents",
    "title": "Intro to LLMs",
    "section": "RAG: Step 1 - Index your Documents",
    "text": "RAG: Step 1 - Index your Documents\n\nRAG is a technique for augmenting LLM knowledge with additional data.\nimage source: langchain docs"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#rag-step-2---query-and-prompt-llm",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#rag-step-2---query-and-prompt-llm",
    "title": "Intro to LLMs",
    "section": "RAG: Step 2 - Query and Prompt LLM",
    "text": "RAG: Step 2 - Query and Prompt LLM"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#rag-resources",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#rag-resources",
    "title": "Intro to LLMs",
    "section": "RAG Resources",
    "text": "RAG Resources\n\nVector DBs\n\nweaviate\npinecone\nvespa\nqdrant\n\nLLM Frameworks: (not necessary for building on prod but good for learning and POC)\n\nLlamaIndex\nlangchain"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-1",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-1",
    "title": "Intro to LLMs",
    "section": "MultiModal",
    "text": "MultiModal"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-2",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-2",
    "title": "Intro to LLMs",
    "section": "MultiModal",
    "text": "MultiModal\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What is unusual about this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://i.pinimg.com/736x/6e/71/0d/6e710de5084379ba6a57b77e6579084f.jpg\",\n                    },\n                },\n            ],\n        }\n    ],\n    max_tokens=300,\n)\n\nprint(response.choices[0].message.content)\n\nThe unusual aspect of this image is a man ironing clothes on an ironing board placed on top of a taxi in the middle of a busy street. This is an uncommon sight, as ironing typically takes place in domestic or commercial indoor settings. The juxtaposition of such a mundane, home-based activity with the fast-paced, outdoor environment of a city street is quite remarkable and humorous. Additionally, both the ironing board and the taxi are branded with the same logo, suggesting that this scene might be part of a promotional event or public stunt to attract attention."
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-3",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-3",
    "title": "Intro to LLMs",
    "section": "MultiModal",
    "text": "MultiModal"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-4",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-4",
    "title": "Intro to LLMs",
    "section": "MultiModal",
    "text": "MultiModal\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What is in this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://media.makeameme.org/created/it-worked-fine.jpg\",\n                    },\n                },\n            ],\n        }\n    ],\n    max_tokens=300,\n)\n\nprint(response.choices[0].message.content)\n\nThe image is a meme featuring two juxtaposed elements. In the background, there is a scene of a house on fire with firefighters and emergency responders at the site, attempting to manage the situation. In the foreground, there is a young girl smirking at the camera with a knowing expression. Overlaid text reads, \"IT WORKED FINE IN DEV, IT'S A DEVOPS PROBLEM NOW,\" humorously suggesting that a problem developed during the software development stage is now a problem for the DevOps team to handle. The meme uses the incongruity between the calm and mischievous expression of the girl and the chaotic scene behind her to underline its comedic message about shifting blame in a development context."
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-5",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-5",
    "title": "Intro to LLMs",
    "section": "MultiModal",
    "text": "MultiModal"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-6",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-6",
    "title": "Intro to LLMs",
    "section": "MultiModal",
    "text": "MultiModal\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Give me a long list of visual search tags/keywords so I can \"\n                    \"index this image in my visual search index. Respond in JSON format {'labels': ['label1', ...]}\",\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://storage.googleapis.com/pai-images/a6d0952a331d40489b216e7f3f1ff6ed.jpeg\",\n                    },\n                },\n            ],\n        }\n    ],\n    response_format={\"type\": \"json_object\"},\n)\n\nprint(response.choices[0].message.content)\n\n{\n  \"labels\": [\n    \"animated character\",\n    \"wizard\",\n    \"Minion\",\n    \"fantasy\",\n    \"3D illustration\",\n    \"cute\",\n    \"magic\",\n    \"staff\",\n    \"long beard\",\n    \"blue hat\",\n    \"glasses\",\n    \"overalls\",\n    \"adventure\",\n    \"comical character\",\n    \"grey beard\",\n    \"wooden staff\",\n    \"round glasses\",\n    \"yellow\",\n    \"character design\",\n    \"creative\",\n    \"digital art\",\n    \"sorcerer\",\n    \"cartoon\",\n    \"funny\",\n    \"elderly character\",\n    \"mystical\",\n    \"storybook\",\n    \"cloak\",\n    \"leather belt\",\n    \"buckle\"\n  ]\n}"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#code-interpreter-data-analysis",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#code-interpreter-data-analysis",
    "title": "Intro to LLMs",
    "section": "Code Interpreter (Data Analysis)",
    "text": "Code Interpreter (Data Analysis)\n\ngive the LLM access to Python\nyour own little data analyst to give tasks to\n\nexample"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#fine-tuning-1",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#fine-tuning-1",
    "title": "Intro to LLMs",
    "section": "Fine Tuning",
    "text": "Fine Tuning\n\ntodo\naxolotl\ntorchtune"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#agents-1",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#agents-1",
    "title": "Intro to LLMs",
    "section": "Agents",
    "text": "Agents\n\ntodo"
  },
  {
    "objectID": "posts/anthropic/anthropic.html",
    "href": "posts/anthropic/anthropic.html",
    "title": "LLM Tool Loops with OpenAI and Anthropic",
    "section": "",
    "text": "In this post I will be using OpenAI and Anthropic APIs fpr LLM inference and function calling. First I will go into some differences between the OpenAI and Anthropic APIs. I will be using a wrapper I wrote for implementing tool calling loops.\nThen I will apply function calling to a problem and run some evaluation. I‚Äôm going to do this in the context of working on a ‚Äúsynthetic‚Äù problem with synthetic data. This ‚Äúfake‚Äù problem has some similarities related to some other projects I‚Äôve been working on recently."
  },
  {
    "objectID": "posts/anthropic/anthropic.html#my-own-wrapper-with-an-emphasis-on-the-tool-calling-loop",
    "href": "posts/anthropic/anthropic.html#my-own-wrapper-with-an-emphasis-on-the-tool-calling-loop",
    "title": "LLM Tool Loops with OpenAI and Anthropic",
    "section": "My Own Wrapper with an Emphasis on the ‚ÄúTool Calling Loop‚Äù",
    "text": "My Own Wrapper with an Emphasis on the ‚ÄúTool Calling Loop‚Äù\nI have written my own wrapper classes around some functionality of OpenAI and Anthropic. Here are some reasons why I did this.\n\nThe best way to learn is to write it yourself. The best wrapper is the one you write and test yourself, and fully understand.\nI wanted to write a wrapper that was focused on tool/function calling and provide a similar interface between OpenAI and Anthropic.\nI wanted to add in some features specific to tool calling such as parallel execution, and followup calls (tool calling loop).\n\nYou don‚Äôt need to understand my OpenAI and Anthropic wrapper code to understand this blog post. I‚Äôm not going to show any of that code directly here, but I am going to use it. At the end of the day it‚Äôs just using OpenAI and Anthropic python libraries for interacting with their models. The wrapper is not really what‚Äôs important here. The focus will be on the tool calling loop. All of the code can be found here if you are intersted.\n\nThe Tool Calling Loop\n\n\n\n\n\nI want the tool calling loop in my wrapper to be able to do the following:\n\nCall functions in parallel when possible. For example, if the LLM says to call several tools which are independent of each other, then these functions should be executed in parallel (not sequentially).\nHandle followup tool calls if necessary. For example, if the output of one tool is required as the input to another tool, allow the LLM to decide if more followup tool calls are required.\nWork with both Anthropic and OpenAI tool calling, using a similar interface.\nKeep record of all the tool calls, the inputs, the outputs, etc. This is useful for debugging and for the evaluation.\n\nThat is essentially the ‚Äútool calling loop‚Äù with some custom logic for my own use case.\nA simple example would be:\nUSER: I want to book a flight for Boston or New York. Pick the location that has the better weather. Thanks!\nASSISTANT:\n\ncalls get_weather(Boston) and get_weather(New York) independently and in parallel.\nPicks the location with the best weather (Boston).\ncalls book_flight(Boston)\nProvides final assistant message to user.\n\nThe tool calling loop bundles up this logic into a single python function."
  },
  {
    "objectID": "posts/anthropic/anthropic.html#api-differences",
    "href": "posts/anthropic/anthropic.html#api-differences",
    "title": "LLM Tool Loops with OpenAI and Anthropic",
    "section": "API Differences",
    "text": "API Differences\nLet‚Äôs start with comparing OpenAI chat completions and Anthropic message creations. I‚Äôm using my wrapper class here but all the basics/foundations are the same. The returned objects are just dictionaries and not the usual Pydantic objects. But it does not matter. If you have used either API before then all this will be pretty familiar.\nThe first major difference is that the system prompt has its own field argument for Anthropic. Whereas with the OpenAI messages format you provide the system role as the first message. It‚Äôs just personal preference but since I started with OpenAI, I like that way of working with the system prompt. So the first thing I did with my Anthropic wrapper is implement the system prompt similar to how OpenAI does it. This way I can pass similar messages objects to either wrapper as input. Let‚Äôs make this clear through a demonstration.\n\n\nCode\nfrom llm import OpenAiLMM, AnthropicLLM\n\n\nllm_openai = OpenAiLMM()\nllm_anthropic = AnthropicLLM()\n\nresp = llm_openai.call(\n    messages=[{\"role\": \"system\", \"content\": \"Talk like a pirate.\"}, {\"role\": \"user\", \"content\": \"hey\"}],\n    model=\"gpt-3.5-turbo-0125\",\n    temperature=0.6,\n    max_tokens=150,\n)\nresp\n\n\n{'message': {'content': \"Ahoy matey! What be ye needin' help with today?\",\n  'role': 'assistant'},\n 'model': 'gpt-3.5-turbo-0125',\n 'token_usage': {'completion_tokens': 15,\n  'prompt_tokens': 17,\n  'total_tokens': 32}}\n\n\n\n\nCode\nresp = llm_anthropic.call(\n    messages=[{\"role\": \"system\", \"content\": \"Talk like a pirate.\"}, {\"role\": \"user\", \"content\": \"hey\"}],\n    model=\"claude-3-5-sonnet-20240620\",\n    temperature=0.6,\n    max_tokens=150,\n)\nresp\n\n\n{'message': {'content': [{'text': \"Ahoy there, matey! What brings ye to these digital waters? Be ye seekin' treasure, adventure, or just a friendly chat with a landlubber like meself?\",\n    'type': 'text'}],\n  'role': 'assistant'},\n 'model': 'claude-3-5-sonnet-20240620',\n 'token_usage': {'completion_tokens': 42,\n  'prompt_tokens': 14,\n  'total_tokens': 56}}\n\n\nAlso note that even things like temperature are different. For OpenAI its range is [0,2] whereas Anthropic it‚Äôs [0,1]. The output token usages are different formats as well. Here I have made them the same, similar to OpenAI token usage format. I don‚Äôt plan to go through all these differences in this post. Just some of the ‚Äúbig‚Äù ones.\nThe next big difference is how the message responses/outputs are returned. By default, OpenAI returns n=1 responses/choices for normal text generation. It returns the {'content': \"...\", 'role': 'assistant'} format. If there are tool calls then it returns those as a separate field tool_calls. But Anthropic returns all its content as a list of messages. Those message objects can have different types such as text, tool_use, tool_result, etc.\nLet‚Äôs go through the all too familiar weather example. It‚Äôs like the ‚Äúhello world‚Äù of function calling. Starting with OpenAI API tool format:\n\n\nCode\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The location city\",\n                    },\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    }\n]\nllm_openai.call(messages=[{\"role\": \"user\", \"content\": \"What is the weather in Boston?\"}], model=\"gpt-3.5-turbo-0125\", tools=tools)\n\n\n{'message': {'content': None,\n  'role': 'assistant',\n  'tool_calls': [{'id': 'call_deXmNvBcbWiLH6yitYk7RlUC',\n    'function': {'arguments': '{\"location\":\"Boston\"}',\n     'name': 'get_current_weather'},\n    'type': 'function'}]},\n 'model': 'gpt-3.5-turbo-0125',\n 'token_usage': {'completion_tokens': 15,\n  'prompt_tokens': 61,\n  'total_tokens': 76}}\n\n\nThis is just the first step. The LLM says we need to call a tool and OpenAI uses this 'tool_calls' field. Also note that OpenAI often leaves content as None when returning tools to be called. This is not always the case though!\nWhen we make this same request with Anthropic we need to define the tools slightly different. It does not accept the same format. If we try and use the same format we get an error.\n\n\nCode\nllm_anthropic.call(messages=[{\"role\": \"user\", \"content\": \"What is the weather in Boston?\"}], model=\"claude-3-5-sonnet-20240620\", tools=tools)\n\n\n{'error': {'code': 'invalid_request_error',\n  'status_code': 400,\n  'type': 'invalid_request_error',\n  'message': 'tools.0.name: Field required'}}\n\n\nI have a simple function to convert from OpenAI tool format to Anthropic tool format.\n\n\nCode\nfrom copy import deepcopy\n\n\ndef convert_openai_tool_to_anthropic(open_ai_tool: dict):\n    t = deepcopy(open_ai_tool)\n    t = t[\"function\"]\n    t[\"input_schema\"] = t[\"parameters\"]\n    t.pop(\"parameters\")\n    return t\n\n\nWhen using Anthropic we need to define the tools like this.\n\n\nCode\nconvert_openai_tool_to_anthropic(tools[0])\n\n\n{'name': 'get_current_weather',\n 'description': 'Get the current weather in a given location',\n 'input_schema': {'type': 'object',\n  'properties': {'location': {'type': 'string',\n    'description': 'The location city'}},\n  'required': ['location']}}\n\n\n\n\nCode\nllm_anthropic.call(\n    messages=[{\"role\": \"user\", \"content\": \"What is the weather in Boston?\"}],\n    model=\"claude-3-5-sonnet-20240620\",\n    tools=[convert_openai_tool_to_anthropic(tools[0])],\n)\n\n\n{'message': {'content': [{'text': \"Certainly! I can help you check the current weather in Boston. To get this information, I'll use the get_current_weather function. Let me fetch that data for you right away.\",\n    'type': 'text'},\n   {'id': 'toolu_01FobdJhscfRVNXEKEM5vFAZ',\n    'input': {'location': 'Boston'},\n    'name': 'get_current_weather',\n    'type': 'tool_use'}],\n  'role': 'assistant'},\n 'model': 'claude-3-5-sonnet-20240620',\n 'token_usage': {'completion_tokens': 95,\n  'prompt_tokens': 374,\n  'total_tokens': 469}}\n\n\nWith the Anthropic output, the content field contains the tool calls and assistant messages together in the same list. There are different types on each object to tell them apart. I think sonnet is being prompted to use chain of thought prompting. That‚Äôs why it explains itself and provides the content message of type text. You can try and change this by adding a system prompt.\n\n\nCode\nllm_anthropic.call(\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"When calling tools/functions, do not talk about which ones you use or mention them.\",\n        },\n        {\"role\": \"user\", \"content\": \"What is the weather in Boston?\"},\n    ],\n    model=\"claude-3-5-sonnet-20240620\",\n    tools=[convert_openai_tool_to_anthropic(tools[0])],\n    temperature=0,\n)\n\n\n{'message': {'content': [{'text': \"Certainly! I'd be happy to check the current weather in Boston for you. Let me fetch that information for you right away.\",\n    'type': 'text'},\n   {'id': 'toolu_01ELcM8BoMiD57h4pi63FA5q',\n    'input': {'location': 'Boston'},\n    'name': 'get_current_weather',\n    'type': 'tool_use'}],\n  'role': 'assistant'},\n 'model': 'claude-3-5-sonnet-20240620',\n 'token_usage': {'completion_tokens': 82,\n  'prompt_tokens': 393,\n  'total_tokens': 475}}"
  },
  {
    "objectID": "posts/anthropic/anthropic.html#simple-tool-calling-loop-example",
    "href": "posts/anthropic/anthropic.html#simple-tool-calling-loop-example",
    "title": "LLM Tool Loops with OpenAI and Anthropic",
    "section": "Simple Tool Calling Loop Example",
    "text": "Simple Tool Calling Loop Example\nLet‚Äôs now go through a tool calling loop example. This tool calling loop in my wrapper is implemented in a function tool_loop. Let‚Äôs consider the simple example of using weather and flight booking tools together.\n\n\nCode\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The location city\",\n                    },\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"book_flight\",\n            \"description\": \"Book a flight from one location to another.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"departure_city\": {\n                        \"type\": \"string\",\n                        \"description\": \"The departure city.\",\n                    },\n                    \"arrival_city\": {\n                        \"type\": \"string\",\n                        \"description\": \"The arrival city.\",\n                    },\n                },\n                \"required\": [\"departure_city\", \"arrival_city\"],\n            },\n        },\n    },\n]\n\n\ndef get_current_weather(location):\n    if \"boston\" in location.lower():\n        return {\"data\": \"Sunny!\"}\n    else:\n        return {\"data\": \"Rainy!\"}\n\n\ndef book_flight(departure_city, arrival_city):\n    return {\"data\": f\"I have booked your flight from {departure_city} to {arrival_city}.\"}\n\n\nfunctions_look_up = {\"get_current_weather\": get_current_weather, \"book_flight\": book_flight}\n\n\nAbove is what is needed for the tool calling loop defined in tool_loop. The LLM will decide which tools to call, the arguments to use and so on. The functions will be executed and the results will be passed back to the LLM. Then the LLM will write the final assistant message. The tools/functions must return a dict with the key 'data', which is the tool result content passed to the LLM.\nFirst we will use OpenAI.\n\n\nCode\nresp = llm_openai.tool_loop(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"I need to book a flight from Halifax to either Boston or New York.\n            I want to fly to the city with the nicer weather. Please book my flight according to these requirements.\n    \"\"\",\n        }\n    ],\n    tools=tools,\n    functions_look_up=functions_look_up,\n    model=\"gpt-3.5-turbo-0125\",\n    temperature=0,\n)\n\n\n\n\nCode\nresp.keys()\n\n\ndict_keys(['message', 'new_messages', 'model', 'tool_calls_details', 'token_usage', 'execution_time'])\n\n\n\nresp['message'] is the final assistant message after all the internal looping logic.\n\n\n\nCode\nresp[\"message\"]\n\n\n{'content': 'I have booked your flight from Halifax to Boston as the weather there is sunny, which seems more favorable. Additionally, I have also booked a flight from Halifax to New York in case you change your mind.',\n 'role': 'assistant'}\n\n\n\nresp['new_messages'] is the record of all new messages created after the user message and up to and including the final assistant message. It is useful for keeping track of the conversation history in the format the API expects. It includes all the tool calls and interactions. It will be in the format expected by either OpenAI or Anthropic, depending on which API is being used. Note that this example is the OpenAI API format.\n\n\n\nCode\nresp[\"new_messages\"]\n\n\n[{'content': None,\n  'role': 'assistant',\n  'tool_calls': [{'id': 'call_WaTfGRRnW2w398swp5MFCJJ7',\n    'function': {'arguments': '{\"location\":\"Boston\"}',\n     'name': 'get_current_weather'},\n    'type': 'function'}]},\n {'tool_call_id': 'call_WaTfGRRnW2w398swp5MFCJJ7',\n  'role': 'tool',\n  'name': 'get_current_weather',\n  'content': 'Sunny!'},\n {'content': None,\n  'role': 'assistant',\n  'tool_calls': [{'id': 'call_wzPAhYvUijO3xHHAPqHcY4Fb',\n    'function': {'arguments': '{\"location\":\"New York\"}',\n     'name': 'get_current_weather'},\n    'type': 'function'}]},\n {'tool_call_id': 'call_wzPAhYvUijO3xHHAPqHcY4Fb',\n  'role': 'tool',\n  'name': 'get_current_weather',\n  'content': 'Rainy!'},\n {'content': None,\n  'role': 'assistant',\n  'tool_calls': [{'id': 'call_Yl4UTQw5YPORMUfnPknXuub6',\n    'function': {'arguments': '{\"departure_city\": \"Halifax\", \"arrival_city\": \"Boston\"}',\n     'name': 'book_flight'},\n    'type': 'function'},\n   {'id': 'call_Z3cwT0v6wVLk4VDtLnWD6X4F',\n    'function': {'arguments': '{\"departure_city\": \"Halifax\", \"arrival_city\": \"New York\"}',\n     'name': 'book_flight'},\n    'type': 'function'}]},\n {'tool_call_id': 'call_Yl4UTQw5YPORMUfnPknXuub6',\n  'role': 'tool',\n  'name': 'book_flight',\n  'content': 'I have booked your flight from Halifax to Boston.'},\n {'tool_call_id': 'call_Z3cwT0v6wVLk4VDtLnWD6X4F',\n  'role': 'tool',\n  'name': 'book_flight',\n  'content': 'I have booked your flight from Halifax to New York.'},\n {'content': 'I have booked your flight from Halifax to Boston as the weather there is sunny, which seems more favorable. Additionally, I have also booked a flight from Halifax to New York in case you change your mind.',\n  'role': 'assistant'}]\n\n\n\nresp['tool_calls_details'] is a dictionary with all the tool calls made, the results, the function names, and the input arguments. This is not used for passing to the LLM. Rather it‚Äôs just my way of keeping track of all the tool calls. It‚Äôs useful for debugging and future evaluation. I use this same format for OpenAI and Anthropic.\n\n\n\nCode\nresp[\"tool_calls_details\"]\n\n\n{'call_WaTfGRRnW2w398swp5MFCJJ7': {'tool_result': {'data': 'Sunny!'},\n  'id': 'call_WaTfGRRnW2w398swp5MFCJJ7',\n  'input': {'location': 'Boston'},\n  'name': 'get_current_weather',\n  'type': 'tool_use'},\n 'call_wzPAhYvUijO3xHHAPqHcY4Fb': {'tool_result': {'data': 'Rainy!'},\n  'id': 'call_wzPAhYvUijO3xHHAPqHcY4Fb',\n  'input': {'location': 'New York'},\n  'name': 'get_current_weather',\n  'type': 'tool_use'},\n 'call_Yl4UTQw5YPORMUfnPknXuub6': {'tool_result': {'data': 'I have booked your flight from Halifax to Boston.'},\n  'id': 'call_Yl4UTQw5YPORMUfnPknXuub6',\n  'input': {'departure_city': 'Halifax', 'arrival_city': 'Boston'},\n  'name': 'book_flight',\n  'type': 'tool_use'},\n 'call_Z3cwT0v6wVLk4VDtLnWD6X4F': {'tool_result': {'data': 'I have booked your flight from Halifax to New York.'},\n  'id': 'call_Z3cwT0v6wVLk4VDtLnWD6X4F',\n  'input': {'departure_city': 'Halifax', 'arrival_city': 'New York'},\n  'name': 'book_flight',\n  'type': 'tool_use'}}\n\n\n\nThe other fields are resp['token_usage'], resp['model'], and resp['execution_time']. They contain the token usage for the entirety of the interactions, the model used, and how long it took to execute the entire process tool_loop.\n\n\n\nCode\n{k: v for k, v in resp.items() if k in [\"token_usage\", \"model\", \"execution_time\"]}\n\n\n{'model': 'gpt-3.5-turbo-0125',\n 'token_usage': {'completion_tokens': 42,\n  'prompt_tokens': 279,\n  'total_tokens': 321},\n 'execution_time': 3.1535420417785645}\n\n\nNow we can use the Anthropic wrapper I wrote to do the same tool call loop with Anthropic‚Äôs claude sonnet 3.5. We just need to convert the tool format. Since I already explained all the fields returned, we will display the final result.\n\n\nCode\nresp = llm_anthropic.tool_loop(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"I need to book a flight from Halifax to either Boston or New York.\n            I want to fly to the city with the nicer weather. Please book my flight according to these requirements.\n    \"\"\",\n        }\n    ],\n    tools=[convert_openai_tool_to_anthropic(t) for t in tools],\n    functions_look_up=functions_look_up,\n    model=\"claude-3-5-sonnet-20240620\",\n    temperature=0,\n)\nresp\n\n\n{'message': {'content': \"Great news! I've successfully booked your flight from Halifax to Boston. To summarize:\\n\\n1. We checked the weather in both Boston and New York.\\n2. Boston currently has sunny weather, while New York is experiencing rain.\\n3. Based on your preference for nicer weather, we chose Boston as your destination.\\n4. Your flight has been booked from Halifax to Boston.\\n\\nIs there anything else you'd like to know about your flight or the weather at your destination?\",\n  'role': 'assistant'},\n 'new_messages': [{'role': 'assistant',\n   'content': [{'text': \"Certainly! I'd be happy to help you book a flight from Halifax to either Boston or New York based on which city has nicer weather. To accomplish this, we'll need to check the current weather in both Boston and New York, and then book your flight accordingly. Let's start by checking the weather in both cities.\",\n     'type': 'text'},\n    {'id': 'toolu_01Ti7jsr9wjDCpC4SPYRU51f',\n     'input': {'location': 'Boston'},\n     'name': 'get_current_weather',\n     'type': 'tool_use'},\n    {'id': 'toolu_01Vs1WZxVMNBywB7Ln71ysaq',\n     'input': {'location': 'New York'},\n     'name': 'get_current_weather',\n     'type': 'tool_use'}]},\n  {'role': 'user',\n   'content': [{'tool_use_id': 'toolu_01Ti7jsr9wjDCpC4SPYRU51f',\n     'type': 'tool_result',\n     'content': 'Sunny!'},\n    {'tool_use_id': 'toolu_01Vs1WZxVMNBywB7Ln71ysaq',\n     'type': 'tool_result',\n     'content': 'Rainy!'}]},\n  {'role': 'assistant',\n   'content': [{'text': \"Based on the weather information we received, it appears that Boston has better weather at the moment. Boston is sunny, while New York is rainy. Given your preference for nicer weather, I recommend booking your flight to Boston.\\n\\nNow, let's proceed with booking your flight from Halifax to Boston.\",\n     'type': 'text'},\n    {'id': 'toolu_01By6mj8VLxCvp8jeGuDRwF3',\n     'input': {'departure_city': 'Halifax', 'arrival_city': 'Boston'},\n     'name': 'book_flight',\n     'type': 'tool_use'}]},\n  {'role': 'user',\n   'content': [{'tool_use_id': 'toolu_01By6mj8VLxCvp8jeGuDRwF3',\n     'type': 'tool_result',\n     'content': 'I have booked your flight from Halifax to Boston.'}]},\n  {'role': 'assistant',\n   'content': [{'text': \"Great news! I've successfully booked your flight from Halifax to Boston. To summarize:\\n\\n1. We checked the weather in both Boston and New York.\\n2. Boston currently has sunny weather, while New York is experiencing rain.\\n3. Based on your preference for nicer weather, we chose Boston as your destination.\\n4. Your flight has been booked from Halifax to Boston.\\n\\nIs there anything else you'd like to know about your flight or the weather at your destination?\",\n     'type': 'text'}]}],\n 'model': 'claude-3-5-sonnet-20240620',\n 'tool_calls_details': {'toolu_01Ti7jsr9wjDCpC4SPYRU51f': {'tool_result': {'data': 'Sunny!'},\n   'id': 'toolu_01Ti7jsr9wjDCpC4SPYRU51f',\n   'input': {'location': 'Boston'},\n   'name': 'get_current_weather',\n   'type': 'tool_use'},\n  'toolu_01Vs1WZxVMNBywB7Ln71ysaq': {'tool_result': {'data': 'Rainy!'},\n   'id': 'toolu_01Vs1WZxVMNBywB7Ln71ysaq',\n   'input': {'location': 'New York'},\n   'name': 'get_current_weather',\n   'type': 'tool_use'},\n  'toolu_01By6mj8VLxCvp8jeGuDRwF3': {'tool_result': {'data': 'I have booked your flight from Halifax to Boston.'},\n   'id': 'toolu_01By6mj8VLxCvp8jeGuDRwF3',\n   'input': {'departure_city': 'Halifax', 'arrival_city': 'Boston'},\n   'name': 'book_flight',\n   'type': 'tool_use'}},\n 'token_usage': {'completion_tokens': 106,\n  'prompt_tokens': 892,\n  'total_tokens': 998},\n 'execution_time': 8.871042728424072}\n\n\nIt‚Äôs kind of neat to see the chain of thought and reasoning. But it depends on the application whether you want all that extra token usage. I hope this helps you understand some differences between Anthropic‚Äôs and OpenAI APIs when it comes to tool calling. Next we will continue looking at a different and more difficult problem with tool calling."
  },
  {
    "objectID": "posts/anthropic/anthropic.html#metrics",
    "href": "posts/anthropic/anthropic.html#metrics",
    "title": "LLM Tool Loops with OpenAI and Anthropic",
    "section": "Metrics",
    "text": "Metrics\nWhen a user asks a question it will be about at least one of these metrics. The user will use some natural description to reference these metrics. Possibly using similar wording to the name or description columns below. Then there is an associated enum value for each metric which is used in the backend when calling the backend APIs. There are about 150 metrics.\n\n\nCode\necommerce_metrics_df = pd.DataFrame(ecommerce_metrics)\nshow(ecommerce_metrics_df)\n\n\n\n\n\n    \n      \n      name\n      enum\n      description\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.1.4 from the internet...\n(need help?)"
  },
  {
    "objectID": "posts/anthropic/anthropic.html#brands",
    "href": "posts/anthropic/anthropic.html#brands",
    "title": "LLM Tool Loops with OpenAI and Anthropic",
    "section": "Brands",
    "text": "Brands\nThen there is the concept of brands, of which I have generated around 130 or so.\n\n\nCode\nbrands_df = pd.DataFrame(brands)\nshow(brands_df)\n\n\n\n\n\n    \n      \n      name\n      enum\n      description\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.1.4 from the internet...\n(need help?)"
  },
  {
    "objectID": "posts/anthropic/anthropic.html#sales-channels",
    "href": "posts/anthropic/anthropic.html#sales-channels",
    "title": "LLM Tool Loops with OpenAI and Anthropic",
    "section": "Sales Channels",
    "text": "Sales Channels\nFinally, we have some various sales channels. These are different channels which sales are made through. Each sale channel has the same associated fields. The number of sales channels is designed to be much less than the number of metrics and brands.\n\n\nCode\nsales_channels_df = pd.DataFrame(sales_channels)\nshow(sales_channels_df)\n\n\n\n\n\n    \n      \n      name\n      enum\n      description\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.1.4 from the internet...\n(need help?)"
  },
  {
    "objectID": "posts/anthropic/anthropic.html#example-user-questions",
    "href": "posts/anthropic/anthropic.html#example-user-questions",
    "title": "LLM Tool Loops with OpenAI and Anthropic",
    "section": "Example User Questions",
    "text": "Example User Questions\nGiven this data, I used Anthropic‚Äôs claude-3-5-sonnet to generate some questions users could ask. The questions can mention the metrics, brands, sales channels, as well as a time dimension. For each question there is the ‚Äúground truth‚Äù expected metrics, brands, sales channels, and time range. This ground truth will be used later on when we do evaluation. Just scroll to the right in the table below to see the other fields/columns.\n\n\nCode\nquestions_df = pd.DataFrame(questions)\nshow(questions_df)\n\n\n\n\n\n    \n      \n      question\n      expected_metric\n      expected_brands\n      expected_sales_channels\n      current_period_start_date\n      current_period_end_date\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.1.4 from the internet...\n(need help?)"
  },
  {
    "objectID": "posts/anthropic/anthropic.html#evaluating-gpt-3.5-turbo-0125",
    "href": "posts/anthropic/anthropic.html#evaluating-gpt-3.5-turbo-0125",
    "title": "LLM Tool Loops with OpenAI and Anthropic",
    "section": "Evaluating gpt-3.5-turbo-0125",
    "text": "Evaluating gpt-3.5-turbo-0125\n\n\nCode\ndf_openai = pd.DataFrame(eval_questions(llm_openai, \"gpt-3.5-turbo-0125\", tools_openai, questions, max_workers=10))\n\n\nEvaluating questions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:21&lt;00:00,  3.71it/s]\n\n\nYou have to scroll far to the right here because there are many columns. But take a look at the expected_ and predicted_ columns in particular. It‚Äôs very useful for looking at the data and debugging any issues.\n\n\nCode\nshow(df_openai)\n\n\n\n\n\n    \n      \n      message\n      new_messages\n      model\n      tool_calls_details\n      token_usage\n      execution_time\n      question\n      expected_metric\n      predicted_metric\n      metric_correct\n      expected_brands\n      predicted_brands\n      brands_correct\n      expected_sales_channels\n      predicted_sales_channels\n      sales_channels_correct\n      expected_current_period_start_date\n      predicted_current_period_start_date\n      current_period_start_date_correct\n      expected_current_period_end_date\n      predicted_current_period_end_date\n      current_period_end_date_correct\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.1.4 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\nCode\ncalculate_accuracies(df_openai)\n\n\n{'metric_accuracy': '100.00%',\n 'brands_accuracy': '98.75%',\n 'sales_channels_accuracy': '68.75%',\n 'current_period_start_date_accuracy': '97.50%',\n 'current_period_end_date_accuracy': '88.75%'}\n\n\nLet‚Äôs see where it‚Äôs making some mistakes on the sales channels.\n\n\nCode\nmistakes = df_openai[~df_openai[\"sales_channels_correct\"]]\nshow(mistakes[[\"question\", \"expected_sales_channels\", \"predicted_sales_channels\", \"token_usage\", \"execution_time\"]])\n\n\n\n\n\n    \n      \n      question\n      expected_sales_channels\n      predicted_sales_channels\n      token_usage\n      execution_time\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.1.4 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWe can take a look at some of these tool calls in detail:\n\n\nCode\ndf_openai.loc[mistakes.index, [\"expected_sales_channels\", \"new_messages\"]][:3].to_dict(orient=\"records\")\n\n\n[{'expected_sales_channels': ['AMAZON', 'ETSY'],\n  'new_messages': [{'content': None,\n    'role': 'assistant',\n    'tool_calls': [{'id': 'call_NdNvc1hAJoC2jSafBDuwu81s',\n      'function': {'arguments': '{\"backend_metric\": \"CUSTOMER_ACQUISITION_COST\", \"backend_brands\": [\"ECO_GREEN\"], \"sales_channels\": [\"ETSY\"], \"current_period_start_date\": \"2024-04-01\", \"current_period_end_date\": \"2024-04-30\"}',\n       'name': 'get_ecommerce_data'},\n      'type': 'function'},\n     {'id': 'call_ClEDXc6IRUWIjSzbjUuPkM8t',\n      'function': {'arguments': '{\"backend_metric\": \"CUSTOMER_ACQUISITION_COST\", \"backend_brands\": [\"ECO_GREEN\"], \"sales_channels\": [\"AMAZON\"], \"current_period_start_date\": \"2024-04-01\", \"current_period_end_date\": \"2024-04-30\"}',\n       'name': 'get_ecommerce_data'},\n      'type': 'function'}]},\n   {'tool_call_id': 'call_NdNvc1hAJoC2jSafBDuwu81s',\n    'role': 'tool',\n    'name': 'get_ecommerce_data',\n    'content': '2'},\n   {'tool_call_id': 'call_ClEDXc6IRUWIjSzbjUuPkM8t',\n    'role': 'tool',\n    'name': 'get_ecommerce_data',\n    'content': '2'},\n   {'content': 'The Customer Acquisition Cost for EcoGreen on Etsy for April 2024 is $25, and on Amazon for the same period is $30.',\n    'role': 'assistant'}]},\n {'expected_sales_channels': ['AMAZON', 'OWN_WEBSITE'],\n  'new_messages': [{'content': None,\n    'role': 'assistant',\n    'tool_calls': [{'id': 'call_hlqZHOQdpfhnzzBpCOyX1ZjN',\n      'function': {'arguments': '{\"backend_metric\": \"CUSTOMER_RETENTION_RATE\", \"backend_brands\": [\"BOOK_WORM\"], \"sales_channels\": [\"AMAZON\"], \"current_period_start_date\": \"2024-01-01\", \"current_period_end_date\": \"2024-06-30\"}',\n       'name': 'get_ecommerce_data'},\n      'type': 'function'},\n     {'id': 'call_YHnnbcJthTkCt51NiYFmOVje',\n      'function': {'arguments': '{\"backend_metric\": \"CUSTOMER_RETENTION_RATE\", \"backend_brands\": [\"BOOK_WORM\"], \"sales_channels\": [\"OWN_WEBSITE\"], \"current_period_start_date\": \"2024-01-01\", \"current_period_end_date\": \"2024-06-30\"}',\n       'name': 'get_ecommerce_data'},\n      'type': 'function'}]},\n   {'tool_call_id': 'call_hlqZHOQdpfhnzzBpCOyX1ZjN',\n    'role': 'tool',\n    'name': 'get_ecommerce_data',\n    'content': '8'},\n   {'tool_call_id': 'call_YHnnbcJthTkCt51NiYFmOVje',\n    'role': 'tool',\n    'name': 'get_ecommerce_data',\n    'content': '3'},\n   {'content': 'The Customer Retention Rate for BookWorm on Amazon for the first half of 2024 is 8%.\\n\\nThe Customer Retention Rate for BookWorm on their Own Website for the first half of 2024 is 3%.',\n    'role': 'assistant'}]},\n {'expected_sales_channels': ['INSTAGRAM_SHOPPING', 'OWN_WEBSITE'],\n  'new_messages': [{'content': None,\n    'role': 'assistant',\n    'tool_calls': [{'id': 'call_FBqZTjauL4wdahrxfMGB3Pz1',\n      'function': {'arguments': '{\"backend_metric\": \"MOBILE_TRAFFIC_PERCENTAGE\", \"backend_brands\": [\"ADIDAS\"], \"sales_channels\": [\"OWN_WEBSITE\"], \"current_period_start_date\": \"2024-05-01\", \"current_period_end_date\": \"2024-05-31\"}',\n       'name': 'get_ecommerce_data'},\n      'type': 'function'},\n     {'id': 'call_qNk1YOzE6flPA9L1zttIO6uh',\n      'function': {'arguments': '{\"backend_metric\": \"MOBILE_TRAFFIC_PERCENTAGE\", \"backend_brands\": [\"ADIDAS\"], \"sales_channels\": [\"INSTAGRAM_SHOPPING\"], \"current_period_start_date\": \"2024-05-01\", \"current_period_end_date\": \"2024-05-31\"}',\n       'name': 'get_ecommerce_data'},\n      'type': 'function'}]},\n   {'tool_call_id': 'call_FBqZTjauL4wdahrxfMGB3Pz1',\n    'role': 'tool',\n    'name': 'get_ecommerce_data',\n    'content': '2'},\n   {'tool_call_id': 'call_qNk1YOzE6flPA9L1zttIO6uh',\n    'role': 'tool',\n    'name': 'get_ecommerce_data',\n    'content': '3'},\n   {'content': 'The Mobile Traffic Percentage for Adidas on their Own Website in May 2024 was 72%, and on Instagram Shopping it was 68%.',\n    'role': 'assistant'}]}]\n\n\nIt looks like these are examples where the LLM made multiple separate tool calls, one for each sales channel. Our evaluation is a little too simplistic since it only grabs one tool call to extract the arguments. We are marking some of these as incorrect, but they could actually be correct if they are applying the correct sales channels over multiple calls."
  },
  {
    "objectID": "posts/anthropic/anthropic.html#evaluating-gpt-4o-mini",
    "href": "posts/anthropic/anthropic.html#evaluating-gpt-4o-mini",
    "title": "LLM Tool Loops with OpenAI and Anthropic",
    "section": "Evaluating gpt-4o-mini",
    "text": "Evaluating gpt-4o-mini\nUPDATE 2024-07-19: This model came out recently, so I went back and updated this post to evaluate it.\n\n\nCode\ndf_openai = pd.DataFrame(eval_questions(llm_openai, \"gpt-4o-mini\", tools_openai, questions, max_workers=10))\n\n\nEvaluating questions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:39&lt;00:00,  2.01it/s]\n\n\n\n\nCode\nshow(df_openai)\n\n\n\n\n\n    \n      \n      message\n      new_messages\n      model\n      tool_calls_details\n      token_usage\n      execution_time\n      question\n      expected_metric\n      predicted_metric\n      metric_correct\n      expected_brands\n      predicted_brands\n      brands_correct\n      expected_sales_channels\n      predicted_sales_channels\n      sales_channels_correct\n      expected_current_period_start_date\n      predicted_current_period_start_date\n      current_period_start_date_correct\n      expected_current_period_end_date\n      predicted_current_period_end_date\n      current_period_end_date_correct\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.1.4 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\nCode\ncalculate_accuracies(df_openai)\n\n\n{'metric_accuracy': '98.75%',\n 'brands_accuracy': '95.00%',\n 'sales_channels_accuracy': '36.25%',\n 'current_period_start_date_accuracy': '98.75%',\n 'current_period_end_date_accuracy': '88.75%'}\n\n\n\n\nCode\nmistakes = df_openai[~df_openai[\"sales_channels_correct\"]]\nshow(mistakes[[\"question\", \"expected_sales_channels\", \"predicted_sales_channels\", \"token_usage\", \"execution_time\"]])\n\n\n\n\n\n    \n      \n      question\n      expected_sales_channels\n      predicted_sales_channels\n      token_usage\n      execution_time\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.1.4 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nNOTE: The lower accuracy is because my evaluation is a little too simplistic since it only grabs one tool call to extract the arguments."
  },
  {
    "objectID": "posts/anthropic/anthropic.html#evaluting-claude-3-5-sonnet-20240620",
    "href": "posts/anthropic/anthropic.html#evaluting-claude-3-5-sonnet-20240620",
    "title": "LLM Tool Loops with OpenAI and Anthropic",
    "section": "Evaluting claude-3-5-sonnet-20240620",
    "text": "Evaluting claude-3-5-sonnet-20240620\nLet‚Äôs run the eval with Anthropic now. At the moment I keep getting rate limiting errors because I am on a lower tier with Anthropic. So it takes longer because I have to do one request at a time.\n\n\nCode\ndf_anthropic = pd.DataFrame(eval_questions(llm_anthropic, \"claude-3-5-sonnet-20240620\", tools_anthropic, questions, max_workers=1))\n\n\nEvaluating questions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [11:15&lt;00:00,  8.45s/it]\n\n\n\n\nCode\ncalculate_accuracies(df_anthropic)\n\n\n{'metric_accuracy': '98.75%',\n 'brands_accuracy': '98.75%',\n 'sales_channels_accuracy': '100.00%',\n 'current_period_start_date_accuracy': '98.75%',\n 'current_period_end_date_accuracy': '90.00%'}\n\n\n\n\nCode\nshow(df_anthropic)\n\n\n\n\n\n    \n      \n      message\n      new_messages\n      model\n      tool_calls_details\n      token_usage\n      execution_time\n      question\n      expected_metric\n      predicted_metric\n      metric_correct\n      expected_brands\n      predicted_brands\n      brands_correct\n      expected_sales_channels\n      predicted_sales_channels\n      sales_channels_correct\n      expected_current_period_start_date\n      predicted_current_period_start_date\n      current_period_start_date_correct\n      expected_current_period_end_date\n      predicted_current_period_end_date\n      current_period_end_date_correct\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.1.4 from the internet...\n(need help?)"
  },
  {
    "objectID": "posts/anthropic/anthropic.html#evaluating-gpt-3.5-turbo-0125-1",
    "href": "posts/anthropic/anthropic.html#evaluating-gpt-3.5-turbo-0125-1",
    "title": "LLM Tool Loops with OpenAI and Anthropic",
    "section": "Evaluating gpt-3.5-turbo-0125",
    "text": "Evaluating gpt-3.5-turbo-0125\n\n\nCode\ndf_openai = pd.DataFrame(eval_questions(llm_openai, \"gpt-3.5-turbo-0125\", tools_openai, questions, max_workers=10))\n\n\nEvaluating questions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:32&lt;00:00,  2.45it/s]\n\n\n\n\nCode\nshow(df_openai)\n\n\n\n\n\n    \n      \n      message\n      new_messages\n      model\n      tool_calls_details\n      token_usage\n      execution_time\n      question\n      expected_metric\n      predicted_metric\n      metric_correct\n      expected_brands\n      predicted_brands\n      brands_correct\n      expected_sales_channels\n      predicted_sales_channels\n      sales_channels_correct\n      expected_current_period_start_date\n      predicted_current_period_start_date\n      current_period_start_date_correct\n      expected_current_period_end_date\n      predicted_current_period_end_date\n      current_period_end_date_correct\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.1.4 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\nCode\ncalculate_accuracies(df_openai)\n\n\n{'metric_accuracy': '91.25%',\n 'brands_accuracy': '100.00%',\n 'sales_channels_accuracy': '96.25%',\n 'current_period_start_date_accuracy': '100.00%',\n 'current_period_end_date_accuracy': '91.25%'}"
  },
  {
    "objectID": "posts/anthropic/anthropic.html#evaluating-gpt-4o-mini-1",
    "href": "posts/anthropic/anthropic.html#evaluating-gpt-4o-mini-1",
    "title": "LLM Tool Loops with OpenAI and Anthropic",
    "section": "Evaluating gpt-4o-mini",
    "text": "Evaluating gpt-4o-mini\nUPDATE 2024-07-19: This model came out recently, so I went back and updated this post to evaluate it.\n\n\nCode\ndf_openai = pd.DataFrame(eval_questions(llm_openai, \"gpt-4o-mini\", tools_openai, questions, max_workers=10))\n\n\nEvaluating questions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [00:31&lt;00:00,  2.52it/s]\n\n\n\n\nCode\nshow(df_openai)\n\n\n\n\n\n    \n      \n      message\n      new_messages\n      model\n      tool_calls_details\n      token_usage\n      execution_time\n      question\n      expected_metric\n      predicted_metric\n      metric_correct\n      expected_brands\n      predicted_brands\n      brands_correct\n      expected_sales_channels\n      predicted_sales_channels\n      sales_channels_correct\n      expected_current_period_start_date\n      predicted_current_period_start_date\n      current_period_start_date_correct\n      expected_current_period_end_date\n      predicted_current_period_end_date\n      current_period_end_date_correct\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.1.4 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\nCode\ncalculate_accuracies(df_openai)\n\n\n{'metric_accuracy': '100.00%',\n 'brands_accuracy': '100.00%',\n 'sales_channels_accuracy': '98.75%',\n 'current_period_start_date_accuracy': '100.00%',\n 'current_period_end_date_accuracy': '90.00%'}"
  },
  {
    "objectID": "posts/anthropic/anthropic.html#evaluating-claude-3-5-sonnet-20240620",
    "href": "posts/anthropic/anthropic.html#evaluating-claude-3-5-sonnet-20240620",
    "title": "LLM Tool Loops with OpenAI and Anthropic",
    "section": "Evaluating claude-3-5-sonnet-20240620",
    "text": "Evaluating claude-3-5-sonnet-20240620\nAnd for Anthropic:\n\n\nCode\ndf_anthropic = pd.DataFrame(eval_questions(llm_anthropic, \"claude-3-5-sonnet-20240620\", tools_anthropic, questions, max_workers=1))\n\n\nEvaluating questions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80/80 [16:40&lt;00:00, 12.50s/it]\n\n\n\n\nCode\nshow(df_anthropic)\n\n\n\n\n\n    \n      \n      message\n      new_messages\n      model\n      tool_calls_details\n      token_usage\n      execution_time\n      question\n      expected_metric\n      predicted_metric\n      metric_correct\n      expected_brands\n      predicted_brands\n      brands_correct\n      expected_sales_channels\n      predicted_sales_channels\n      sales_channels_correct\n      expected_current_period_start_date\n      predicted_current_period_start_date\n      current_period_start_date_correct\n      expected_current_period_end_date\n      predicted_current_period_end_date\n      current_period_end_date_correct\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.1.4 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\nCode\ncalculate_accuracies(df_anthropic)\n\n\n{'metric_accuracy': '97.50%',\n 'brands_accuracy': '98.75%',\n 'sales_channels_accuracy': '98.75%',\n 'current_period_start_date_accuracy': '97.50%',\n 'current_period_end_date_accuracy': '88.75%'}\n\n\nLet‚Äôs look at the token usage and execution time.\n\n\nCode\nshow(df_openai[[\"execution_time\", \"token_usage\"]])\n\n\n\n\n\n    \n      \n      execution_time\n      token_usage\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.1.4 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\nCode\nshow(df_anthropic[[\"execution_time\", \"token_usage\"]])\n\n\n\n\n\n    \n      \n      execution_time\n      token_usage\n    \n  \n\n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\n\n\nLoading ITables v2.1.4 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nThe execution time has gone up. We are using more tools and calling out to OpenAI for embeddings within those tools. But the results are still great, and now we are generating around 75% less tokens. Both approaches have pros and cons depending on the requirements and situation."
  },
  {
    "objectID": "posts/basic_transformer_notes/transformers.html",
    "href": "posts/basic_transformer_notes/transformers.html",
    "title": "Basic Transformer Architecture Notes",
    "section": "",
    "text": "Here are some notes on the basic transformer architecture for my personal learning and understanding. Useful as a secondary resource, not the first stop. There are many resources out there, but here are several I enjoyed learning from:\n\nChapter 3 of the book Natural Language Processing With Transformers (Tunstall, Von Werra, and Wolf 2022)\nAndrej Karpathy‚Äôs video Let‚Äôs build GPT: from scratch, in code, spelled out (Karpathy 2023)\nSebastian Raschka‚Äôs Blog Post Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs (Raschka 2024)\nOmar Sanseviero‚Äôs Blog Post The Random Transformer (Sanseviero 2024)\nThe Illustrated Transformer (Alammar 2018)\nThe original paper: Attention Is All You Need (Vaswani et al. 2017)"
  },
  {
    "objectID": "posts/basic_transformer_notes/transformers.html#multi-head-attention",
    "href": "posts/basic_transformer_notes/transformers.html#multi-head-attention",
    "title": "Basic Transformer Architecture Notes",
    "section": "Multi Head Attention",
    "text": "Multi Head Attention\n\nThere are multiple attention heads, each with their own independent queries, keys, values.\nEach attention head takes the input embeddings of shape (B, T, C) and produces an output (B, T, H).\nConcatenate the outputs from each head so that the concatenated tensor is back to the original input shape (B, T, C).\nOnce we have the concatenated output tensor, we put it through a linear projection, nn.Linear(embed_dim, embed_dim) to get the output from the multi head attention: a tensor of shape (B, T, C)."
  },
  {
    "objectID": "posts/colpali/colpali_blog.html",
    "href": "posts/colpali/colpali_blog.html",
    "title": "PDF Q&A App using ColPali, Modal, and FastHTML",
    "section": "",
    "text": "Intro\nLately, I‚Äôve been following FastHTML from a distance. As someone who sticks to backend Python development, frontend development has always been a bit foreign to me, but I‚Äôm interested in giving it a shot. FastHTML feels like a good way to get started with some basics by building small apps.\nI‚Äôve also noticed a lot of chatter on X about Colpali and document retrieval with vision language models, which caught my attention. I like exploring new stuff so I want to see what that is all about.\nOn top of that, I‚Äôm still enjoying Modal, which I‚Äôve written about before here and here. I thought it would be fun to combine these tools into a simple app and see what I can learn from it.\nAll the code for this project is in this folder. The main code is the following:\n\nmulti_modal_rag.py - A Modal app running on CPU that runs the multimodal retrieval logic.\npdf_retriever.py - A Modal app running on GPU which processes and caches images/embeddings for each PDF and runs inference for ColPali.\nutils.py - some simple utility functions for logging and generating unique folder names in the Modal Volumes.\nmain.py - the FastHTML app that runs the frontend.\ncolpali_blog.ipynb - a notebook that I used to generate the blog post for this project.\n\nSee the README for more details.\n\n\nColPali\nThere are already so many great resources out there about ColPali. Checkout the resources below for more information. I will give a quick overview.\nI have already deployed ColPali to Modal as a remote function I can call, running on an A10 GPU.\nmodal deploy pdf_retriever.py\nRemember that with Modal, you only pay for compute when running requests in active containers. My deployed app can sit there idle without costing me anything!\n\n\n\nColPali Model Deployed on Modal\n\n\nThere are a couple functions I have decorated with @modal.method() within the PDFRetriever class:\n\ndef forward(self, inputs) ‚Äì&gt; here\ndef top_pages(self, pdf_url, queries, use_cache=True, top_k=1) ‚Äì&gt; here\n\nLet‚Äôs look at the forward function first as it can be used to run inference on a list of strings or images to get the embeddings.\nFirst we will pass in text inputs to ColPali.\n\n\nCode\nimport modal\n\nforward = modal.Function.lookup(\"pdf-retriever\", \"PDFRetriever.forward\")\nembeddings_batch = forward.remote([\"How does the latency between ColPali and standard retrieval methods compare?\"])\nassert len(embeddings_batch) == 1  # we passed in one document i.e. batch size of 1\nembeddings = embeddings_batch[0]\nprint(embeddings.shape)\nembeddings\n\n\ntorch.Size([28, 128])\n\n\ntensor([[ 0.1572, -0.0240,  0.0942,  ..., -0.0278, -0.0791, -0.0129],\n        [-0.0688, -0.1260,  0.0038,  ..., -0.0073, -0.1162,  0.0962],\n        [ 0.0413, -0.1055, -0.1055,  ..., -0.0055, -0.2178,  0.1406],\n        ...,\n        [-0.0825, -0.0444, -0.0674,  ..., -0.0327, -0.1504,  0.1670],\n        [ 0.1465,  0.0016, -0.1338,  ...,  0.0127, -0.2119,  0.1191],\n        [ 0.1641, -0.0405, -0.1338,  ...,  0.0175, -0.2080,  0.1177]],\n       dtype=torch.bfloat16)\n\n\nThe first thing to note is that we don‚Äôt get a single dense embedding vector. Traditionally that is the case where a single vector is used to represent one input. But ColPali is generating ColBERT-style multi-vector representations of the input. With the late interaction paradigm you get back multiple embeddings, one per input token. Each embedding is 128-dimensional.\nColPali is trained to take image documents as input. It was trained on query-document pairs where each document is a page of a PDF. Each PDF page (‚Äúdocument‚Äù) is treated as an image. It uses a vision language model to create multi-vector embeddings purely from visual document features.\nConsider the following image of a PDF page from the ColPali paper:\n We can pass this image to the forward function and get the embeddings back. The ColPali model divides each page image into a 32 x 32 = 1024 patches. In addition to the image grid patches, ColPali includes 6 instruction text tokens that are prepended to the image input. These tokens represent the text: ‚ÄúDescribe the image.‚Äù Combining the image grid patches and the instruction tokens, we get: 1024 (image patches) + 6 (instruction tokens) = 1030 total patches/embeddings.\n\n\nCode\nfrom PIL import Image\n\nimg = Image.open(\"imgs/colpali_paper_page_sample.png\")\nembeddings = forward.remote([img])[0]\nprint(embeddings.shape)\nembeddings\n\n\ntorch.Size([1030, 128])\n\n\ntensor([[-0.1562, -0.0396, -0.0908,  ...,  0.1426, -0.1113,  0.1079],\n        [-0.1260,  0.0427,  0.0991,  ..., -0.0286, -0.0170,  0.0786],\n        [-0.1621,  0.0297,  0.0874,  ..., -0.0255, -0.0168,  0.0625],\n        ...,\n        [ 0.1045, -0.0178,  0.0522,  ..., -0.0986, -0.1011, -0.0366],\n        [ 0.0078,  0.0674,  0.0674,  ..., -0.0226, -0.0479, -0.0908],\n        [ 0.0062,  0.0623,  0.1396,  ...,  0.0264, -0.1699, -0.1533]],\n       dtype=torch.bfloat16)\n\n\nUsing the ColPali model we produce multi-vector embeddings per page which can be indexed. At query time, we use the same model to generate multi-vector embeddings for the query. So both queries and documents are represented as sets of vectors rather than single vector.\nThe MaxSim (Maximum Similarity) scoring function is used to compute the similarity between query embeddings and document embeddings. The scoring function performs the following steps:\n\nComputes dot products between all query token embeddings and all document page patch embeddings\nApplies a max reduce operation over the patch dimension\nPerforms a sum reduce operation over the query tokens\n\nThere is a great and simple explanation in this blog post\nI have wrapped the logic for a given PDF url and query/question within the deployed Modal function\ndef top_pages(self, pdf_url, queries, use_cache=True, top_k=1).\nThe function takes a pdf_url and a list of queries (questions) and returns the top top_k pages for each query/question. The use of ColPali and the MaxSim scoring function allows us to retrieve the most relevant pages from the PDF that will assist in answering the question\n\n\nCode\nget_top_pages = modal.Function.lookup(\"pdf-retriever\", \"PDFRetriever.top_pages\")\npdf_url = \"https://arxiv.org/pdf/2407.01449\"\ntop_pages = get_top_pages.remote(pdf_url, queries=[\"How does the latency between ColPali and standard retrieval methods compare?\"], top_k=3)[0]\ntop_pages\n\n\n[1, 0, 4]\n\n\nThis first returned index page 1 is actually the second page of the PDF since we start counting from 0. And that page being returned is the image we saw earlier from the ColPali paper. It‚Äôs really cool because the answer is found in the figure on that page.\n\n\nGenerating the Answer\nOnce we have the top pages/images as context, we can pass them along with the query/question to a vision language model to generate an answer. The images are passed as the context and the question/query is passed as text. I have this logic deployed in a Modal Application as well running on CPU. It communicates with the other deployed ColPali Modal app running on the GPU when it needs to compute the embeddings. I am using OpenAI‚Äôs gpt-4o-mini for the vision language model to generate the answer with the provided image context and question.\nmodal deploy multi_modal_rag.py\nThe deployed Modal function here is\ndef answer_question_with_image_context(pdf_url, query, top_k=1, use_cache=True, max_new_tokens=2000, additional_instructions=\"\"):\n\n\nCode\nanswer_question_with_image_context = modal.Function.lookup(\"multi-modal-rag\", \"answer_question_with_image_context\")\nres = answer_question_with_image_context.remote_gen(\n    pdf_url=\"https://arxiv.org/pdf/2407.01449\", query=\"How does the latency between ColPali and standard retrieval methods compare?\", top_k=5\n)\nanswer = \"\".join([chunk for chunk in res if type(chunk) == str])\nprint(answer)\n\n\nThe latency comparison between ColPali and standard retrieval methods indicates that ColPali is significantly faster. Specifically:\n\n- **ColPali**: 0.39 seconds per page.\n- **Standard Retrieval**: 7.22 seconds per page.\n\nThis demonstrates that ColPali achieves better performance in terms of latency while maintaining a stronger relevance score in document retrieval tasks.\n\n\n\n\nFastHTML App\nTo demo the FastHTML App I created, I will share images and videos of running it locally. The entire app is in the code main.py.\npython main.py\nHere is what the app looks like when you first load it up:\n\nHere are two videos of running the app and asking questions about the ColPali paper.\n\n\nThis PDF url of the ColPali paper was already processed and cached which means I already stored the embeddings and images inside volumes on Modal. So it loads the document embeddings and images very quickly. Also, the Modal container was warm and running so there were no cold start delays.\nIn this next video I will demo the app with a new PDF url that was not processed and cached yet. I will also send the requests to the backend when the Modal containers are idle. These requests will trigger the Modal containers to start up and run the inference. It will take longer but you will see how everything is logged from the backend in the terminal window I created. It uses server-sent events (SSE) to stream the logs to the frontend so you can see what is happening in the backend. This example will use a longer PDF from Meta, Movie Gen: A Cast of Media Foundation Models, which is 92 pages.\n\nThis next video runs the same PDF and question a second time. Now that all the images and document embeddings are cached in a volume on Modal, everything is much faster. This is also using a warm Modal container so there were no cold start delays. Most of the time is spent in the OpenAI API call which takes five images as input and streams back the text response.\n\n\n\nHighlights\nThere are a few highlights I want to call out. The first is the use of server-sent events (SSE) to stream the logs to the frontend. The backend code is running in the cloud on Modal‚Äôs infrastructure. In the frontend code I created the terminal looking window with this code. It continually calls the /poll-queue endpoint to get the latest logs from Modal and streams them via SSE. In Modal I am using a Queue to collect the logs. Throughout my Modal application code I use these functions. Anytime I want to log a message I just call log_to_queue. It gets placed on the queue and then read_from_queue is used to pop the message off the queue and display it. It‚Äôs a fun and neat way to provide more visibility to the frontend about what the backend is doing. It‚Äôs also neat since messages are being logged from multiple containers.\n\nAnother highlight is the use of Modal‚Äôs volume functionality. I use a volume to store the images and document embeddings for each PDF that is processed. This way if the PDF is used a second time, the images and embeddings are stored to the Volume for fast retrieval. This avoids having to call ColPali processing and PDF processing for each question/query related to the same document.\n\nThere is a folder for each PDF processed (for images and embeddings).\n\n\nEach image for each page is stored in the volume like this: \nAnd all the document embeddings, for a single PDF, are stored in Pickle format in a file called embeddings.pkl. One .pkl file per PDF. \nSince I am only allowing to ask questions about a single PDF at a time, there is no need for fancy vector DBs etc. The embeddings for a specific PDF are cached and can be loaded into memory very quickly when needed. When a new PDF comes along that is not cached, we process it, and then store the images and embeddings in the volume. You can see all the details about PDF processing and ColPali inference in the PDFRetriever class.\nOne final highlight was streaming the OpenAI response back to the frontend in markdown format via SSE. This took me a while to figure out how to do. On the frontend I did this. There could be better ways to do this but it works for now. Big shout out to @Frax and @Phi from the FastHTML Discord channel for helping me out with that. Streaming from Modal was really easy. I just made used of yield here and remote_gen here.\n\n\nConclusions\nThis was really fun to build. I am such a noob with FastHTML and look forward to the documentation and community expanding. Some improvements to this app could be:\n\nwhen clicking the submit button, it would clear the log terminal window/div and answer window/div.\nAdding the heatmaps to the PDF page images which highlight which sections/tokens are most relevant to the query.\nadding sessions, authentication and authorization\n\n\n\nResources\nIn no particular order:\n\nColpali paper\nColbert paper\nColbert V2 paper\nPaliGemma\nA little pooling goes a long way for multi-vector representations: Blog answer.ai\n\nReducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling, Paper\n\nPLAID paper\nBeyond the Basics of Retrieval for Augmenting Generation (w/ Ben Clavi√©), Youtube Talk\nRAG is more than dense embedding, Google Slides, Ben Clavi√©\nThe quick start in the README Original ColPali Repo as well as the sample inference code\nHugging Face Model Cards\nThe Future of Search: Vision Models and the Rise of Multi-Model Retrieval\nScaling ColPali to billions of PDFs with Vespa\nBeyond Text: The Rise of Vision-Driven Document Retrieval for RAG\nVision Language Models Explained\nDocument Similarity Search with ColPali\nJo Kristian Bergum: X\nManuel Faysse: X\nTony Wu: X\nOmar Khattab: X\nfastHTML"
  },
  {
    "objectID": "posts/intro_fine_tune/intro_fine_tune.html",
    "href": "posts/intro_fine_tune/intro_fine_tune.html",
    "title": "Getting Started with Axolotl for Fine-Tuning LLMs",
    "section": "",
    "text": "I have experience fine-tuning smaller encoder style LLMs such as DistilBERT for classification style tasks. I have deployed such models in a production environment and have had great success with them. The Hugging Face trainer class makes it relatively easy and the models are easy to deploy, even on CPU based infrastructure.\nBut when it comes to training decoder style LLMs for text generation (GPT, llama, Mistral, Qwen, Gemma, etc.), I will be the first to admit that I am a complete noob. It‚Äôs something I have followed from a distance, trying to keep up with the recent methods/libraries, but I have not had any experience with it in terms of actual hands on practice.\nI don‚Äôt want to get into the debate on whether Fine-Tuning LLMs is valuable. The answer is probably most likely, ‚Äúit depends‚Äù. It‚Äôs something I want to learn more about, and just want to get started. There are so many new terms/ideas to learn (PEFT, LORA, QLORA, TRL, RHLF, DPO, etc.). My academic mathematical background says to start from the bottom and learn everything along the way. I know that is horrible advice in practice though because I will just get stuck. Instead, I will take advice from Jeremy Howard which is to begin at the top and just get started.\nSo in this post I will fine-tune my first model with the axolotl library. The model will probably suck, but that‚Äôs not the point. Whenever I learn a new tool/library the first thing is to set things up and run a ‚Äúhello world‚Äù type example. Even if it‚Äôs just copy/paste. That‚Äôs what I will do here. Don‚Äôt come here for advanced advice. Follow along if you are in a similar situation as me and just want to get started."
  },
  {
    "objectID": "posts/intro_fine_tune/intro_fine_tune.html#configuring-the-yaml-file",
    "href": "posts/intro_fine_tune/intro_fine_tune.html#configuring-the-yaml-file",
    "title": "Getting Started with Axolotl for Fine-Tuning LLMs",
    "section": "Configuring The YAML File",
    "text": "Configuring The YAML File\nThere are general guidelines in the README. The idea is to start with one of the example YAML files here. I wanted to fine-tune mistral-7b using QLORA, so I started with this YAML file here. I git clone the axolotl repo locally and open the code in my Pycharm IDE. Then I simply start editing the file examples/mistral/qlora.yml directly. That way I can easily see what changes I made with git diff annotations.\nThe only lines I changed were\ndatasets:\n  - path: data.jsonl\n    type: completion\nsequence_len: 1000\neval_sample_packing: False\nwandb_project: cal-train\nnum_epochs: 3\nevals_per_epoch: 1\neval_max_new_tokens: 1000\nI wanted to start with text completion, not instruction fine-tuning. I will try instruction fine-tuning later. I created a small dataset with some transcripts from some of the Cal Newport Podcast episodes. Either way, this is the part where you need to bring your own dataset to fine-tune the model on. It‚Äôs important to choose the correct dataset format and configure it properly in the YAML. My dataset is in a data.jsonl file at the root of the axolotl repo. It looks like this:\n{\"text\": \" I'm Cal Newport and this is Deep Questions Episode 185. I'm here in my deep work HQ along with my producer Jesse. ......\"}\n{\"text\": \"Alright, our next question is from Vinny. Vinny asks, how should I adjust my approach to hourly billing .....\"}\nEach record is around 500 words long. That is why I chose sequence_len to be 1000 which is counting tokens (not words). I have about 4000 rows like this in my dataset. Each record is a random excerpt from a podcast transcription.\nThe complete YAML looks like\nbase_model: mistralai/Mistral-7B-v0.1\nmodel_type: MistralForCausalLM\ntokenizer_type: LlamaTokenizer\n\nload_in_8bit: false\nload_in_4bit: true\nstrict: false\n\ndatasets:\n  - path: data.jsonl\n    type: completion\ndataset_prepared_path: last_run_prepared\nval_set_size: 0.1\noutput_dir: ./qlora-out\n\nadapter: qlora\nlora_model_dir:\n\nsequence_len: 1000\nsample_packing: true\neval_sample_packing: False\npad_to_sequence_len: true\n\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\nlora_fan_in_fan_out:\nlora_target_modules:\n  - gate_proj\n  - down_proj\n  - up_proj\n  - q_proj\n  - v_proj\n  - k_proj\n  - o_proj\n\nwandb_project: cal-train\nwandb_entity:\nwandb_watch:\nwandb_name:\nwandb_log_model:\n\ngradient_accumulation_steps: 4\nmicro_batch_size: 2\nnum_epochs: 3\noptimizer: adamw_bnb_8bit\nlr_scheduler: cosine\nlearning_rate: 0.0002\n\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: auto\nfp16:\ntf32: false\n\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\n\nloss_watchdog_threshold: 5.0\nloss_watchdog_patience: 3\n\nwarmup_steps: 10\nevals_per_epoch: 1\neval_table_size:\neval_max_new_tokens: 1000\nsaves_per_epoch: 1\ndebug:\ndeepspeed:\nweight_decay: 0.0\nfsdp:\nfsdp_config:\nspecial_tokens:\nThere is much more I need to learn about the parameters but that will slow me down. Therefore, I‚Äôm simply sticking with the defaults.\nPut the file on the GPU server/machine. I put it at the root.\nvim qlora.yml # copy/paste in your config"
  },
  {
    "objectID": "posts/intro_fine_tune/intro_fine_tune.html#pre-processing-the-dataset",
    "href": "posts/intro_fine_tune/intro_fine_tune.html#pre-processing-the-dataset",
    "title": "Getting Started with Axolotl for Fine-Tuning LLMs",
    "section": "Pre-Processing the Dataset",
    "text": "Pre-Processing the Dataset\nThe docs say to run\nCUDA_VISIBLE_DEVICES=\"\" python -m axolotl.cli.preprocess qlora.yml\nto pre-process the dataset.\nBy default, it puts the processed dataset in dataset_prepared_path: last_run_prepared.\nls last_run_prepared\nIt‚Äôs good to take a look at the data in there. See tips on debugging here.\nJust drop into an ipython shell and run this:\nfrom transformers import AutoTokenizer\nfrom datasets import load_from_disk\nimport yaml\n\ndirectory = !ls last_run_prepared/\nwith open('qlora.yml', 'r') as f:\n    cfg = yaml.safe_load(f)\nmodel_id = cfg['base_model']\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nds = load_from_disk(f'last_run_prepared/{directory[0]}/')\n\nrow = ds[0]\nprint(tokenizer.decode(row['input_ids']))\nFor me, it returns my first data record. I then can confirm that this is the first row in my data.jsonl file.\n&lt;s&gt;  Let's do a few more questions here. ....... Thank you. Bye..&lt;/s&gt;\nIt‚Äôs good to take a look through these and make sure things look all right. There are some special tokens added there. Those are the default special tokens I believe. You can read more about configuring special tokens here."
  },
  {
    "objectID": "posts/intro_fine_tune/intro_fine_tune.html#training-the-model",
    "href": "posts/intro_fine_tune/intro_fine_tune.html#training-the-model",
    "title": "Getting Started with Axolotl for Fine-Tuning LLMs",
    "section": "Training the Model",
    "text": "Training the Model\nI run this on a screen -S train but that is optional depending on your preference/setup.\naccelerate launch -m axolotl.cli.train qlora.yml\n\nYou can inspect GPU usage with nvidia-smi -l.\nYou can also follow along with the progress using Weights and Biases.\n\n\nHonestly, the training loss curve looks odd. I know Jeremy Howard and Jonathan Whitaker have written about such things before."
  },
  {
    "objectID": "posts/intro_fine_tune/intro_fine_tune.html#inference-with-the-model",
    "href": "posts/intro_fine_tune/intro_fine_tune.html#inference-with-the-model",
    "title": "Getting Started with Axolotl for Fine-Tuning LLMs",
    "section": "Inference with the Model",
    "text": "Inference with the Model\nThe saved model is in qlora-out directory.\nls qlora-out\nI think you can load it like this. I‚Äôm new to loading these adapters and the quantization config.\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\nimport json\n\ndef read_json_file(file_path):\n    with open(file_path, 'r') as f:\n        return json.load(f)\n    \nconfig = read_json_file('qlora-out/config.json')\nprint(config)\n{'_name_or_path': 'mistralai/Mistral-7B-v0.1',\n 'architectures': ['MistralForCausalLM'],\n 'attention_dropout': 0.0,\n 'bos_token_id': 1,\n 'eos_token_id': 2,\n 'hidden_act': 'silu',\n 'hidden_size': 4096,\n 'initializer_range': 0.02,\n 'intermediate_size': 14336,\n 'max_position_embeddings': 32768,\n 'model_type': 'mistral',\n 'num_attention_heads': 32,\n 'num_hidden_layers': 32,\n 'num_key_value_heads': 8,\n 'quantization_config': {'_load_in_4bit': True,\n  '_load_in_8bit': False,\n  'bnb_4bit_compute_dtype': 'bfloat16',\n  'bnb_4bit_quant_storage': 'float32',\n  'bnb_4bit_quant_type': 'nf4',\n  'bnb_4bit_use_double_quant': True,\n  'llm_int8_enable_fp32_cpu_offload': False,\n  'llm_int8_has_fp16_weight': False,\n  'llm_int8_skip_modules': None,\n  'llm_int8_threshold': 6.0,\n  'load_in_4bit': True,\n  'load_in_8bit': False,\n  'quant_method': 'bitsandbytes'},\n 'rms_norm_eps': 1e-05,\n 'rope_theta': 10000.0,\n 'sliding_window': 4096,\n 'tie_word_embeddings': False,\n 'torch_dtype': 'bfloat16',\n 'transformers_version': '4.40.0.dev0',\n 'use_cache': False,\n 'vocab_size': 32000}\nWe can use the quantization_config when loading the model:\nmodel_ckpt = 'qlora-out/checkpoint-672/'\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nquantized_config = BitsAndBytesConfig(**config['quantization_config'])\nmodel = AutoModelForCausalLM.from_pretrained(model_ckpt, device_map=\"auto\", quantization_config=quantized_config)\n\ntext = \"\"\"&lt;s&gt;My name is Cal &lt;/s&gt;\"\"\"\ninputs = tokenizer(text, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=750, do_sample=True, temperature=1)\n    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])\nInput: \"\"\"&lt;s&gt;My name is Cal &lt;/s&gt;\"\"\"\nOutput: My name is Cal And this is the Deep Questions podcast, episode 238. I‚Äôm here in my Deep Work HQ joined by my producer, Jesse. Jesse, how are you doing? Good, good. Glad to be here, glad to be here. Before we jump into today‚Äôs questions, there was a couple of announcements I wanted to make. The Deep Questions podcast is proud to be a sponsor of the 2023 Tropical Health and Wellness Summit, the THW summit that I have been talking about on the show. The summit brings together experts on health and wellness from a wide variety of fields, nutrition, fitness and so on. It allows you to enjoy a full-day virtual conference with the top experts to help you cultivate a healthier and deeper life. You can join from wherever you are and whenever you want, because the talks are available 24 hours a day on demand. If you‚Äôre watching this at youtube.com slash Cal Newport media, this will be the THW summit. A lot of good talks I thought, like the one I talked about last week with Mark Sisson and the whole day of these type of speakers talking, so you should definitely check it out. So go to T-H-E-W-S-U-M-M-I-T dot com slash Cal, T-H-E-W-S-U-M-M-I-T dot com slash Cal, that‚Äôs all lowercase and all one word, dot com slash Cal to register for free. And if you happen to mention in the registration that you came from deep questions, they will give you a free book to Mark Sisson‚Äôs last book, a new definition of fitness. All right, so that‚Äôs thws.com slash Cal. I also wanna briefly mention one of the sponsors that makes this podcast possible, which is ExpressVPN. See, if you‚Äôre like me, you are worried when you use the internet that people have access to what are you talking about and who are you talking to? The way that you can prevent this from being an issue is by using a VPN. Instead of directly connecting to whatever website or service you wanna use, you instead connect to a VPN server. You explain that server who you wanna talk to. And then that server talks to that website or service on your behalf, encrypts the response and sends it back. Now the people you‚Äôre connected to access point, the people who are watching the packets you‚Äôre sending, they don‚Äôt have a way of knowing for sure who you‚Äôre talking to, what you‚Äôre saying. Here‚Äôs the thing, a VPN really is needed in a world of widespread surveillance. So we‚Äôre talking about the people who are at the internet service provider level like your internet service provider, who can see all this stuff. They can see who you‚Äôre connecting to, but just the fact that you‚Äôre using a VPN blocks them from collecting all of that data.\nInput: \"\"\"&lt;s&gt; Social media is bad. Dont use it. Here is why. &lt;/s&gt;\"\"\"\nOutput: Social media is bad. Dont use it. Here is why. And that was the end of the conversation. And I think that approach, which is the approach that‚Äôs common today to talking about digital tech and kids, does more harm than good. So I wanna give two examples to illustrate the potential harms of this simplified response. The first comes from my own experience when I was a teen. I was a tech writer for a while when I was in high school, published my second book, How to Win at College when I was 19. In an interview, I talked about digital distractions and teenage life. And the reaction from readers was pretty severe. I received a lot of angry emails about this. There‚Äôs been a lot of yelling at me on social media for things that I was suggesting. And I did some public appearances and, you know, got grilled pretty hard on these topics. I actually have this one example I remember. So I didn‚Äôt give this talk on the radio, the local NPR in D.C. And so the whole interview was, I was being grilled about kids and tech. And here‚Äôs the key question that was asked. So the host said, okay, this question comes from Sam. He‚Äôs a 49-year-old teacher and he doesn‚Äôt have kids, which was a nice turn of phrases, right? So 49-year-old teacher, does not have kids. He says, can kids really do math or write these days? Like, do you think that all the screen time has gotten them so lost that they‚Äôve forgotten how to actually write or hold a pencil or solve even a simple addition problem? And then he goes on about like, okay, the way these kids walk or whatever. And then he says, he‚Äôs a teacher, I don‚Äôt want to get fired, right? I mean a lot of teachers have sent this stuff to me, but this was a good version of it. I do a fair number of, he‚Äôs like, I don‚Äôt want to get fired. All right, so I think he was worried about his job safety. I‚Äôm gonna get thrown out of his job for asking this question. All right, so then the host said, okay, well, Cal, you‚Äôre on the spot. What do you say? So I had to give an answer and my answer was, let me start by dismissing this particular point as being, and I wasn‚Äôt mean, I tried to be gentle, but I was dismissing this particular point as being somewhat silly. Right, because if we‚Äôre talking about the impact of social media on math and science achievements, there is not a huge body of scholastic literature on this. So this is a little bit of anecdote. If we‚Äôre talking about impact of social media and video games on attention, let‚Äôs start with attention span.\nThese are not copy/pastes from the training data. These are in fact newly generated text with some learned knowledge from the training data as well as some hallucinations :)."
  },
  {
    "objectID": "posts/modal_fun/modal_blog.html",
    "href": "posts/modal_fun/modal_blog.html",
    "title": "üöÄ Building with Modal üöÄ",
    "section": "",
    "text": "In my professional life I write code which ends up on a production environment, supported by sophisticated DevOps infrastructure. This system leverages a suite of tools such as Kubernetes, Rancher, Karpenter, Helm charts, Argo CD, GitHub Actions, and of course AWS. I‚Äôm fortunate to work alongside an exceptional DevOps team that keeps this complex machinery running smoothly. While I‚Äôm not deeply involved in the nitty-gritty of DevOps and infrastructure, I‚Äôm certainly exposed to it.\nOn the other hand, I also crave the simplicity of building and tinkering without infrastructure concerns, especially in my free time. Ideally, I‚Äôd work directly with Python code using just my IDE and terminal. I‚Äôd rather avoid writing another YAML file or worrying about spinning up instances, managing IAM roles, installing CUDA drivers, or juggling multiple microservices and containers. What I seek is a streamlined development experience that lets me focus on creativity and problem-solving, not infrastructure management.\nThis is where Modal enters the picture. I‚Äôm genuinely excited about Modal and consider it the most impressive platform I‚Äôve encountered for running code without infrastructure concerns. Modal is a serverless platform designed for Data/ML/AI teams that seamlessly bridges the gap between local development and cloud execution. The primary interface is a Python SDK, where decorators are used to quickly move function execution into the cloud. You write your code as if it were running locally, and Modal effortlessly deploys and runs it in the cloud. This approach offers the best of both worlds: the simplicity of local development with the power and scalability of cloud computing.\nModal didn‚Äôt simply create a wrapper on top of Kubernetes or Docker. While I won‚Äôt even pretend to understand the engineering behind it; it‚Äôs clearly their secret sauce. From what I‚Äôve read and heard, they‚Äôve built their own systems from scratch in Rust, including a container runtime, custom file system, custom image builder, and custom job scheduler. This allows for launching containers in the cloud within seconds.\nFor many AI applications, GPUs are a necessity. This can be a barrier for developers, including myself, who don‚Äôt have access to a local GPU. This is where Modal can really shine, providing easy access to GPU resources in the cloud within an isolated environment. You can experiment within the isolated environment without worrying about messing up your local machine.\nOf course there are many great options out there for spinning up GPU instances in the cloud. Some of the other platforms I enjoy are Jarvis Labs, Lambda Labs, and RunPod. I have tried all of these and I like them. I have even written previously about using some of these services here and here. Modal is offering something different though. It‚Äôs the developer experience that has hooked me on Modal. It‚Äôs the lower cold start times and the feeling of developing locally that make it so nice.\nI should note that I have only used Modal for personal projects and tinkering around with various ideas. However, I anticipate incorporating it more into my professional work, particularly for research projects and proofs of concept. Looking ahead, I can envision leveraging Modal directly in our production environment as well. It seems particularly well-suited for deploying complex AI models that require specific container configurations and GPU resources, especially in scenarios with unpredictable or spiky traffic patterns.\nIf you want to learn more about the history of Modal or keep up with the latest news, I recommend the following resources:\n\nModal Website\nModal X Account\nModal Slack Account (They are so helpful and responsive on Slack)\nCharles Frye X Account (AI Engineer at Modal)\nErik Bernhardsson X Account (CEO at Modal)\n1 to 100: Modal Labs (Interview with Erik Bernhardsson)\nWhy you should join Modal (Article)\nWhat I have been working on: Modal (Older article with relevant background)\n\n\n\nI could simply direct you to the Modal Documentation, which is exceptionally comprehensive and well-crafted. In fact, it‚Äôs so good that I doubt I could do it justice in a single post. However, I‚Äôm currently investing time in learning Modal, and what better way to solidify my understanding than by writing about it? Even if it means repeating some of the information in the documentation, it will still be a valuable exercise. Moreover, I‚Äôm eager to spread the word about this game-changing platform that I believe is still flying under the radar for many developers. By sharing my experiences and insights, I hope to contribute to the growing community of Modal enthusiasts."
  },
  {
    "objectID": "posts/modal_fun/modal_blog.html#why-am-i-writing-this-post",
    "href": "posts/modal_fun/modal_blog.html#why-am-i-writing-this-post",
    "title": "üöÄ Building with Modal üöÄ",
    "section": "",
    "text": "I could simply direct you to the Modal Documentation, which is exceptionally comprehensive and well-crafted. In fact, it‚Äôs so good that I doubt I could do it justice in a single post. However, I‚Äôm currently investing time in learning Modal, and what better way to solidify my understanding than by writing about it? Even if it means repeating some of the information in the documentation, it will still be a valuable exercise. Moreover, I‚Äôm eager to spread the word about this game-changing platform that I believe is still flying under the radar for many developers. By sharing my experiences and insights, I hope to contribute to the growing community of Modal enthusiasts."
  },
  {
    "objectID": "posts/modal_fun/modal_blog.html#shell-into-your-container",
    "href": "posts/modal_fun/modal_blog.html#shell-into-your-container",
    "title": "üöÄ Building with Modal üöÄ",
    "section": "Shell into your container",
    "text": "Shell into your container\nWe will see in later examples how to customize the environment of the container. But even with this simple example, we can shell into the default container and poke around. There are numerous ways to develop and debug your application with Modal.\nHere we use the modal shell command to quickly create a container and shell into it.\nmodal shell hello_modal.py::f\nThis video shows how easy it is to shell into the container.\n\nBy shelling into the container you get direct access to that isolated environment. You can inspect the file system, test the installation of additional dependencies, and generally look around to ensure your application is configured correctly."
  },
  {
    "objectID": "posts/dspy/dspy.html",
    "href": "posts/dspy/dspy.html",
    "title": "DSPy",
    "section": "",
    "text": "Intro\nDSPy kept popping up on my X timeline and I thought it looked pretty interesting, so I decided to take a few days to look into it. I didn‚Äôt get super deep into it yet, but I think I have a high level understanding. The library is fairly new IMO (as of writing this). There is excitement around it though and a growing community. I am hopeful that the documentation and library will continue to improve throughout the year. If you are completely new to DSPy I would suggest the following resources below.\n\nRead through the newer documentation here.\nCheckout the README from DSPY GitHub repo and the examples there.\nTry and code up some simple examples on your own data.\nCheckout the Discord server.\nSkim through or read some of the associated papers (see the paper links on the DSPy repo README). For example:\n\nDSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines(Khattab et al. (2023))\nDSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines(Singhvi et al. (2024))\n\nThere are also some decent videos on YouTube. Simply Search for DSPy LLM etc.\nFollow Omar Khattab\n\n\n\nENV Setup\npython3 -m venv env\nsource env/bin/activate\npip install dspy-ai\npip install openai --upgrade\npip install --upgrade notebook ipywidgets\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nBIG-Bench Hard Dataset - Penguins In a Table - Example\nWithin the BIG-Bench Hard dataset (Suzgun et al. 2022) there are various tasks. You can use one of these strings when using load_dataset to load in the corresponding records for that task.\n['tracking_shuffled_objects_seven_objects', 'salient_translation_error_detection', 'tracking_shuffled_objects_three_objects', 'geometric_shapes', 'object_counting', 'word_sorting', 'logical_deduction_five_objects', 'hyperbaton', 'sports_understanding', 'logical_deduction_seven_objects', 'multistep_arithmetic_two', 'ruin_names', 'causal_judgement', 'logical_deduction_three_objects', 'formal_fallacies', 'snarks', 'boolean_expressions', 'reasoning_about_colored_objects', 'dyck_languages', 'navigate', 'disambiguation_qa', 'temporal_sequences', 'web_of_lies', 'tracking_shuffled_objects_five_objects', 'penguins_in_a_table', 'movie_recommendation', 'date_understanding']\nWe will use the penguins_in_a_table task.\n\n\nCode\nfrom datasets import load_dataset\nimport dspy\n\nds = load_dataset(\"maveriq/bigbenchhard\", \"penguins_in_a_table\")[\"train\"]\nexamples = [dspy.Example({\"question\": r[\"input\"], \"answer\": r[\"target\"]}).with_inputs(\"question\") for r in ds]\nprint(f\"There are {len(examples)} examples.\")\ntrainset = examples[0:20]\nvalset = examples[20:]\n\n\nThere are 146 examples.\n\n\n\n\nCode\nexample = trainset[10]\nfor k, v in example.items():\n    print(f\"\\n{k.upper()}:\\n\")\n    print(v)\n\n\n\nQUESTION:\n\nHere is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We then delete the penguin named Bernard from the table.\nHow many penguins are more than 8 years old?\nOptions:\n(A) 1\n(B) 2\n(C) 3\n(D) 4\n(E) 5\n\nANSWER:\n\n(A)\n\n\nWe will use the DSPy OpenAI connector to make calls to gpt-3.5. Note that DSPy caches API calls so that subsequent calls with the same input will read from the cache instead of calling the OpenAI API a second time.\n\n\nCode\nllm = dspy.OpenAI(model=\"gpt-3.5-turbo-0125\", max_tokens=250)\ndspy.settings.configure(lm=llm)\n\n\nWe can test that the calls to OpenAI are working:\n\n\nCode\nllm(\"Testing testing, is anyone out there?\")\n\n\n[\"Hello! I'm here to help. What can I assist you with today?\"]\n\n\n\n\nCode\nllm(example.question)\n\n\n['There are 2 penguins who are more than 8 years old: Vincent (9 years old) and Gwen (8 years old). \\n\\nTherefore, the answer is (B) 2.']\n\n\nAt any point we can look at the last n calls to the llm:\n\n\nCode\nllm.inspect_history(n=2)\n\n\n\n\n\n\nTesting testing, is anyone out there? Hello! I'm here to help. What can I assist you with today?\n\n\n\n\n\n\n\nHere is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We then delete the penguin named Bernard from the table.\nHow many penguins are more than 8 years old?\nOptions:\n(A) 1\n(B) 2\n(C) 3\n(D) 4\n(E) 5 There are 2 penguins who are more than 8 years old: Vincent (9 years old) and Gwen (8 years old). \n\nTherefore, the answer is (B) 2.\n\n\n\n\nOur evaluation metric will check if the llm output contains the correct multiple choice answer. To define an evaluation metric in DSPy we create a function like the example below. The first two inputs should be instances of dspy.Example. The metric function can contain any logic you need to evaluate your task. You can read more about the trace argument in the documentation. It needs to be there, even if not explicitly using it.\n\n\nCode\nimport re\n\n\ndef eval_metric(true, prediction, trace=None):\n    pred = prediction.answer\n    matches = re.findall(r\"\\([A-Z]\\)\", pred)\n    parsed_answer = matches[-1] if matches else \"\"\n    return parsed_answer == true.answer\n\n\nWe set up an evaluation pipeline:\n\n\nCode\nfrom dspy.evaluate import Evaluate\n\nevaluate = Evaluate(devset=valset, metric=eval_metric, num_threads=6, display_progress=True, display_table=10)\n\n\nHere is a simple module in DSPy for basic question and answer.\n\n\nCode\nclass BasicQA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.prog = dspy.Predict(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.prog(question=question)\n\n\nbasic_qa = BasicQA()\n\n\nThe forward method calls __call__ similar to how things work in pytorch.\n\n\nCode\npred = basic_qa(question=example.question)\nprint(\"\\nQUESTION:\\n\")\nprint(example.question)\nprint(\"\\nANSWER:\\n\")\nprint(example.answer)\nprint(\"\\nPREDICTION:\\n\")\nprint(pred.answer)\n\n\n\nQUESTION:\n\nHere is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We then delete the penguin named Bernard from the table.\nHow many penguins are more than 8 years old?\nOptions:\n(A) 1\n(B) 2\n(C) 3\n(D) 4\n(E) 5\n\nANSWER:\n\n(A)\n\nPREDICTION:\n\n(B) 2\n\n\n\n\nCode\neval_metric(example, pred)\n\n\nFalse\n\n\n\n\nCode\nllm.inspect_history(n=1)\n\n\n\n\n\n\nGiven the fields `question`, produce the fields `answer`.\n\n---\n\nFollow the following format.\n\nQuestion: ${question}\nAnswer: ${answer}\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. We then delete the penguin named Bernard from the table. How many penguins are more than 8 years old? Options: (A) 1 (B) 2 (C) 3 (D) 4 (E) 5\nAnswer: (B) 2\n\n\n\n\nNow we can pass each example question through the LLM in the validation set and check if we get the correct answer:\n\n\nCode\nevaluate(basic_qa)\n\n\nAverage Metric: 44 / 126  (34.9%)\n\n\n\n\n\n\n\n¬†\nquestion\nexample_answer\npred_answer\neval_metric\n\n\n\n\n0\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\n3\nFalse\n\n\n1\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\n(C) 50\nFalse\n\n\n2\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nAnswer: (C) 3\nFalse\n\n\n3\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nAnswer: (B) 2\nFalse\n\n\n4\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(B)\n(B) 5\n‚úîÔ∏è [True]\n\n\n5\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\n(B) 2\nFalse\n\n\n6\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(E)\nJames\nFalse\n\n\n7\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\n(B) 2\nFalse\n\n\n8\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\nAnswer: Vincent\nFalse\n\n\n9\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\nAnswer: Donna\nFalse\n\n\n\n\n\n\n                \n                    ... 116 more rows not displayed ...\n                \n                \n\n\n34.92\n\n\nDSPy uses optimizers to optimize the modules. In this example, optimization is a process that will choose which demos/examples are best to put into the prompt in order to increase the evaluation metric. At the time of writing the optimizers are called teleprompters (prompting from a distance). I think they will change the name though to optimizers in future refactoring. The DSPy documentation states that the optimizer can adjust/edit:\n\nDemo examples in the prompt.\nInstructions of the prompt.\nWeights of the actual LLM (for example fine tuning an open source model).\n\nI have only played around with optimizers that optimize which demos/examples are put into the prompt.\n\n\nCode\nfrom dspy.teleprompt import BootstrapFewShotWithRandomSearch\n\nconfig = dict(max_bootstrapped_demos=2, max_labeled_demos=4, num_candidate_programs=2, num_threads=6)\n\nteleprompter = BootstrapFewShotWithRandomSearch(metric=eval_metric, **config)\noptimized_qa = teleprompter.compile(basic_qa, trainset=trainset, valset=valset)\n\n\nThere is a lot of output from the above code block which I am hiding to keep things cleaner. You can now evaluate the optimized model to see if the accuracy has improved.\n\n\nCode\nevaluate(optimized_qa)\n\n\nAverage Metric: 54 / 126  (42.9%)\n\n\n\n\n\n\n\n¬†\nquestion\nexample_answer\npred_answer\neval_metric\n\n\n\n\n0\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\n(C)\nFalse\n\n\n1\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\n(C) 50\nFalse\n\n\n2\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\n(B)\nFalse\n\n\n3\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\n(B)\nFalse\n\n\n4\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(B)\n(B)\n‚úîÔ∏è [True]\n\n\n5\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\n(C)\n‚úîÔ∏è [True]\n\n\n6\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(E)\n(D)\nFalse\n\n\n7\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\n(B)\nFalse\n\n\n8\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\n(D) Gwen\nFalse\n\n\n9\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\n(D) Donna\n‚úîÔ∏è [True]\n\n\n\n\n\n\n                \n                    ... 116 more rows not displayed ...\n                \n                \n\n\n42.86\n\n\n\n\nCode\nllm.inspect_history()\n\n\n\n\n\n\nGiven the fields `question`, produce the fields `answer`.\n\n---\n\nFollow the following format.\n\nQuestion: ${question}\nAnswer: ${answer}\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. And here is a similar table, but listing giraffes: name, age, height (cm), weight (kg) Jody, 5, 430, 620 Gladys, 10, 420, 590 Marian, 2, 310, 410 Donna, 9, 440, 650 How many giraffes are more than 5 years old? Options: (A) 1 (B) 2 (C) 3 (D) 4 (E) 5\nAnswer: (B)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. What is the name of the last penguin sorted by alphabetic order? Options: (A) Louis (B) Bernard (C) Vincent (D) Gwen (E) James\nAnswer: (C)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. We now add a penguin to the table: James, 12, 90, 12 We then delete the penguin named Bernard from the table. How many penguins are more than 5 years old and weight more than 12 kg? Options: (A) 1 (B) 2 (C) 3 (D) 4 (E) 5\nAnswer: (A)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. How many animals are listed in the table? Options: (A) 1 (B) 2 (C) 3 (D) 4 (E) 5\nAnswer: (D)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. Which is the second heaviest penguin? Options: (A) Louis (B) Bernard (C) Vincent (D) Gwen (E) James\nAnswer: (B) Bernard\n\n\n\n\nNow we can try a Chain of Thought (Wei et al. 2023) prompt.\n\n\nCode\nclass CoT(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.prog = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.prog(question=question)\n\n\ncot_qa = CoT()\n\n\n\n\nCode\nevaluate(cot_qa)\n\n\nAverage Metric: 90 / 126  (71.4%)\n\n\n\n\n\n\n\n¬†\nquestion\nexample_answer\nrationale\npred_answer\neval_metric\n\n\n\n\n0\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. We first identify the penguins who are less than 8 years old. From the table, we see that Louis is 7 years...\n(B) 2\nFalse\n\n\n1\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\nproduce the answer. We need to add up the weights of all the penguins in the table. Louis weighs 11 kg, Bernard weighs 13 kg,...\n(D) 62\n‚úîÔ∏è [True]\n\n\n2\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. We need to go through each penguin's age and count how many are more than 8 years old.\n(C) 3\nFalse\n\n\n3\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. We need to identify the penguins who are both more than 5 years old and weigh more than 12 kg. Looking at...\n(C) 3\nFalse\n\n\n4\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(B)\nproduce the answer. We can see from the table that Bernard's age is 5.\n(B) 5\n‚úîÔ∏è [True]\n\n\n5\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\nproduce the answer. We first identify the penguins who are less than 10 years old. Louis is 7 years old, Bernard is 5 years old,...\n(D) 4\nFalse\n\n\n6\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(E)\nproduce the answer. We need to identify the last penguin added to the table. By looking at the last entry in the penguin table, we...\nJames\nFalse\n\n\n7\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. We first need to identify the penguins who are more than 5 years old and weigh more than 12 kg. From the...\n(A) 1\n‚úîÔ∏è [True]\n\n\n8\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\nproduce the answer. We need to find the penguin with a height of 60 cm. Looking at the table, we see that Vincent is the...\n(C) Vincent\n‚úîÔ∏è [True]\n\n\n9\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\nproduce the answer. We need to look at the last entry in the table listing giraffes. The last giraffe listed is Donna.\n(D) Donna\n‚úîÔ∏è [True]\n\n\n\n\n\n\n                \n                    ... 116 more rows not displayed ...\n                \n                \n\n\n71.43\n\n\n\n\nCode\nllm.inspect_history()\n\n\n\n\n\n\nGiven the fields `question`, produce the fields `answer`.\n\n---\n\nFollow the following format.\n\nQuestion: ${question}\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\nAnswer: ${answer}\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. We now add a penguin to the table: James, 12, 90, 12 Which penguin is taller than the other ones? Options: (A) Louis (B) Bernard (C) Vincent (D) Gwen (E) James\nReasoning: Let's think step by step in order to produce the answer. We need to compare the height of each penguin in the table and determine which one is the tallest. Louis is 50 cm tall, Bernard is 80 cm tall, Vincent is 60 cm tall, Gwen is 70 cm tall, and James is 90 cm tall. Therefore, James is taller than all the other penguins.\nAnswer: (E) James\n\n\n\n\nNow we will try and optimize our chain of thought program. I am also hiding the output from this cell to keep things cleaner.\n\n\nCode\ntqdm._instances.clear()\nconfig = dict(max_bootstrapped_demos=1, max_labeled_demos=4, num_candidate_programs=4, num_threads=6)\nteleprompter = BootstrapFewShotWithRandomSearch(metric=eval_metric, **config)\noptimized_cot_qa = teleprompter.compile(cot_qa, trainset=trainset, valset=valset)\n\n\n\n\nCode\nevaluate(optimized_cot_qa)\n\n\nAverage Metric: 102 / 126  (81.0%)\n\n\n\n\n\n\n\n¬†\nquestion\nexample_answer\nrationale\npred_answer\neval_metric\n\n\n\n\n0\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. After deleting Bernard, the penguins left are Louis, Vincent, and Gwen. Among them, Louis and Gwen are less than 8 years old.\n(B) 2\nFalse\n\n\n1\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\nproduce the answer. We sum up the weights of all the penguins: 11 + 13 + 11 + 15 + 12 = 62.\n(D) 62\n‚úîÔ∏è [True]\n\n\n2\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. We know that after deleting Bernard, the penguins left are Louis, Vincent, and Gwen. Among them, only Vincent is more than 8...\n(A) 1\n‚úîÔ∏è [True]\n\n\n3\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. We have Louis, Vincent, Gwen, and James in the table. Among them, only James is more than 5 years old and weighs...\n(A) 1\n‚úîÔ∏è [True]\n\n\n4\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(B)\nproduce the answer. We know that the age of Bernard is 5 years old.\n(B) 5\n‚úîÔ∏è [True]\n\n\n5\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\nproduce the answer. We know that after deleting Bernard, the penguins left are Louis, Vincent, and Gwen. Among them, Louis and Gwen are less than...\n(B) 2\nFalse\n\n\n6\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(E)\nproduce the answer. We know that the last penguin added to the table is James.\n(E) James\n‚úîÔ∏è [True]\n\n\n7\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. After deleting Bernard, we are left with Louis, Vincent, and Gwen. Among them, only Gwen is more than 5 years old and...\n(A) 1\n‚úîÔ∏è [True]\n\n\n8\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\nproduce the answer. We know that the only penguin with a height of 60 cm is Vincent.\n(C) Vincent\n‚úîÔ∏è [True]\n\n\n9\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\nproduce the answer. We know that the last giraffe listed is Donna.\n(D) Donna\n‚úîÔ∏è [True]\n\n\n\n\n\n\n                \n                    ... 116 more rows not displayed ...\n                \n                \n\n\n80.95\n\n\n\n\nCode\nllm.inspect_history(n=1)\n\n\n\n\n\n\nGiven the fields `question`, produce the fields `answer`.\n\n---\n\nFollow the following format.\n\nQuestion: ${question}\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\nAnswer: ${answer}\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. We then delete the penguin named Bernard from the table. How many penguins are more than 8 years old? Options: (A) 1 (B) 2 (C) 3 (D) 4 (E) 5\nReasoning: Let's think step by step in order to produce the answer. We know that after deleting Bernard, the penguins left are Louis, Vincent, and Gwen. Among them, only Vincent is more than 8 years old.\nAnswer: (A) 1\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. How many penguins are more than 5 years old? Options: (A) 1 (B) 2 (C) 3 (D) 4 (E) 5\nAnswer: (C)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. And here is a similar table, but listing giraffes: name, age, height (cm), weight (kg) Jody, 5, 430, 620 Gladys, 10, 420, 590 Marian, 2, 310, 410 Donna, 9, 440, 650 How many animals are more than 5 years old? Options: (A) 5 (B) 6 (C) 7 (D) 8 (E) 9\nAnswer: (A)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. Which penguin is older than Gwen? Options: (A) Louis (B) Bernard (C) Vincent (D) Gwen (E) James\nAnswer: (C)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. We then delete the penguin named Bernard from the table. What is the name of the last penguin sorted by alphabetic order? Options: (A) Louis (B) Bernard (C) Vincent (D) Gwen (E) James\nReasoning: Let's think step by step in order to produce the answer. After deleting Bernard, the remaining penguins are Louis, Vincent, and Gwen. Sorting them alphabetically, the last penguin is Vincent.\nAnswer: (C) Vincent\n\n\n\n\nIt‚Äôs really nice that the above focused on:\n\nWriting small modules/programs.\nChoosing an optimizer.\nRunning the compile/optimization step.\nRunning an evaluation.\n\nI really like this idea instead of manually writing prompts and hoping for the best.\n\n\n\n\n\nReferences\n\nKhattab, Omar, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, et al. 2023. ‚ÄúDSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines.‚Äù https://arxiv.org/abs/2310.03714.\n\n\nSinghvi, Arnav, Manish Shetty, Shangyin Tan, Christopher Potts, Koushik Sen, Matei Zaharia, and Omar Khattab. 2024. ‚ÄúDSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines.‚Äù https://arxiv.org/abs/2312.13382.\n\n\nSuzgun, Mirac, Nathan Scales, Nathanael Sch√§rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, et al. 2022. ‚ÄúChallenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them.‚Äù arXiv Preprint arXiv:2210.09261.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. ‚ÄúChain-of-Thought Prompting Elicits Reasoning in Large Language Models.‚Äù https://arxiv.org/abs/2201.11903."
  },
  {
    "objectID": "posts/bits_and_bytes/bits_bytes.html",
    "href": "posts/bits_and_bytes/bits_bytes.html",
    "title": "Memory Usage for Quantized LLMS",
    "section": "",
    "text": "Intro\nIn this blog post I will be writing about some of my high level learnings from a recent LLM conference I took part in. These are some notes for myself so I don‚Äôt forget. These concepts are basic and high level but are details I was missing before when thinking about fine-tuning LLMs. Most of the details here were inspired by one of the conference speakers: Jonathan Whitaker‚Äôs ‚Äî&gt; talk.\n\n\nBits and Byte\nYou have probably heard the saying, ‚ÄúIn the computer it‚Äôs all 0‚Äôs and 1‚Äôs‚Äù. Well, those 0‚Äôs and 1‚Äôs are called bits. A bit is the smallest unit of storage and simply stores a 0 or a 1. A byte is a group of 8 bits together.\n\n1 byte = 8 bits.\n\nAll storage is measured in bytes.\n\n\n\nUnit\nAbbreviation\nApproximate Size\n\n\n\n\nKilobyte\nKB\nabout 1 thousand bytes\n\n\nMegabyte\nMB\nabout 1 million bytes\n\n\nGigabyte\nGB\nabout 1 billion bytes\n\n\nTerabyte\nTB\nabout 1 trillion bytes\n\n\n\n\n\nMemory Usage During Inference\nI‚Äôm running this code from a local Jupyter notebook in Pycharm connected to a A6000 running on JarvisLabs. I used the axolotl template.\n\n\nCode\nimport torch\nimport gc\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\ndevice = \"cuda\"\n\n\ndef cleanup():\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats(device)\n\n\ndef print_memory_stats():\n    \"\"\"Print two different measures of GPU memory usage\"\"\"\n    print(f\"Max memory allocated: {torch.cuda.max_memory_allocated(device)/1e9:.2f}GB\")\n    # reserved (aka 'max_memory_cached') is ~the allocated memory plus pre-cached memory\n    print(f\"Max memory reserved: {torch.cuda.max_memory_reserved(device)/1e9:.2f}GB\")\n\n\nprint_memory_stats()\n\ncleanup()\n\n\nMax memory allocated: 0.00GB\nMax memory reserved: 0.00GB\n\n\nLet‚Äôs load the model TinyLlama/TinyLlama-1.1B-Chat-v1.0. By default, it will load in float32.\n\n\nCode\nmodel_ckpt = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nmodel = AutoModelForCausalLM.from_pretrained(model_ckpt, device_map=device)\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\n\nWe can check the dtype of the parameters and indeed see that they are stored in float32.\n\n\nCode\nset([(x.dtype) for x in model.parameters()])\n\n\n{torch.float32}\n\n\nSince the parameters are in float32, we can estimate that each parameter will takes up 32/8=4 bytes of memory. For a 1.1B parameter model that is 4.4gb. Let‚Äôs see if our rough back of the napkin calculation is correct.\n\n\nCode\n!nvidia-smi\n\n\nSat Jun 22 20:45:06 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA RTX A6000               Off | 00000000:1B:00.0 Off |                  Off |\n| 30%   34C    P2              67W / 300W |   4475MiB / 49140MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n\n\n\n\nCode\nprint_memory_stats()\n\n\nMax memory allocated: 4.40GB\nMax memory reserved: 4.40GB\n\n\nYes, that‚Äôs what we thought.\nLet‚Äôs run some inference with the model.\n\n\nCode\ndef inference(messages):\n    tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model.generate(input_ids=tokenized_chat.to(\"cuda\"), max_new_tokens=128, do_sample=False)\n    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])\ninference([{\"role\": \"user\", \"content\": \"How many bytes are in one gigabyte?\"}])\n\n\n&lt;|user|&gt;\nHow many bytes are in one gigabyte? \n&lt;|assistant|&gt;\nYes, there are 1,000,000,000 bytes in a gigabyte (GB).\n\n\nNow let‚Äôs load the model in a lower precision. The model config points to what precision to use.\n\n\nCode\nmodel.config.torch_dtype\n\n\ntorch.bfloat16\n\n\n\n\nCode\ndel model\ncleanup()\nprint_memory_stats()\n!nvidia-smi\n\n\nMax memory allocated: 0.01GB\nMax memory reserved: 0.02GB\nSat Jun 22 20:45:08 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA RTX A6000               Off | 00000000:1B:00.0 Off |                  Off |\n| 30%   35C    P2              91W / 300W |    351MiB / 49140MiB |     51%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n\n\nNow let‚Äôs load the model in bfloat16. Estimating that each parameter will use 16/8=2 bytes of memory. For the same model, it should use roughly half the memory as before, 2.2GB.\n\n\nCode\nmodel = AutoModelForCausalLM.from_pretrained(model_ckpt, torch_dtype=torch.bfloat16, device_map=device)\n\n\n\n\nCode\nprint_memory_stats()\n\n\nMax memory allocated: 2.21GB\nMax memory reserved: 2.32GB\n\n\n\n\nCode\nset([(x.dtype) for x in model.parameters()])\n\n\n{torch.bfloat16}\n\n\n\n\nCode\ninference([{\"role\": \"user\", \"content\": \"How many bytes are in one gigabyte?\"}])\n\n\n&lt;|user|&gt;\nHow many bytes are in one gigabyte? \n&lt;|assistant|&gt;\nYes, there are 1,000,000,000 bytes in a gigabyte (GB).\n\n\nThis is the exact same output we got before. Since most models are currently trained using bfloat16, there‚Äôs no need to use full float32 precision. In this example if we use float32, it won‚Äôt improve inference results compared to bfloat16.\n\n\nCode\ndel model\ncleanup()\nprint_memory_stats()\n\n\nMax memory allocated: 0.01GB\nMax memory reserved: 0.02GB\n\n\nNow let‚Äôs try loading a quantized model using BitsAndBytesConfig to a load a model in 4bit precision. Note that the model weights are stored in 4bit precision but the computations are done in a higher precision. Here we are specifying that the dtype for computations is bf16. During inference, as well as training, the weights of the model are constantly being dequantized (from 4bit to bf16). This can be done for specific layers at a time during forward and backward passes, to keep memory requirements low. The computation happens in a higher precision.\nHere we specify that the model should be loaded in 4bit and the computations done in bf16. Since we are using 4bit we expect each parameter to use 4/8=0.5 bytes so we should be using less than a GB of memory.\n\n\nCode\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    quantization_config=bnb_config,\n    device_map='auto'\n)\n\n\n\n\nCode\nprint_memory_stats()\n\n\nMax memory allocated: 0.84GB\nMax memory reserved: 0.89GB\n\n\nNow we can expect the model inference results to be different, for the same query we used before.\n\n\nCode\ninference([{\"role\": \"user\", \"content\": \"How many bytes are in one gigabyte?\"}])\n\n\n&lt;|user|&gt;\nHow many bytes are in one gigabyte? \n&lt;|assistant|&gt;\nYes, one gigabyte (GB) is equal to 1,073,741,824 bytes. A byte is a unit of information storage in the binary system, which is the basis for digital computing. In binary, each byte has a value of 10, with each bit representing a single binary digit. So, one gigabyte is equivalent to 1,073,741,824 bytes, which is approximately 1,000,000,000 bytes.\n\n\nYou can experiment with different types of quantization.\nIn this next example we load the model using:\n\nNF4 quantization.\nbnb_4bit_use_double_quant which uses a second quantization after the first one.\nbfloat16 for computation\n\n\n\nCode\ndel model\ncleanup()\nprint_memory_stats()\n\n\nMax memory allocated: 0.01GB\nMax memory reserved: 0.02GB\n\n\n\n\nCode\nnf4_config = BitsAndBytesConfig(\n   load_in_4bit=True,\n   bnb_4bit_quant_type=\"nf4\",\n   bnb_4bit_use_double_quant=True,\n   bnb_4bit_compute_dtype=torch.bfloat16\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    quantization_config=nf4_config,\n    device_map='auto'\n)\n\n\n\n\nCode\nprint_memory_stats()\n\n\nMax memory allocated: 0.80GB\nMax memory reserved: 0.84GB\n\n\n\n\nCode\ninference([{\"role\": \"user\", \"content\": \"How many bytes are in one gigabyte?\"}])\n\n\n&lt;|user|&gt;\nHow many bytes are in one gigabyte? \n&lt;|assistant|&gt;\nYes, I can provide you with the answer to your question. A gigabyte (GB) is a unit of measurement for data storage. It is equal to 1,000 bytes. So, 1 GB is equal to 1,000,000,000 bytes.\n\n\nI really like the high level explanation of quantization from this post by Patrick von Platen. In general, when running inference with quantized models the steps are:\n\nQuantize all the weights of the model and load it (for example 4bit).\nPass through the input sequence in bf16.\nDynamically dequantize the weights to bf16 layer by layer during the forward pass\nQuantize the weights back to 4bit after the computation\n\nSo if we want to do \\(Y = X W\\) where \\(W\\) and \\(X\\) are the weights and input sequence respectively, then for each matrix multiplication we do:\n\\(Y = X \\cdot \\text{dequantize}(W)\\) ; \\(\\text{quantize}(W);\\)\nFor this reason, inference is usually not faster when using quantized models. It‚Äôs slower. It is good to remember that quantization is a tradeoff between memory usage and output quality, as well as possibly inference time.\n\n\nMemory Usage During Training\nThis is not something I know a lot about at the moment, so I can‚Äôt go into too much detail. But I can cover some high level basics that I learned recently. There are three main areas which contribute to the memory during training:\n\nModel Parameters\nGradients\nOptimizer State\n\nThere are other things to consider such as the activations which tend to dominate the memory at larger batch sizes and context lengths. But let‚Äôs ignore this to keep this simple and high level for now.\nSuppose we want to fine-tune llama3-8B in bfloat16 with the basic Adam optimizer.\n\nModel Parameters: 2 bytes per parameter for 8B parameters is 16GB\nGradients: To store the gradients for each tunable parameter is 16GB\nOptimizer State: needs 2X the size of the model, to store first/second moments, which is 32GB.\n\nSo you would need at least 64GB to fully fine-tune llama3-8B in bfloat16. What can we do to fine-tune the model with much less memory?\nFirst, we can quantize the model to 4bits. Then llama3-8B would take up 4GB of memory for the model parameters. That is a 4X reduction! But when training we don‚Äôt quantize the trainable parameters or gradients because the training would not converge. Training still needs to happen higher precision. This is where PEFT methods come in handy, such as LORA and QLORA. Let‚Äôs consider QLORA since we are discussing quantization.\nWith QLORA we add a set of trainable adapters whose parameters take up a much smaller percentage of the total model parameters. We can freeze the entire quantized model and keep it in 4bit. We can store the corresponding gradients and optimizer state in higher precision. This is possible because we are only dealing with a very small percentage of the total model parameters that are trainable.\nIn my last blog post I fine-tuned llama3-8B using the axolotl Library. It was configured to use QLORA with the model parameters in 4bit precision. It was using around 15GB of memory during training. There were some spikes due to me loading a model in a different python session, so just ignore those.\n\n\n\nCode\ndel model\ncleanup()\nprint_memory_stats()\n\n\nMax memory allocated: 0.01GB\nMax memory reserved: 0.03GB\n\n\n\n\nInference with Axolotl Fine-Tuned Model\nThis was the corresponding bits and bytes config for my Axolotl fine-tune. As said previously, it was trained using QLORA in 4bit precision.\n\n\nCode\nbnb_llama_config = {\n    \"_load_in_4bit\": True,\n    \"_load_in_8bit\": False,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"bfloat16\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": True,\n    \"llm_int8_enable_fp32_cpu_offload\": False,\n    \"llm_int8_has_fp16_weight\": False,\n    \"llm_int8_skip_modules\": None,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": True,\n    \"load_in_8bit\": False,\n    \"quant_method\": \"bitsandbytes\"\n  }\n\nmodel_ckpt = 'model/checkpoint-1224/'\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nquantized_config = BitsAndBytesConfig(**bnb_llama_config)\nmodel = AutoModelForCausalLM.from_pretrained(model_ckpt, device_map=\"auto\", quantization_config=quantized_config)\nprint_memory_stats()\n\n\n\n\n\nMax memory allocated: 6.05GB\nMax memory reserved: 6.12GB\n\n\n\n\nCode\nmodel.config\n\n\nLlamaConfig {\n  \"_name_or_path\": \"meta-llama/Meta-Llama-3-8B\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": 128001,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 8192,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n    \"bnb_4bit_quant_storage\": \"bfloat16\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": true,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": null,\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.41.1\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\n\n\n\nCode\nmodel.config.torch_dtype\n\n\ntorch.bfloat16\n\n\nRemember, when we run inference:\n\nThe model weights are indeed stored in 4-bit quantized format. This is what allows for the significant reduction in memory usage.\nDuring inference, the weights are dequantized on-the-fly as they are needed for computation. However, it‚Äôs important to note that this dequantization happens in small chunks, not for the entire model at once.\nAs data passes through each layer, the relevant 4-bit weights for that layer are temporarily dequantized to bfloat16.\nThe computation for that layer is then performed using these dequantized bfloat16 weights and the input data (also in bfloat16).\nAfter the computation for a layer is complete, the dequantized weights can be discarded, and the next layer‚Äôs weights are dequantized.\nGiven this information, 6 GB of memory usage for a 4-bit quantized 8B parameter model computing in bfloat16 does indeed sound reasonable. The base 4-bit model takes about 4 GB.\n\nI took a few random tweets about Anthropic‚Äôs new model release Claude 3.5 Sonnet and run it through the fine-tuned model.\n\n\nCode\ntext = \"\"\"&lt;|begin_of_text|&gt;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGenerate a list of interests.\n\n### Input:\nIntroducing Claude 3.5 Sonnet‚Äîour most intelligent model yet.\nThis is the first release in our 3.5 model family.\nSonnet now outperforms competitor models on key evaluations, at twice the speed of Claude 3 Opus and one-fifth the cost.\n-------\nLess than 24 hours since Anthropic released Claude 3.5 Sonnet, and it surpassed GPT-4o.\nHere are 10 wild examples you don't want to miss:\n-------\nRIP ChatGPT?\nAnthropic just released Claude 3.5 Sonnet ‚Äî ChatGPT's biggest competitor.\n12 Wild Examples of what it's capable of:\n-------\nI‚Äôm not as excited about OpenAI's new voice mod anymore. After seeing Anthropic's Sonnet 3.5, I realize that what matters most to me is the model's intelligence. \nI‚Äôll be more excited for the next generation of OpenAI models rather than a voice mod that sounds more human.\n-------\nthe sheer pettiness of anthropic saying \"good evening, sam\" in every single one of their demo videos for sonnet 3.5 sends me üíÄ\nhow many more days will \"sam\" sit on gpt5?\n-------\nIt really seems like Anthropic has scratched and Claude its way to the top.\n-------\nAnthropic is so back. Two things I like the most about Claude-3's release:\n1. Domain expert benchmarks. I'm much less interested in the saturated MMLU & HumanEval. Claude specifically picks Finance, Medicine, and Philosophy as expert domains and report performance. I recommend all LLM model cards to follow this, so that the different downstream applications know what to expect. \n2. Refusal rate analysis. LLMs' overly cautious answers to innocent questions are becoming a pandemic. Anthropic is typically on the ultra safe end of the spectrum, but they recognize the problem and highlight their efforts on it. Bravo! \nI love that Claude dials up heat in the arena that GPT and Gemini dominate. Though keep in mind that GPT-4V, the high water mark that everyone desperately tries to beat, finished training in 2022. It's the calm before the storm.\n\n### Response:\n\"\"\"\n\ninputs = tokenizer(text, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=500, do_sample=True, temperature=1)\n    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=False)[0])\n\n\n&lt;|begin_of_text|&gt;&lt;|begin_of_text|&gt;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGenerate a list of interests.\n\n### Input:\nIntroducing Claude 3.5 Sonnet‚Äîour most intelligent model yet.\nThis is the first release in our 3.5 model family.\nSonnet now outperforms competitor models on key evaluations, at twice the speed of Claude 3 Opus and one-fifth the cost.\n-------\nLess than 24 hours since Anthropic released Claude 3.5 Sonnet, and it surpassed GPT-4o.\nHere are 10 wild examples you don't want to miss:\n-------\nRIP ChatGPT?\nAnthropic just released Claude 3.5 Sonnet ‚Äî ChatGPT's biggest competitor.\n12 Wild Examples of what it's capable of:\n-------\nI‚Äôm not as excited about OpenAI's new voice mod anymore. After seeing Anthropic's Sonnet 3.5, I realize that what matters most to me is the model's intelligence. \nI‚Äôll be more excited for the next generation of OpenAI models rather than a voice mod that sounds more human.\n-------\nthe sheer pettiness of anthropic saying \"good evening, sam\" in every single one of their demo videos for sonnet 3.5 sends me üíÄ\nhow many more days will \"sam\" sit on gpt5?\n-------\nIt really seems like Anthropic has scratched and Claude its way to the top.\n-------\nAnthropic is so back. Two things I like the most about Claude-3's release:\n1. Domain expert benchmarks. I'm much less interested in the saturated MMLU & HumanEval. Claude specifically picks Finance, Medicine, and Philosophy as expert domains and report performance. I recommend all LLM model cards to follow this, so that the different downstream applications know what to expect. \n2. Refusal rate analysis. LLMs' overly cautious answers to innocent questions are becoming a pandemic. Anthropic is typically on the ultra safe end of the spectrum, but they recognize the problem and highlight their efforts on it. Bravo! \nI love that Claude dials up heat in the arena that GPT and Gemini dominate. Though keep in mind that GPT-4V, the high water mark that everyone desperately tries to beat, finished training in 2022. It's the calm before the storm.\n\n### Response:\nLLM model,Anthropic,Claude,GPT,OpenAI,intelligence,competitor,ChatGPT,benchmark,expert domains,refusal rate,training,spectrum,GPT-4V&lt;|end_of_text|&gt;\n\n\n\n\nConclusion\nAlthough my understanding of fine-tuning LLMs and memory usage is pretty high level, it‚Äôs making a lot more sense then it did before. I‚Äôm happy to have these notes to refer back to as I continue to learn about this topic.\n\n\nResources/Links\nBits and Bytes Basics\nA Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes\nMaking LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA\nOptimizing your LLM in production\nAccelerating Large Language Models with Mixed-Precision Techniques\nUnderstanding how big of a model can fit on your machine"
  },
  {
    "objectID": "posts/intro_modal/intro_modal.html",
    "href": "posts/intro_modal/intro_modal.html",
    "title": "Using Modal to Transcribe YouTube Videos with Whisper",
    "section": "",
    "text": "Intro\nI have been following Modal for a while but never used it until last week. I recently wrote a blog post on using Axolotl to fine-tune a decoder LLM for the first time. For that work I needed to transcribe some YouTube podcasts with OpenAI Whisper model. I figured it would be a cool use case for Modal and a chance to learn something new.\nI‚Äôm no stranger to parallel processing with Python. I have used celery extensively with a Redis backend in production for doing all sorts of things, including deployment of ML models. It‚Äôs actually given me a good understanding of tasks, queues, concurrency, CPU/MEM usage, Redis, and so on. But I didn‚Äôt want to have to deal with all that for this personal project. I didn‚Äôt want to think about infrastructure, Kubernetes, helm charts, node groups, ec2 instances, etc. I just wanted to write some Python code and get the job done.\nIn this post I will describe how I used Modal to accomplish this task. I‚Äôm no expert in using Modal because this is the first project I used it on. But I did love every bit of it. Honestly, Modal is amazing. It‚Äôs pure magic. I can not wait to use it for my next project.\n\n\nSetup\nFirst I signed up and logged in via GitHub. Then I simply followed the instructions on screen.\n\nI created a virtual env and installed modal as well as python-dotenv and pytube.\npython3 -m venv env\nsource env/bin/activate\npip install python-dotenv\npip install pytube\npip install modal\npython3 -m modal setup\n\n\nTask Description\nThe Modal docs are amazing, so I‚Äôm not going to repeat that here. Just go read the docs and try some of the simple hello world type of examples.\nFor the task of transcribing a YouTube video with Whisper I want a Python function that will do the following:\n\ntakes a YouTube video url as input ‚Äì&gt; https://www.youtube.com/watch?v=&lt;video_id&gt;.\nchecks if the url i.e.¬†&lt;video_id&gt; has already been processed by checking if the transcript json file &lt;video_id&gt;.json is already in s3. If so, then exit the function.\nsave the audio as a mp4 file to s3: s3://&lt;s3-bucket&gt;/youtube_downloader/audio_files/&lt;video_id&gt;.mp4.\nsave audio transcript to s3: s3://&lt;s3-bucket&gt;/youtube_downloader/transcripts/&lt;video_id&gt;.json.\n\nThen I can use the Modal map feature to fan out this function/task in parallel.\n\n\nShow me the Code\nThere are various ways to use secrets in Modal. One simple way is through dotenv. I defined some environment variables in a local .env file.\nS3_BUCKET=&lt;my-s3-bucket&gt;\nS3_PREFIX=youtube_downloader/\nAWS_ACCESS_KEY_ID=&lt;my access key&gt;\nAWS_SECRET_ACCESS_KEY=&lt;my secret access key&gt;\nHere is the code in a file transcribe_video.py:\nimport os\nimport io\nimport json\nimport modal\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nS3_BUCKET = os.environ[\"S3_BUCKET\"]\nS3_PREFIX = os.environ[\"S3_PREFIX\"]\nstub = modal.Stub(\"video-transcription\")\n\nimage = (\n    modal.Image.debian_slim(python_version=\"3.10\")\n    .run_commands(\"apt-get update\", \"apt update && apt install ffmpeg -y\")\n    .pip_install(\n        \"openai-whisper\",\n        \"pytube\",\n        \"boto3\",\n        \"python-dotenv\",\n    )\n)\n\n\ndef upload_file(filename, s3_filename, bucket_name):\n    import boto3\n\n    client = boto3.client(\"s3\")\n    headers = {\"ACL\": \"public-read\"}\n    headers[\"CacheControl\"] = \"max-age %d\" % (3600 * 24 * 365)\n    client.upload_file(filename, bucket_name, s3_filename, ExtraArgs=headers)\n    return f\"https://{bucket_name}.s3.amazonaws.com/{s3_filename}\"\n\n\ndef check_file(filename, bucket_name):\n    import boto3\n    from botocore.errorfactory import ClientError\n\n    client = boto3.client(\"s3\")\n    file_exists = True\n    try:\n        client.head_object(Bucket=bucket_name, Key=filename)\n    except ClientError:\n        file_exists = False\n    return file_exists\n\n\ndef upload_fileobj(file_object, s3_filename, bucket_name):\n    import boto3\n\n    client = boto3.client(\"s3\")\n    headers = {\"ACL\": \"public-read\"}\n    headers[\"CacheControl\"] = \"max-age %d\" % (3600 * 24 * 365)\n    client.upload_fileobj(file_object, bucket_name, s3_filename, ExtraArgs=headers)\n    return f\"https://{bucket_name}.s3.amazonaws.com/{s3_filename}\"\n\n\ndef dict_to_s3(record, s3_filename, bucket_name):\n    in_mem_file = io.BytesIO()\n    in_mem_file.write(json.dumps(record, sort_keys=True, indent=4).encode())\n    in_mem_file.seek(0)\n    upload_fileobj(in_mem_file, s3_filename, bucket_name)\n    return f\"https://{bucket_name}.s3.amazonaws.com/{s3_filename}\"\n\n\n@stub.function(\n    image=image,\n    secrets=[modal.Secret.from_dotenv()],\n    cpu=2,\n    memory=1024 * 3,\n    gpu=\"A10G\",\n    timeout=600,\n)\ndef process_video(url):\n    import re\n    from pytube import YouTube\n    import whisper\n\n    video_id = re.search(r\"v=([a-zA-Z0-9_-]{11})\", url).group(1)\n    file_name_audio = f\"{video_id}.mp4\"\n    s3_file_name_audio = os.path.join(S3_PREFIX, \"audio_files\", file_name_audio)\n    s3_file_name_transcript = os.path.join(S3_PREFIX, \"transcripts\", f\"{video_id}.json\")\n    if check_file(s3_file_name_transcript, S3_BUCKET):\n        print(f\"Already processed {s3_file_name_audio}. Skipping.\")\n        return\n\n    yt = YouTube(url)\n    audio_stream = yt.streams.get_audio_only()\n    audio_stream.download(filename=file_name_audio)\n    upload_file(file_name_audio, s3_file_name_audio, S3_BUCKET)\n\n    # transcribe video\n    model = whisper.load_model(\"small\", device=\"cuda\")\n    audio = whisper.load_audio(file_name_audio)\n    # audio = whisper.pad_or_trim(audio)  # useful for debugging\n    result = model.transcribe(audio, fp16=True)\n    # for debugging in ipython shell\n    # modal.interact()\n    # import IPython\n    # IPython.embed()\n    return dict_to_s3(result, s3_file_name_transcript, S3_BUCKET)\n\n\n@stub.local_entrypoint()\ndef main():\n    from pytube import Playlist\n    import random\n\n    all_urls = set()\n    for playlist_url in [\n        \"https://www.youtube.com/playlist?list=PL8xK8kBHHUX4NW8GqUsyFhBF_xCnzIdPe\",\n        \"https://www.youtube.com/playlist?list=PL8xK8kBHHUX7VsJPqv6OYp71Qj24zcTIr\",\n        \"https://www.youtube.com/playlist?list=PL8xK8kBHHUX5X-jGZlltoZOpv5sKXeGVV\",\n    ]:\n        p = Playlist(playlist_url)\n        for url in p.video_urls:\n            all_urls.add(url)\n\n    urls = random.sample(list(all_urls), 20)\n    for msg in process_video.map(urls):\n        print(msg)\nYou can execute it with Modal using modal run transcribe_video.py.\n\nI think there is more optimizations that could be done such as caching the model downloading but I have not looked into it.\nSome comments on the code:\n\ndefine your own custom container image and install whatever you like on it.\nfor the task function process_video we simply decorate it with @stub.function and we can specify things such as the image, mem/cpu resources, secrets etc. In this case we can even run it on an A10 GPU for faster inference with Whisper. I increased the timeout because I was transcribing longer videos.\nI am using from pytube import Playlist to list out the video urls in a given YouTube playlist within the main function. That logic runs on my local machine. You can add whatever logic you want there, different video urls, etc. The main idea is that process_video takes the video url as input. You can increase the sample size in urls = random.sample(list(all_urls), 20) to whatever you like. This code, modal run transcribe_video.py, can be run over and over again and any video that was already transcribed and saved to s3 will be skipped automatically.\nprocess_video.map(urls): fans out the tasks and it runs entirely on Modal‚Äôs cloud infrastructure. Not locally!\n\nThe output is saved to s3.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJumping into Ipython Shell for Interactive Debugging\nYou can even jump into the ipython shell on the server at any point in the function logic to debug. That is pretty cool! Just remove the comments:\n# for debugging in ipython shell\nmodal.interact()\nimport IPython\nIPython.embed()\nI also removed the comment # audio = whisper.pad_or_trim(audio)  # useful for debugging during debugging so the transcription was just for 30 seconds and hence the code runs much faster.\nThen just run the code in interactive mode with modal run -i transcribe_video.py\nI did this several times to check the output of result = model.transcribe(audio, fp16=True). I also debugged the files being saved locally on the image container. It gave the same feeling as developing locally but it was all running in the cloud! I did not have to get ffmpeg and OpenAI Whisper running locally on a GPU or mess around with cuda drivers. It just worked, surprisingly. Here are some screenshots of the interactive ipython shell:\n\n\n\n\nConclusion\nI hope this got you excited to go and try out Modal. They actually give you $30 of compute each month. And the dashboard is great. It can show you detailed logs, resource consumption, and the billing is very transparent (updated in real time)."
  },
  {
    "objectID": "posts/fine_tune_jarvis/fine_tune_jarvis.html",
    "href": "posts/fine_tune_jarvis/fine_tune_jarvis.html",
    "title": "Fine-Tuning LLMs with Axolotl on JarvisLabs",
    "section": "",
    "text": "Intro\nI‚Äôm currently taking part in the course/conference Mastering LLMs: A Conference For Developers & Data Scientists. It started off as a fine-tuning course and then kind of exploded into a mini conference with numerous speakers and talks on LLMs. Many of the organizations involved gave out free credits to try out their platform. Vishnu was very generous and gave out some free credits for JarvisLabs.ai.\nI previously wrote about my first time using axolotl here for text completion (not instruction). In this post I will be going through fine-tuning an instruction model with axolotl on JarvisLabs.ai.\n\n\nJarvisLabs Setup\n\nJarvisLabs is a cloud provider for GPUs.\nThe docs are great.\nThey even have templates for getting up and running quickly. I am using the axolotl template.\n\nWhen you create an instance, you can easily use Jupyter or the terminal directly in the browser.\n \nYou can also ssh in from a local terminal which is what I prefer to do.\nI created an ssh key on my local machine:\nssh-keygen -t ed25519 -C \"&lt;my_email.com&gt;\" -f ~/.ssh/jarivs_labs\ncat ~/.ssh/jarivs_labs.pub\nI copied it into the JarvisLabs ssh settings as explained in their FAQ.\nNote that the ssh details can be seen by clicking on the ssh button on the running instance. They may be slightly different from this particular example.\nssh -i ~/.ssh/jarivs_labs -o StrictHostKeyChecking=no -p 11314 root@sshf.jarvislabs.ai\nOnce connected, these are the commands I run to set up the environment:\ncd /workspace/axolotl/\ngit pull\nThen from axolotl readme.\npip3 install packaging ninja\npip3 install -e '.[flash-attn,deepspeed]'\nI also find it useful to define these:\ngit config --global credential.helper store\nexport HF_DATASETS_CACHE=\"/workspace/data/huggingface-cache/datasets\"\nexport HUGGINGFACE_HUB_CACHE=\"/workspace/data/huggingface-cache/hub\"\nexport TRANSFORMERS_CACHE=\"/workspace/data/huggingface-cache/hub\"\nexport HF_HOME=\"/workspace/data/huggingface-cache/hub\"\nexport HF_HUB_ENABLE_HF_TRANSFER=\"1\"\nConnect wandb and huggingface-cli.\nwandb login\nhuggingface-cli login\n\n\nAxolotl Config\nThis is the axolotl config which I made edits to and stored at examples/llama-3/qlora.yml.\nbase_model: meta-llama/Meta-Llama-3-8B\nmodel_type: AutoModelForCausalLM\ntokenizer_type: AutoTokenizer\n\nload_in_8bit: false\nload_in_4bit: true\nstrict: false\n\ndatasets:\n  - path: data.jsonl\n    type: alpaca\ndataset_prepared_path:\nval_set_size: 0.05\noutput_dir: qlora-test\n\nsequence_len: 2048\nsample_packing: true\neval_sample_packing: False\npad_to_sequence_len: true\n\nadapter: qlora\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_modules:\nlora_target_linear: true\nlora_fan_in_fan_out:\n\nwandb_project: synthetic-social-llama3\nwandb_entity:\nwandb_watch:\nwandb_name: seq-length-2048-lr-0.0002-r-32-alpha-16\nwandb_log_model:\n\ngradient_accumulation_steps: 4\nmicro_batch_size: 2\nnum_epochs: 2\noptimizer: paged_adamw_32bit\nlr_scheduler: cosine\nlearning_rate: 0.0002\n\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: auto\nfp16:\ntf32: false\n\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\n\nwarmup_steps: 10\nevals_per_epoch: 2\neval_table_size:\nsaves_per_epoch: 1\ndebug:\ndeepspeed:\nweight_decay: 0.0\nfsdp:\nfsdp_config:\nspecial_tokens:\n  pad_token: \"&lt;|end_of_text|&gt;\"\n\nI simply copy/pasted the above into the existing yaml from the repo.\nrm -rf examples/llama-3/qlora.yml\nvim examples/llama-3/qlora.yml\n\n\nPreparing and Debugging the Processed Dataset\nI created a synthetic dataset generated with another LLM. Both the inputs and labels are synthetic and generated with an LLM. You can find it here. It‚Äôs just meant for simple experimentation and playing around. I stored it as a data.jsonl file on the JarvisLabs instance at /workspace/axolotl/data.jsonl.\nThe idea of this dataset and fine-tuning a model from it is simple. You provide a list of social media posts from a user. Then you can ask the model to either generate a list of interests/keywords or write a summary based on the input posts. Note that this is a very simple task which can be achieved with existing closed or open source instruction models. So technically fine-tuning is not required for this task in most scenarios. But this exercise is for educational purposes so let‚Äôs continue.\nWe process the dataset as recommended by the axolotl docs. My dataset is in the alpaca format.\nrm -rf last_run_prepared/\nCUDA_VISIBLE_DEVICES=\"\" python -m axolotl.cli.preprocess examples/llama-3/qlora.yml --debug\n \nThis screenshot shows the output of the dataset preprocess step when running in --debug mode. The red tokens are not included in the loss calculation and the green ones are. This is because we are using train_on_inputs: false. This means the training does not consider the user submitted inputs when calculating the loss.\nThe processed dataset is in the default last_run_prepared folder because I did not specify a different location in the yaml configuration. We can debug the pre-processed dataset more in the ipython shell.\nfrom transformers import AutoTokenizer\nfrom datasets import load_from_disk\nimport yaml\n\ndirectory = !ls last_run_prepared/\nwith open('examples/llama-3/qlora.yml', 'r') as f:\n    cfg = yaml.safe_load(f)\nmodel_id = cfg['base_model']\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nds = load_from_disk(f'last_run_prepared/{directory[0]}/')\n\nrow = ds[4552] # can pick various rows to test\nprint(tokenizer.decode(row['input_ids']))\nHere is an example record in full where the task is to generate a list of interests:\n&lt;|begin_of_text|&gt;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGenerate a list of interests\n\n### Input:\nExploring architectural wonders: Casa Malaparte on the Isle of Capri, Italy. Designed by Adalberto Libera in 1937. A true masterpiece of modernist architecture. #casamalaparte #adalbertolibera #architecturalwonders\n-------\nVintage charm: The Miller House in Lexington, Kentucky, designed by Jos√© Oubrerie in the 1980s. A unique blend of contemporary design and historic influences. #millerhouse #joseoubrerie #vintagecharm\n-------\nA glimpse into the past: The Eames House in Pacific Palisades, California, 1949. A landmark of mid-century modern architecture and a symbol of creativity and innovation. #eameshouse #midcenturymodern #designinspiration\n-------\nArchitectural serenity: The Bruder Klaus Field Chapel in Mechernich, Germany, designed by Peter Zumthor in 2007. A place of contemplation and spiritual connection with nature. #bruderklausfieldchapel #peterzumthor #architecturalserenity\n-------\nThe poetic beauty of Casa Gilardi in Mexico City, Mexico, designed by Luis Barrag√°n. Every corner of this house is a work of art that celebrates light, color, and form. #casagilardi #luisbarragan #poeticbeauty\n-------\nRediscovering architectural gems: The Lovell Beach House in Newport Beach, California, 1926. A pioneering example of modern residential architecture by Rudolph Schindler. #lovellbeachhouse #rudolphschindler #architecturalgems\n-------\nMinimalist elegance: The Mies van der Rohe Pavilion in Barcelona, Spain. An iconic modernist structure that exemplifies the essence of simplicity and sophistication in design. #miesvanderrohe #minimalistelegance #barcelonapavilion\n-------\nSustainable design spotlight: The Adam Joseph Lewis Center in Oberlin, Ohio, designed by William McDonough in 2000. A visionary example of environmentally conscious architecture. #adamjosephlewiscenter #williammcdonough #sustainabledesign\n-------\nArchitectural harmony: Fallingwater in Pennsylvania, designed by Frank Lloyd Wright in 1935. A masterpiece that seamlessly integrates with its natural surroundings. #fallingwater #franklloydwright #architecturalharmony\n-------\nThe ethereal grace of the Teshima Art Museum in Japan, designed by Ryue Nishizawa in 2010. A place where architecture, art, and nature converge to create a transcendent experience. #teshimaartmuseum #ryuenishizawa #etherealgrace\n-------\nUnveiling design treasures: The Unit√© d'Habitation in Marseille, France, designed by Le Corbusier in the 1950s. A revolutionary housing complex that redefined urban living concepts. #unitehabitation #lecorbusier #designtreasures\n-------\nCultural heritage in focus: The Sheikh Lotfollah Mosque in Isfahan, Iran, a masterpiece of Safavid architecture from the 17th century. A symbol of intricate beauty and Islamic artistry. #sheikhlotfollahmosque #isfahan #culturalheritage\n-------\nArchitectural splendor: The Elbphilharmonie in Hamburg, Germany, designed by Herzog & de Meuron. A modern marvel that harmoniously combines old and new architectural elements. #elbphilharmonie #herzogdemeuron #architecturalsplendor\n-------\nExploring urban landscapes: The Metropol Parasol in Seville, Spain, affectionately known as 'The Mushrooms.' A contemporary architectural marvel that transforms public spaces with innovative design. #metropolparasol #seville #urbanlandscapes\n-------\nEnchanting design: The Biblioteca Vasconcelos in Mexico City, a cultural institution with a breathtaking blend of contemporary architecture and artistic expression. #bibliotecavasconcelos #mexicocity #enchantingdesign\n-------\nArchitectural evolution: The High Line in New York City, a groundbreaking urban park built on a historic elevated rail line. A symbol of innovative adaptive reuse in contemporary architecture. #highline #newyorkcity #architecturalevolution\n-------\nTimeless elegance: The Kimbell Art Museum in Fort Worth, Texas, designed by Louis Kahn in the 1970s. A space where architecture serves as a silent partner to art, creating a harmonious environment. #kimbellartmuseum #louiskahn #timelesselegance\n-------\nArchitectural symmetry: The Pantheon in Rome, Italy, a marvel of ancient Roman engineering and design. A timeless masterpiece that continues to inspire architects and visitors alike. #pantheon #rome #architecturalsymmetry\n-------\nDesigning with nature: The Salk Institute in La Jolla, California, designed by Louis Kahn. A sanctuary of research and contemplation where architecture and landscape coexist in perfect harmony. #salkinstitute #louiskahn #designwithnature\n-------\nIconic modernism: The Farnsworth House in Plano, Illinois, designed by Mies van der Rohe in 1951. A glass pavilion that blurs the line between interior and exterior, embodying the essence of modernist design. #farnsworthhouse #miesvanderrohe #iconicmodernism\n-------\nExploring the architectural masterpiece that is the Casa Malaparte in Capri, Italy. Designed by Curzio Malaparte in 1937. üèõÔ∏è #casamalaparte #curziomalaparte #italianarchitecture\n-------\nCapturing the essence of mid-century modern design at the iconic Stahl House (Case Study House #22) in Los Angeles by architect Pierre Koenig. üåü #stahlhouse #casestudyhouse #midcenturymodern\n-------\nA glimpse of the minimalist beauty of the Teshima Art Museum on the island of Teshima, Japan. Designed by Ryue Nishizawa and Rei Naito in 2010. üé® #teshimaartmuseum #japanesearchitecture\n-------\nVintage vibes at the Sheats-Goldstein Residence in Beverly Hills, a landmark of organic architecture designed by John Lautner in 1963. üåø #sheatsgoldsteinresidence #johnlautner #organicarchitecture\n-------\nStep inside the breathtaking geometric wonderland of the Salk Institute in La Jolla, California, designed by Louis Kahn. A true architectural marvel! üî∑ #salkinstitute #louiskahn #geometricarchitecture\n-------\nReflecting on the enchanting play of light and shadows at the Naoshima Bath ''I‚ô•ÊπØ (I Love Yu)'' in Japan, designed by artist Shinro Ohtake. üõÅ #naoshimabath #shinroohtake #artinstallation\n-------\nAn homage to the timeless elegance of Greta Magnusson Grossman's Grasshopper Floor Lamp, an icon of modern design from the mid-20th century. ü¶óüí° #grasshopperlamp #gretamagnussongrossman #modernlighting\n-------\nDiscovering the organic forms and natural materials of the Bavinger House in Norman, Oklahoma, a visionary creation by architect Bruce Goff in 1955. üåø #bavingerhouse #brucegoff #organicarchitecture\n\n### Response:\narchitecture,architectural wonders,design,mid-century modern,modernist,contemporary design,historic influences,sustainable design,urban architecture,natural elements,cultural heritage,minimalist elegance,architectural harmony,artistic expression,architectural evolution,symmetry,organic architecture,iconic modernism,timeless elegance,geometric architecture,light and shadows,play of light,natural materials,innovative design,architectural splendor,aesthetic,architecture and nature,architectural gems,spiritual connection,sustainable architecture,artistic beauty,innovative architecture,creative design,historical architecture,urban landscapes,design treasures,aesthetic appeal,architectural marvels,pioneering architecture,visual beauty&lt;|end_of_text|&gt;\nWhen we look at the row['labels'] we can see many of the entries are -100. This means we will ignore these outputs when doing the loss calculation during training.\nYou can decode just the output of row['labels'] where the token is not -100.\nprint(tokenizer.decode([r for r in row['labels'] if r != -100]))\nThis returns just the output of the record after ### Response:. You can also do some sanity checks to make sure the input tokens and label tokens match up:\nfor input_token, output_token in zip(row['input_ids'], row['labels']):\n    if output_token != -100:\n        print(input_token, output_token)\n        assert input_token == output_token\nDuring training the output tokens will be right shifted so that the model learns to predict the next token.\nIt‚Äôs really important to look through the data to make sure everything lines up and there are no weird tokenization errors. Here is another example training record of the type where the instruction says to generate a summary.\nrow = ds[3684] # can pick various rows to test\nprint(tokenizer.decode(row['input_ids']))\n&lt;|begin_of_text|&gt;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGenerate a summary.\n\n### Input:\nChasing waves and sunsets, living for the moments that take your breath away üåÖüåäüèÑ #sunsetchasers\n-------\nThe ocean is my therapist, saltwater my remedy, surfing my meditation üåäüßòüôè #surftherapy\n-------\nSalty hair, sandy toes, and endless waves - this is where I belong üèÑüåäüåû #oceanchild\n-------\nDancing with the waves under the golden sun, feeling alive and free üåûüåäüíÉ #soulfulsurfing\n-------\nSurfing is not just a sport, it's a way of life - a journey of self-discovery and pure joy üèÑüå∫üåü #surferforlife\n-------\nBeneath the surface of the ocean, I find serenity and a connection to something greater üåäüßú‚Äç‚ôÄÔ∏èüåå #deepblue\n-------\nRiding the rhythm of the waves, each crest and trough a heartbeat of nature's symphony üèÑüé∂üåä #harmony\n-------\nSun-kissed and salt-soaked, my happy place is where the sea meets the shore üåûüèñÔ∏èüåä #beachlife\n-------\nIn the dance between surfboard and wave, I find grace, balance, and pure exhilaration üèÑüí´üåä #flowstate\n-------\nSandy cheeks, gleaming smiles, and endless laughter - the perfect day by the sea üåäüòÅüèñÔ∏è #goodvibesonly\n-------\nLet the waves carry you to new horizons, where the possibilities are as vast as the ocean itself üåäüåÖüö£ #exploreandflow\n-------\nSurfing at dawn, greeted by the rising sun and the promise of a day filled with adventure üèÑüåÖ‚òÄÔ∏è #earlymorningmagic\n-------\nSaltwater runs through my veins, adventure in my heart, and the spirit of the sea in my soul üåä‚ù§Ô∏èüåû #oceanvibes\n-------\nAmong the waves, I find clarity, purpose, and a sense of belonging that words cannot capture üèÑüåäüåå #surfingbliss\n-------\nEvery wave tells a story, and I am just a chapter in the never-ending narrative of the sea üìñüåäüèÑ #saltystories\n-------\nBreathing in the salty air, feeling the energy of the ocean, and being present in every moment üåäüå¨Ô∏èüôè #mindfulsurfing\n-------\nFrom dawn 'til dusk, chasing waves and memories that will last a lifetime üèÑüåÑüåå #adventureseeker\n-------\nThe thrill of the drop, the rush of the ride, and the sheer joy of being one with the ocean üåäüèÑüòÑ #surfingbliss\n-------\nWhere the sea meets the sky, I find freedom, peace, and a sense of infinite possibility üåä‚òÅÔ∏èüåà #limitless\n-------\nMorning vibes with a cup of @rockstarenergy to kickstart the day üåû‚òïÔ∏è\n-------\nChasing waves in Bali with @rockstarenergy üèÑüåä #SurfLife\n-------\nShredding through Indo tunnels with a unique 5'2 board shaped by @mike_andrews3 üèÑüå™ #SurfingAdventures\n-------\nPanama jumps with style in my favorite @hurley gear üå¥üì∏ #SurfingParadise\n-------\nExcited to be part of the new series by @hurley featuring some rad surfers ü§ôüåä #Stoked\n-------\nGoPro footage of an epic wave session posted by @stab and @hurley üìΩÔ∏èüåä #WaveRider\n-------\nSunset session at Lower Trestles with @rockstarenergy üåÖüèÑ‚Äç‚ôÇÔ∏è #SurfSunRepeat\n-------\nCelebrating my little girl's moment of fame with joy and pride ‚ù§Ô∏èüåü #ProudDad\n-------\nRevamped a surfboard design for @parkercoffin with a fiery twist üî•üèÑ #UniqueBoard\n-------\nCheers to adventure-filled days with @smithoptics üçªüö£‚Äç‚ôÇÔ∏è #ThrillSeeker\n-------\nRiding glorious mush burgers in tropical paradise üçîüå¥üèÑ #TropicalVibes\n-------\nLet's rock it with @rockstarenergy on this awesome day! ü§òüèÑ #RockStarLife\n-------\nSurf's up with peace and love in every wave üèÑüí¶‚úåÔ∏è #SurfLife\n-------\nCapturing the beauty of the ocean with @goltershot üì∏üåäüêï #OceanLove\n-------\nWinter vibes with high waves and endless energy üåä‚ùÑÔ∏èüöÄ #WinterSurfing\n-------\nGrateful for epic surf shots by @gromarazzi üôèü§ô #SurfPhotography\n-------\nSurf bug bit my little pug! üèÑüê∂‚ù§Ô∏è #SurfDog\n-------\n#NationalSunglassesDay vibes with stylish shades from @smithoptics üòéüåû #EyeProtection\n-------\nLopey showing off his skills with @rockstarenergy üì∏üèÑü§ô #SkillsOnPoint\n-------\nEmbracing the surf lifestyle with @surfinglens üåéüèÑüåä #SurfLife\n-------\nDouble dose of fun with @rockstarenergy üèÑü§ôüåä #SurfAdventures\n-------\nAdventurous jumps caught on camera by @jacobvanderwork üåäüèÑüé• #EpicMoments\n-------\nLayback ride with a twist of whiplash captured brilliantly by @jacobvanderwork üèÑü§° #SurfLife\n-------\nMoment before diving into the North Pacific captured by @muirman üåäüèÑüì∏ #SurfingThrills\n-------\nBeach day fun with Kingy under the sun ‚òÄÔ∏èüèñü§ô #BeachLife\n-------\nSurf ride alongs with the legends @kellyslater and Kingy üèÑü§£üåä #SurfLegends\n-------\nEven on a bad day, there's always room for humor üòÜ @ian_crane coronakingsley #ChinUp\n-------\nEpic surf moments with @rockstarenergy captured by @jacobvanderwork üåäüèÑüí• #EpicSurfing\n-------\nWhen you try something your dad recommended... and regret it! ü§¢üôà #DadAdvice\n-------\nCorona messing with @miguelpuposurf's surf setup! üç∫üåä #CoronaWoes\n-------\nFamily moments with the Mrs and the kids üíëüë®‚Äçüë©‚Äçüëß‚Äçüë¶ #FamilyLove\n-------\nUnexpected struggles with a board choice... lessons learned! üí©üò± #SurfStruggles\n-------\nRocking the trend with Birkenstock buddies! ü¶∂üå¥ #FashionForward\n-------\nWherefore art thou, Dillon? üïµÔ∏è‚Äç‚ôÇÔ∏è #MissingPerson\n-------\nHappy birthday shoutout to the awesome @brettsimpson üéâüéÇ #BirthdayVibes\n-------\n#50ShadesOfGay - celebrating love diversity! üè≥Ô∏è‚Äçüåà‚ù§Ô∏è #LoveIsLove\n\n### Response:\nI am an avid surfer and ocean enthusiast, finding solace and joy in the waves. Surfing is not just a hobby for me; it's a way of life that brings me clarity, purpose, and a deep connection to the sea. I often share my surfing adventures and experiences, capturing the beauty of the ocean through social media posts. Beyond surfing, I enjoy spending time with family, embracing moments of humor, and celebrating love and diversity. Adventure, joy, and a deep appreciation for the natural world define my lifestyle, and I find happiness in sharing these passions with others.&lt;|end_of_text|&gt;\n\n\nTraining the Model\nNow that we have verified that the pre-processed dataset is all good, we can kick off the training.\nrm -rf qora-test/ # this is where the final model checkpoints are stored\naccelerate launch -m axolotl.cli.train examples/llama-3/qlora.yml\nYou will see output to the terminal, and you can also follow along with Weights & Biases.\n \nHere is a public Weights & Biases link to the training run.\n\n\nInference with the Model\nWhile the GPU is still running, hop in there and run some inference tests with the trained model.\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\nimport json\n\ndef read_json_file(file_path):\n    with open(file_path, 'r') as f:\n        return json.load(f)\n    \nconfig = read_json_file('qlora-test/config.json')\nprint(config)\n\n\nmodel_ckpt = 'qlora-test/checkpoint-1224/'\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nquantized_config = BitsAndBytesConfig(**config['quantization_config'])\nmodel = AutoModelForCausalLM.from_pretrained(model_ckpt, device_map=\"auto\", quantization_config=quantized_config)\n\n\ntext = \"\"\"&lt;|begin_of_text|&gt;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGenerate a list of interests.\n\n### Input:\nExplore the fascinating world of probiotics and their potential to revolutionize your health. Uncover the science behind probiotics, their benefits, and how they can empower your gut microbiome. Click the link in our bio to get started on your probiotic journey!\n-------\nDid you know that a healthy gut microbiome is pivotal for overall wellness? üåü Good gut bacteria and fungi play a crucial role in maintaining a strong immune system and digestive health. Learn more about how you can boost your gut health by visiting the link in our bio! #GutHealth #HealthyMicrobiome\n-------\nSwap out sugary drinks for the goodness of Super Greens! ü•¨ Your gut will thank you for this positive change. Feel refreshed and rejuvenated from within. Ready to make the switch? Shop Super Greens via the link in our bio! #SuperGreens #GutHealth\n-------\nLooking to diversify your gut microbiome? Look no further! üíö BIOHM Super Greens offers a blend of 34 powerful greens along with probiotics and prebiotics. Enhance your gut health with just a single scoop a day! Click the link in our bio to get your hands on Super Greens today! #BIOHM #SuperGreens\n-------\nSavor the flavors of a tangy cilantro lime dressing over a colorful veggie burrito bowl! üòã This gut-friendly dish is not just a treat to the taste buds but also a feast for your gut microbiome. Ready to whip up this delightful recipe? Find it in our bio! #HealthyRecipe #GutFriendly\n-------\nDiscover the simplicity and benefits of BIOHM's products! üåø Your gut deserves the best, and BIOHM delivers just that - pure goodness without any junk. Start your journey towards better gut health today! Tap the link in our bio to explore our range! #BIOHM #GutWellness\n-------\nInvest in your health because health truly is wealth! üí∞üíö Prioritize your well-being and kickstart your gut health journey. It's never too late to begin - take that step towards a healthier you today! Visit our bio to shop for your gut health essentials! #HealthIsWealth #GutHealth\n-------\nMorning routines can impact your digestive health significantly! Start your day right with these 3 tips for better mornings: Deep breathing, hydrate with a glass of water, and resist the snooze button. Try these tips and witness the difference! For more morning hacks, click the link in our bio! #MorningRoutine #HealthyHabits\n-------\nGive your gut the love it deserves with BIOHM's Total Probiotic! üíö Experience reduced bloating, improved gut function, and overall digestive wellness. Join the journey towards a happier gut today! Check out the link in our bio to shop BIOHM Total Probiotic! #Probiotics #GutHealth\n-------\nListening to your body is key to holistic wellness! üåø‚ú® Stay attuned to your body's signals and nurture it with care. Your body knows best when it comes to its needs. Trust the process of healing and listen closely to what your body tells you. #HolisticWellness #ListenToYourBody\n-------\nIndecisive between Super Greens and Super Reds? Why not have both! ‚ù§Ô∏èüíö Enjoy the fantastic health benefits offered by these superfoods. Mix them together for a powerhouse of nutrients that your body will love. Curious to try our Super Food bundle? Visit our bio to get yours! #SuperFoods #Nutrition\n-------\nUnravel the importance of digestive enzymes for optimal nutrient absorption! üåü These enzymes are vital for breaking down carbohydrates, proteins, and fats, ensuring your body absorbs all the goodness from your meals. Learn all about digestive enzymes by clicking the link in our bio! #DigestiveHealth #NutrientAbsorption\n\n### Response:\n\"\"\"\n\ninputs = tokenizer(text, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=500, do_sample=True, temperature=1)\n    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=False)[0])\nThis returns\nprobiotics,gut microbiome,gut health,Super Greens,healthy diet,BIOHM,superfoods,healthy recipes,gut wellness,morning routines,holistic wellness,superfoods bundle,digestive health,nutrient absorption&lt;|end_of_text|&gt;\nAnd if we replace the instruction with Generate a summary. in the above text variable we get something like:\nI am passionate about holistic health and wellness, especially when it comes to gut health and the importance of a healthy microbiome. I love exploring the latest research and sharing tips on how to improve gut health through probiotics, Super Greens, and other gut-friendly foods. My focus is on promoting holistic wellness and helping others prioritize their health for a better overall well-being. I believe in the power of small changes like switching to healthier drinks and morning routines for a significant impact on digestive health. Join me on this journey towards a happier, healthier gut and a happier you!&lt;|end_of_text|&gt;\n\n\nConclusion\nI‚Äôm excited to continue learning more about axolotl, JarvisLabs, fine-tuning LLMs, and all the other great content in this Mastering LLMs: A Conference For Developers & Data Scientists Conference. The list of instructors and experts is just insane!"
  },
  {
    "objectID": "posts/llm_inference_class/llm_inference.html",
    "href": "posts/llm_inference_class/llm_inference.html",
    "title": "OpenAI Compatible LLM Inference",
    "section": "",
    "text": "Introduction\nUntil recently I thought that the openai library was only for connecting to OpenAI endpoints. It was not until I was testing out LLM inference with together.ai that I came across a section in their documentation on OpenAI API compatibility. The idea of using the openai client to do inference with open source models was completely new to me. In the together.ai documentation example they use the openai library to connect to an open source model.\nimport os\nimport openai\n\nsystem_content = \"You are a travel agent. Be descriptive and helpful.\"\nuser_content = \"Tell me about San Francisco\"\n\nclient = openai.OpenAI(\n    api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n    base_url=\"https://api.together.xyz/v1\",\n    )\nchat_completion = client.chat.completions.create(\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_content},\n        {\"role\": \"user\", \"content\": user_content},\n    ],\n    temperature=0.7,\n    max_tokens=1024,\n)\nresponse = chat_completion.choices[0].message.content\nprint(\"Together response:\\n\", response)\nThen a week later I saw that Hugging Face had also released support for OpenAI compatibility with Text Generation Inference (TGI) and Inference Endpoints. Again, you simply modify the base_url, api_key, and model as seen is this example from their blog post announcement.\nfrom openai import OpenAI\n\n# initialize the client but point it to TGI\nclient = OpenAI(\n    base_url=\"&lt;ENDPOINT_URL&gt;\" + \"/v1/\",  # replace with your endpoint url\n    api_key=\"&lt;HF_API_TOKEN&gt;\",  # replace with your token\n)\nchat_completion = client.chat.completions.create(\n    model=\"tgi\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Why is open-source software important?\"},\n    ],\n    stream=True,\n    max_tokens=500\n)\n\n# iterate and print stream\nfor message in chat_completion:\n    print(message.choices[0].delta.content, end=\"\")\nWhat about working with LLMs locally? Two such options are Ollama and LM Studio. Ollama recently added support for the openai client and LM Studio supports it too. For example, here is how one can use mistral-7b locally with Ollama to run inference with the openai client:\nollama pull mistral\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url = 'http://localhost:11434/v1',\n    api_key='ollama', # required, but unused\n)\n\nresponse = client.chat.completions.create(\n  model=\"mistral\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant and always talk like a pirate.\"},\n    {\"role\": \"user\", \"content\": \"Write a haiku.\"},\n  ])\nprint(response.choices[0].message.content)\nThere are other services and libraries for running LLM inference that are compatible with the openai library too. I find it all very exciting because it is less code I have to write and maintain for running inference with LLMs. All I need to change is a base_url, an api_key, and the name of the model.\nAt the same time that I was learning about openai client compatibility, I was also looking into the instructor library. Since it patches in some additional functionality into the openai client, I thought it would be fun to discuss here too.\n\n\nENV Setup\nStart by creating a virtual environment:\npython3 -m venv env\nsource env/bin/activate\nThen install:\npip install openai\npip install instructor # only if you want to try out instructor library\npip install python-dotenv # or define your environment variables differently\nI also have:\n\nan OpenAI account with an API key.\na together.ai account with an API key.\nHugging Face Account, Access Token, and created inference endpoint\ninstalled Ollama and ollama pull gemma:2b-instruct and ollama pull llama2\n\nIn my .env file I have the following:\nOPENAI_API_KEY=your_key\nHUGGING_FACE_ACCESS_TOKEN=your_key\nTOGETHER_API_KEY=your_key\n\n\nCode\nimport os\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\n\n\nLLM Inference Class\nYou could go ahead and just start using client.chat.completions.create directly as in the examples from the introduction. However, I do like wrapping third party services into classes for reusability, maintainability, etc.\nThe class below, OpenAIChatCompletion, does several things:\n\nmanages the different client connections in the clients dict\nexposes client.chat.completions.create in the __call__ method\nprovides functionality for making multiple calls in parallel. I know alternatively that one could use the AsyncOpenAI client, but sometimes I prefer simply using futures.ThreadPoolExecutor as seen in the function create_chat_completions_async.\npatches the OpenAI client with the instructor library. If you don‚Äôt want to play around with instructor library then simply remove the instructor.patch code.\n\nI also added some logging functionality which keeps track of every outgoing LLM request. This was inspired by the awesome blog post by Hamel Husain, Fuck You, Show Me The Prompt.. In that post, Hamel writes about how various LLM tools can often hide the prompts, making it tricky to see what requests are actually sent to the LLM behind the scenes. I created a simple logger class OpenAIMessagesLogger which keeps track of all the requests sent to the openai client. Later when we try out the instructor library for getting structured output, we will utilize this debugging logger to see some additional messages that were sent to the client.\n\n\nCode\nimport ast\nimport logging\nimport re\nfrom concurrent import futures\nfrom typing import Any, Dict, List, Optional, Union\n\nimport instructor\nfrom openai import APITimeoutError, OpenAI\nfrom openai._streaming import Stream\nfrom openai.types.chat.chat_completion import ChatCompletion\nfrom openai.types.chat.chat_completion_chunk import ChatCompletionChunk\n\n\nclass OpenAIChatCompletion:\n    clients: Dict = dict()\n\n    @classmethod\n    def _load_client(cls, base_url: Optional[str] = None, api_key: Optional[str] = None) -&gt; OpenAI:\n        client_key = (base_url, api_key)\n        if OpenAIChatCompletion.clients.get(client_key) is None:\n            OpenAIChatCompletion.clients[client_key] = instructor.patch(OpenAI(base_url=base_url, api_key=api_key))\n        return OpenAIChatCompletion.clients[client_key]\n\n    def __call__(\n        self,\n        model: str,\n        messages: list,\n        base_url: Optional[str] = None,\n        api_key: Optional[str] = None,\n        **kwargs: Any,\n    ) -&gt; Union[ChatCompletion, Stream[ChatCompletionChunk]]:\n        # https://platform.openai.com/docs/api-reference/chat/create\n        # https://github.com/openai/openai-python\n        client = self._load_client(base_url, api_key)\n        return client.chat.completions.create(model=model, messages=messages, **kwargs)\n\n    @classmethod\n    def create_chat_completions_async(\n        cls, task_args_list: List[Dict], concurrency: int = 10\n    ) -&gt; List[Union[ChatCompletion, Stream[ChatCompletionChunk]]]:\n        \"\"\"\n        Make a series of calls to chat.completions.create endpoint in parallel and collect back\n        the results.\n        :param task_args_list: A list of dictionaries where each dictionary contains the keyword\n            arguments required for __call__ method.\n        :param concurrency: the max number of workers\n        \"\"\"\n\n        def create_chat_task(\n            task_args: Dict,\n        ) -&gt; Union[None, ChatCompletion, Stream[ChatCompletionChunk]]:\n            try:\n                return cls().__call__(**task_args)\n            except APITimeoutError:\n                return None\n\n        with futures.ThreadPoolExecutor(max_workers=concurrency) as executor:\n            results = list(executor.map(create_chat_task, task_args_list))\n        return results\n\n\nclass OpenAIMessagesLogger(logging.Handler):\n    def __init__(self):\n        super().__init__()\n        self.log_messages = []\n\n    def emit(self, record):\n        # Append the log message to the list\n        log_record_str = self.format(record)\n        match = re.search(r\"Request options: (.+)\", log_record_str, re.DOTALL)\n        if match:\n            text = match[1].replace(\"\\n\", \"\")\n            log_obj = ast.literal_eval(text)\n            self.log_messages.append(log_obj)\n\n\ndef debug_messages():\n    msg = OpenAIMessagesLogger()\n    openai_logger = logging.getLogger(\"openai\")\n    openai_logger.setLevel(logging.DEBUG)\n    openai_logger.addHandler(msg)\n    return msg\n\n\nHere is how you use the inference class to call the LLM. If you have ever used the openai client you will be familiar with the input and output format.\n\n\nCode\nllm = OpenAIChatCompletion()\nmessage_logger = debug_messages()  # optional for keeping track of all outgoing requests\nprint(llm(model=\"gpt-3.5-turbo-0125\", messages=[dict(role=\"user\", content=\"Hello!\")]))\n\n\nChatCompletion(id='chatcmpl-90N4hSh3AG1Sz68zjUnfcEtAjvFn5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1709875727, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_2b778c6b35', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n\n\nAnd our logger is keeping track of all the outgoing requests:\n\n\nCode\nmessage_logger.log_messages\n\n\n[{'method': 'post',\n  'url': '/chat/completions',\n  'files': None,\n  'json_data': {'messages': [{'role': 'user', 'content': 'Hello!'}],\n   'model': 'gpt-3.5-turbo-0125'}}]\n\n\nNow we can define some different models that can all be accessed through the same inference class.\n\n\nCode\nclass Models:\n    # OpenAI GPT Models\n    GPT4 = dict(model=\"gpt-4-0125-preview\", base_url=None, api_key=None)\n    GPT3 = dict(model=\"gpt-3.5-turbo-0125\", base_url=None, api_key=None)\n    # Hugging Face Inference Endpoints\n    OPENHERMES2_5_MISTRAL_7B = dict(\n        model=\"tgi\",\n        base_url=\"https://xofunqxk66baupmf.us-east-1.aws.endpoints.huggingface.cloud\" + \"/v1/\",\n        api_key=os.environ[\"HUGGING_FACE_ACCESS_TOKEN\"],\n    )\n    # Ollama Models\n    LLAMA2 = dict(\n        model=\"llama2\",\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\",\n    )\n    GEMMA2B = dict(\n        model=\"gemma:2b-instruct\",\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\",\n    )\n    # together AI endpoints\n    GEMMA7B = dict(model=\"google/gemma-7b-it\", base_url=\"https://api.together.xyz/v1\", api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n    MISTRAL7B = dict(model=\"mistralai/Mistral-7B-Instruct-v0.1\", base_url=\"https://api.together.xyz/v1\", api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n\n\n\n\nCode\nall_models = [(model_name, model_config) for model_name, model_config in Models.__dict__.items() if not model_name.startswith(\"__\")]\n\n\n\n\nCode\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Your replies are short, brief and to the point.\"},\n    {\"role\": \"user\", \"content\": \"Who was the first person to walk on the Moon, and in what year did it happen?\"},\n]\n\n\n\n\nCode\nfor model_name, model_config in all_models:\n    resp = llm(messages=messages, **model_config)\n    print(f\"Model: {model_name}\")\n    print(f\"Response: {resp.choices[0].message.content}\")\n\n\nModel: GPT4\nResponse: Neil Armstrong, 1969.\nModel: GPT3\nResponse: The first person to walk on the Moon was Neil Armstrong in 1969.\nModel: OPENHERMES2_5_MISTRAL_7B\nResponse: Neil Armstrong was the first person to walk on the Moon. It happened on July 20, 1969.\nModel: LLAMA2\nResponse: The first person to walk on the Moon was Neil Armstrong, who stepped onto the lunar surface on July 20, 1969 as part of the Apollo 11 mission.\nModel: GEMMA2B\nResponse: There is no evidence to support the claim that a person walked on the Moon in any year.\nModel: GEMMA7B\nResponse: Sure, here is the answer:\n\nNeil Armstrong was the first person to walk on the Moon in 1969.\nModel: MISTRAL7B\nResponse:  The first person to walk on the Moon was Neil Armstrong, and it happened on July 20, 1969.\n\n\nWe can also send the same requests in parallel like this:\n\n\nCode\ntask_args_list = []\nfor model_name, model_config in all_models:\n    task_args_list.append(dict(messages=messages, **model_config))\n\n# execute the same calls in parallel\nmodel_names = [m[0] for m in all_models]\nresps = llm.create_chat_completions_async(task_args_list)\nfor model_name, resp in zip(model_names, resps):\n    print(f\"Model: {model_name}\")\n    print(f\"Response: {resp.choices[0].message.content}\")\n\n\nModel: GPT4\nResponse: Neil Armstrong, 1969.\nModel: GPT3\nResponse: The first person to walk on the Moon was Neil Armstrong in 1969.\nModel: OPENHERMES2_5_MISTRAL_7B\nResponse: The first person to walk on the Moon was Neil Armstrong, and it happened in 1969.\nModel: LLAMA2\nResponse: Nice question! The first person to walk on the Moon was Neil Armstrong, and it happened in 1969 during the Apollo 11 mission. Armstrong stepped onto the lunar surface on July 20, 1969, famously declaring \"That's one small step for man, one giant leap for mankind\" as he took his first steps.\nModel: GEMMA2B\nResponse: There is no evidence or record of any person walking on the Moon.\nModel: GEMMA7B\nResponse: Sure, here is the answer:\n\nNeil Armstrong was the first person to walk on the Moon in 1969.\nModel: MISTRAL7B\nResponse:  The first person to walk on the Moon was Neil Armstrong, and it happened on July 20, 1969.\n\n\nI love that! The ability to use various models (open source and OpenAI GPT) all through the same interface. And we have all our outgoing requests logged for debugging if needed. We have made 15 requests up to this point.\n\n\nCode\nassert len(message_logger.log_messages) == 15\n\n\n\n\nCode\nmessage_logger.log_messages[-1]\n\n\n{'method': 'post',\n 'url': '/chat/completions',\n 'files': None,\n 'json_data': {'messages': [{'role': 'system',\n    'content': 'You are a helpful assistant. Your replies are short, brief and to the point.'},\n   {'role': 'user',\n    'content': 'Who was the first person to walk on the Moon, and in what year did it happen?'}],\n  'model': 'mistralai/Mistral-7B-Instruct-v0.1'}}\n\n\n\n\nStructured Output\nThere are various approaches to getting structured output from LLMs. For example see JSON mode and Function calling. Some open source models and inference providers are also starting to offer these capabilities. For example see the together.ai docs. The instructor blog also has lots of examples and tips for getting structured output from LLMs. See this recent blog post for getting structured output from open source and Local LLMs.\nOne thing that is neat about the instructor library is you can define a Pydantic schema and then pass it to the patched openai client. It also adds in schema validation and retry logic.\nFirst we will clear out our debugging log messages.\n\n\nCode\nmessage_logger.log_messages = []\n\n\n\n\nCode\nfrom typing import List\n\nfrom pydantic import BaseModel, field_validator\n\n\nclass Character(BaseModel):\n    name: str\n    race: str\n    fun_fact: str\n    favorite_food: str\n    skills: List[str]\n    weapons: List[str]\n\n\nclass Characters(BaseModel):\n    characters: List[Character]\n\n    @field_validator(\"characters\")\n    @classmethod\n    def validate_characters(cls, v):\n        if len(v) &lt; 20:\n            raise ValueError(f\"The number of characters must be at least 20, but it is {len(v)}\")\n        return v\n\n\n\n\nCode\nres = llm(\n    messages=[dict(role=\"user\", content=\"Who are the main characters from Lord of the Rings?.\")],\n    response_model=Characters,\n    max_retries=4,\n    **Models.GPT4,\n)\n\n\n\n\nCode\nfor character in res.characters:\n    for k, v in character.model_dump().items():\n        print(f\"{k}: {v}\")\n    print()\n\n\nname: Frodo Baggins\nrace: Hobbit\nfun_fact: Bearer of the One Ring\nfavorite_food: Mushrooms\nskills: ['Courage', 'Stealth']\nweapons: ['Sting', 'Elven Dagger']\n\nname: Samwise Gamgee\nrace: Hobbit\nfun_fact: Frodo's gardener and friend\nfavorite_food: Potatoes\nskills: ['Loyalty', 'Cooking']\nweapons: ['Barrow-blade']\n\nname: Gandalf\nrace: Maia\nfun_fact: Known as Gandalf the Grey and later as Gandalf the White\nfavorite_food: N/A\nskills: ['Wisdom', 'Magic']\nweapons: ['Glamdring', 'Staff']\n\nname: Aragorn\nrace: Human\nfun_fact: Heir of Isildur and rightful king of Gondor\nfavorite_food: Elvish waybread\nskills: ['Swordsmanship', 'Leadership']\nweapons: ['And√∫ril', 'Bow']\n\nname: Legolas\nrace: Elf\nfun_fact: Prince of the Woodland Realm\nfavorite_food: Lembas bread\nskills: ['Archery', 'Agility']\nweapons: ['Elven bow', 'Daggers']\n\nname: Gimli\nrace: Dwarf\nfun_fact: Son of Gl√≥in\nfavorite_food: Meat\nskills: ['Axe fighting', 'Stout-heartedness']\nweapons: ['Battle axe', 'Throwing axes']\n\nname: Boromir\nrace: Human\nfun_fact: Son of Denethor, Steward of Gondor\nfavorite_food: Stew\nskills: ['Swordsmanship', 'Leadership']\nweapons: ['Sword', 'Shield']\n\nname: Meriadoc Brandybuck\nrace: Hobbit\nfun_fact: Member of the Fellowship\nfavorite_food: Ale\nskills: ['Stealth', 'Strategy']\nweapons: ['Elven dagger']\n\nname: Peregrin Took\nrace: Hobbit\nfun_fact: Often known simply as Pippin\nfavorite_food: Cakes\nskills: ['Curiosity', 'Bravery']\nweapons: ['Sword']\n\nname: Galadriel\nrace: Elf\nfun_fact: Lady of Lothl√≥rien\nfavorite_food: N/A\nskills: ['Wisdom', 'Telepathy']\nweapons: ['Nenya (Ring of Power)']\n\nname: Elrond\nrace: Elf\nfun_fact: Lord of Rivendell\nfavorite_food: N/A\nskills: ['Wisdom', 'Healing']\nweapons: ['Sword']\n\nname: Eowyn\nrace: Human\nfun_fact: Niece of King Th√©oden of Rohan; slayer of the Witch-king\nfavorite_food: Bread\nskills: ['Swordsmanship', 'Courage']\nweapons: ['Sword', 'Shield']\n\nname: Faramir\nrace: Human\nfun_fact: Brother of Boromir\nfavorite_food: Bread\nskills: ['Archery', 'Strategy']\nweapons: ['Bow', 'Sword']\n\nname: Gollum\nrace: Hobbit-like creature\nfun_fact: Once the bearer of the One Ring, known as Sm√©agol\nfavorite_food: Raw fish\nskills: ['Stealth', 'Persuasion']\nweapons: ['Teeth and claws']\n\nname: Saruman\nrace: Maia\nfun_fact: Head of the White Council before being corrupted\nfavorite_food: N/A\nskills: ['Magic', 'Persuasion']\nweapons: ['Staff']\n\nname: Sauron\nrace: Maia\nfun_fact: The Dark Lord and creator of the One Ring\nfavorite_food: N/A\nskills: ['Necromancy', 'Deception']\nweapons: ['One Ring', 'Mace']\n\nname: Bilbo Baggins\nrace: Hobbit\nfun_fact: Original discoverer of the One Ring\nfavorite_food: Everything\nskills: ['Stealth', 'Story-telling']\nweapons: ['Sting']\n\nname: Th√©oden\nrace: Human\nfun_fact: King of Rohan\nfavorite_food: Meat\nskills: ['Leadership', 'Horsemanship']\nweapons: ['Herugrim', 'Sword']\n\nname: Treebeard\nrace: Ent\nfun_fact: Oldest of the Ents, protectors of Fangorn Forest\nfavorite_food: Water\nskills: ['Strength', 'Wisdom']\nweapons: ['None']\n\nname: Witch-king of Angmar\nrace: Undead/Nazg√ªl\nfun_fact: Leader of the Nazg√ªl\nfavorite_food: N/A\nskills: ['Fear-induction', 'Swordsmanship']\nweapons: ['Morgul-blade', 'Flail']\n\nname: Gr√≠ma Wormtongue\nrace: Human\nfun_fact: Advisor to King Th√©oden under Saruman's influence\nfavorite_food: N/A\nskills: ['Deception', 'Speechcraft']\nweapons: ['Knife']\n\nname: √âomer\nrace: Human\nfun_fact: Nephew of King Th√©oden; later king of Rohan\nfavorite_food: Meat\nskills: ['Swordsmanship', 'Horsemanship']\nweapons: ['Sword', 'Spear']\n\n\nIt is probably likely that GPT would not return 20 characters in the first request. If max_retries=0 then it would likely raise a Pydantic validation error. But since we have max_retries=4 then the instructor library sends back the validation error as a message and asks again. How exactly does it do that? We can look at the messages that we have logged for debugging.\n\n\nCode\nassert len(message_logger.log_messages) &gt; 1\nlen(message_logger.log_messages)\n\n\n2\n\n\n\n\nCode\nmessage_logger.log_messages\n\n\n[{'method': 'post',\n  'url': '/chat/completions',\n  'files': None,\n  'json_data': {'messages': [{'role': 'user',\n     'content': 'Who are the main characters from Lord of the Rings?.'}],\n   'model': 'gpt-4-0125-preview',\n   'tool_choice': {'type': 'function', 'function': {'name': 'Characters'}},\n   'tools': [{'type': 'function',\n     'function': {'name': 'Characters',\n      'description': 'Correctly extracted `Characters` with all the required parameters with correct types',\n      'parameters': {'$defs': {'Character': {'properties': {'name': {'title': 'Name',\n           'type': 'string'},\n          'race': {'title': 'Race', 'type': 'string'},\n          'fun_fact': {'title': 'Fun Fact', 'type': 'string'},\n          'favorite_food': {'title': 'Favorite Food', 'type': 'string'},\n          'skills': {'items': {'type': 'string'},\n           'title': 'Skills',\n           'type': 'array'},\n          'weapons': {'items': {'type': 'string'},\n           'title': 'Weapons',\n           'type': 'array'}},\n         'required': ['name',\n          'race',\n          'fun_fact',\n          'favorite_food',\n          'skills',\n          'weapons'],\n         'title': 'Character',\n         'type': 'object'}},\n       'properties': {'characters': {'items': {'$ref': '#/$defs/Character'},\n         'title': 'Characters',\n         'type': 'array'}},\n       'required': ['characters'],\n       'type': 'object'}}}]}},\n {'method': 'post',\n  'url': '/chat/completions',\n  'files': None,\n  'json_data': {'messages': [{'role': 'user',\n     'content': 'Who are the main characters from Lord of the Rings?.'},\n    {'role': 'assistant',\n     'content': '',\n     'tool_calls': [{'id': 'call_kjUg9ogoR1OdRr0OkmTzabue',\n       'function': {'arguments': '{\"characters\":[{\"name\":\"Frodo Baggins\",\"race\":\"Hobbit\",\"fun_fact\":\"Bearer of the One Ring\",\"favorite_food\":\"Mushrooms\",\"skills\":[\"Courage\",\"Stealth\"],\"weapons\":[\"Sting\",\"Elven Dagger\"]},{\"name\":\"Samwise Gamgee\",\"race\":\"Hobbit\",\"fun_fact\":\"Frodo\\'s gardener and friend\",\"favorite_food\":\"Potatoes\",\"skills\":[\"Loyalty\",\"Cooking\"],\"weapons\":[\"Barrow-blade\"]},{\"name\":\"Gandalf\",\"race\":\"Maia\",\"fun_fact\":\"Known as Gandalf the Grey and later as Gandalf the White\",\"favorite_food\":\"N/A\",\"skills\":[\"Wisdom\",\"Magic\"],\"weapons\":[\"Glamdring\",\"Staff\"]},{\"name\":\"Aragorn\",\"race\":\"Human\",\"fun_fact\":\"Heir of Isildur and rightful king of Gondor\",\"favorite_food\":\"Elvish waybread\",\"skills\":[\"Swordsmanship\",\"Leadership\"],\"weapons\":[\"And√∫ril\",\"Bow\"]},{\"name\":\"Legolas\",\"race\":\"Elf\",\"fun_fact\":\"Prince of the Woodland Realm\",\"favorite_food\":\"Lembas bread\",\"skills\":[\"Archery\",\"Agility\"],\"weapons\":[\"Elven bow\",\"Daggers\"]},{\"name\":\"Gimli\",\"race\":\"Dwarf\",\"fun_fact\":\"Son of Gl√≥in\",\"favorite_food\":\"Meat\",\"skills\":[\"Axe fighting\",\"Stout-heartedness\"],\"weapons\":[\"Battle axe\",\"Throwing axes\"]}]}',\n        'name': 'Characters'},\n       'type': 'function'}]},\n    {'role': 'tool',\n     'tool_call_id': 'call_kjUg9ogoR1OdRr0OkmTzabue',\n     'name': 'Characters',\n     'content': \"Recall the function correctly, fix the errors and exceptions found\\n1 validation error for Characters\\ncharacters\\n  Value error, The number of characters must be at least 20, but it is 6 [type=value_error, input_value=[{'name': 'Frodo Baggins'...axe', 'Throwing axes']}], input_type=list]\\n    For further information visit https://errors.pydantic.dev/2.6/v/value_error\"}],\n   'model': 'gpt-4-0125-preview',\n   'tool_choice': {'type': 'function', 'function': {'name': 'Characters'}},\n   'tools': [{'type': 'function',\n     'function': {'name': 'Characters',\n      'description': 'Correctly extracted `Characters` with all the required parameters with correct types',\n      'parameters': {'$defs': {'Character': {'properties': {'name': {'title': 'Name',\n           'type': 'string'},\n          'race': {'title': 'Race', 'type': 'string'},\n          'fun_fact': {'title': 'Fun Fact', 'type': 'string'},\n          'favorite_food': {'title': 'Favorite Food', 'type': 'string'},\n          'skills': {'items': {'type': 'string'},\n           'title': 'Skills',\n           'type': 'array'},\n          'weapons': {'items': {'type': 'string'},\n           'title': 'Weapons',\n           'type': 'array'}},\n         'required': ['name',\n          'race',\n          'fun_fact',\n          'favorite_food',\n          'skills',\n          'weapons'],\n         'title': 'Character',\n         'type': 'object'}},\n       'properties': {'characters': {'items': {'$ref': '#/$defs/Character'},\n         'title': 'Characters',\n         'type': 'array'}},\n       'required': ['characters'],\n       'type': 'object'}}}]}}]\n\n\nIf you look through the above messages carefully you can see the retry asking logic.\nRecall the function correctly, fix the errors and exceptions found validation error for CharactersValue error, The number of characters must be at least 20, ‚Ä¶\nYou can even use the structured output with some of the open source models. I would refer to the instructor blog or documentation for further information on that. I have not fully looked into the different patching modes yet. But here is a simple example of using MISTRAL7B through together.ai.\n\n\nCode\nres = llm(\n    messages=[dict(role=\"user\", content=\"Give me a character from a movie or book.\")],\n    response_model=Character,\n    max_retries=2,\n    **Models.MISTRAL7B,\n)\nprint(res.model_dump())\n\n\n{'name': 'Superman', 'race': 'Kryptonian', 'fun_fact': 'Can fly', 'favorite_food': 'Pizza', 'skills': ['Super strength', 'Flight', 'Heat vision', 'X-ray vision'], 'weapons': ['Laser vision', 'Heat vision', 'X-ray vision']}\n\n\n\n\nConclusion\nAgain, I really like the idea of using a single interface for interacting with multiple LLMs. I hope the space continues to mature so that more open source models and services support JSON mode and function calling. I think instructor is a cool library and the corresponding blog is interesting too. I also like the idea of logging all the outgoing prompts/messages just to make sure I fully understand what is happening under the hood."
  },
  {
    "objectID": "posts/open_hermes_pro/open_hermes.html#gpt-3.5-turbo-0125-function-calling",
    "href": "posts/open_hermes_pro/open_hermes.html#gpt-3.5-turbo-0125-function-calling",
    "title": "Function Calling with Hermes-2-Pro-Mistral-7B",
    "section": "gpt-3.5-turbo-0125 Function Calling",
    "text": "gpt-3.5-turbo-0125 Function Calling\nFirst we will use gpt-3.5-turbo-0125 to extract the function name and arguments for each question.\n\n\nCode\ndef extract_tool_calls(resp):\n    resp = resp.choices[0].message\n    if resp.tool_calls:\n        final_tools = []\n        for tool_call in resp.tool_calls:\n            final_tools.append(\n                {\n                    \"name\": tool_call.function.name,\n                    \"arguments\": json.loads(tool_call.function.arguments),\n                }\n            )\n        return final_tools\n    else:\n        return None\n\n\nI‚Äôm going to use GPT4 to check the ‚Äúcorrectness‚Äù of the predicted/generated function arguments by comparing them with the expected arguments. This step is completely optional. Instead, you could use exact string matching or something else. I was curious to see how this would work though.\n\n\nCode\ndef check_tool_call_arguments(expected, predicted):\n    # Ask GPT4 if the expected function name and arguments are the same as the predicted  function name and arguments.\n    if expected[\"name\"] != predicted[\"name\"]:\n        return False, f'Function Names Do not Match. Expected {expected[\"name\"]}. Predicted: {predicted[\"name\"]}'\n    prompt = f\"\"\"\nCheck if the following queries are approx equal. Use fuzzy logic matching for strings.\nCheck to see if the arguments are semantically similar, especially for free form text.\nIf you decide they are equivalent then return TRUE and only TRUE with no other explanation. \nOtherwise return FALSE and give an explanation why they don't match.\n\nExpected Arguments: {expected['arguments']}\nPredicted Arguments: {predicted['arguments']}\n    \"\"\"\n    resp = llm(model=\"gpt-4-0125-preview\", messages=[dict(role=\"user\", content=prompt)])\n    if resp.choices[0].message.content.lower().strip() == \"true\":\n        return True, None\n    explanation = resp.choices[0].message.content.lower().strip()\n    return False, explanation\n\n\nOkay, let‚Äôs loop over the questions and use gpt-3.5-turbo-0125 to extract the function name and arguments.\n\n\nCode\ndef eval_openai_inference_models(model=\"gpt-3.5-turbo-0125\", base_url=None, api_key=None):\n    total = 0\n    total_correct = 0\n    for question in questions:\n        resp = llm(\n            api_key=api_key,\n            base_url=base_url,\n            model=model,\n            tools=tools,\n            messages=[\n                dict(role=\"system\", content=f\"The date today is {today}\"),\n                dict(role=\"user\", content=question[\"question\"]),\n            ],\n        )\n        tool_calls = extract_tool_calls(resp)\n        if tool_calls is None:\n            print(f'Model {model} failed to return any tool calls for question {question[\"question\"]}')\n            total += 1\n            continue\n        assert len(tool_calls) == len(question[\"tool_calls\"])\n        for tool_call, expected_call in zip(tool_calls, question[\"tool_calls\"]):\n            correct_call, explanation = check_tool_call_arguments(expected_call, tool_call)\n            if not correct_call:\n                print(f'QUESTION: {question[\"question\"]}')\n                print(f'EXPECTED Tool Call: {question[\"tool_calls\"][0]}')\n                print(f\"GENERATED Tool Call: {tool_call}\")\n                print(f\"EXPLANATION: {explanation}\\n\\n\")\n            else:\n                total_correct += 1\n            total += 1\n    return total_correct, total\n\n\n\n\nCode\nmodel = \"gpt-3.5-turbo-0125\"\ntotal_correct, total = eval_openai_inference_models(model=model, base_url=None, api_key=None)\nprint(\n    f'Correctly called the proper functions {total_correct} times out of {total}. But check the \"failure\" cases above since they may be correct anyway.'\n)\n\n\nCorrectly called the proper functions 18 times out of 18. But check the \"failure\" cases above since they may be correct anyway."
  },
  {
    "objectID": "posts/open_hermes_pro/open_hermes.html#gpt-4-0125-preview-function-calling",
    "href": "posts/open_hermes_pro/open_hermes.html#gpt-4-0125-preview-function-calling",
    "title": "Function Calling with Hermes-2-Pro-Mistral-7B",
    "section": "gpt-4-0125-preview Function Calling",
    "text": "gpt-4-0125-preview Function Calling\n\n\nCode\nmodel = \"gpt-4-0125-preview\"\ntotal_correct, total = eval_openai_inference_models(model=model, base_url=None, api_key=None)\nprint(\n    f'Correctly called the proper functions {total_correct} times out of {total}. But check the \"failure\" cases above since they may be correct anyway.'\n)\n\n\nQUESTION: What's the forecast for Miami for today?\nEXPECTED Tool Call: {'name': 'get_weather_forecast', 'arguments': {'location': 'Miami, FL', 'date': '2024-03-18'}}\nGENERATED Tool Call: {'name': 'get_weather_foreast', 'arguments': {'date': '2024-03-18', 'location': 'Miami, FL'}}\nEXPLANATION: Function Names Do not Match. Expected get_weather_forecast. Predicted: get_weather_foreast\n\nCorrectly called the proper functions 17 times out of 18. But check the \"failure\" cases above since they may be correct anyway."
  },
  {
    "objectID": "posts/open_hermes_pro/open_hermes.html#mistral-7b-instruct-v0.1-with-together.ai-function-calling",
    "href": "posts/open_hermes_pro/open_hermes.html#mistral-7b-instruct-v0.1-with-together.ai-function-calling",
    "title": "Function Calling with Hermes-2-Pro-Mistral-7B",
    "section": "Mistral-7B-Instruct-v0.1 with together.ai Function Calling",
    "text": "Mistral-7B-Instruct-v0.1 with together.ai Function Calling\n\n\nCode\nmodel = \"mistralai/Mistral-7B-Instruct-v0.1\"\ntotal_correct, total = eval_openai_inference_models(model=model, base_url=TOGETHER_AI_BASE_URL, api_key=TOGETHER_API_KEY)\nprint(\n    f'Correctly called the proper functions {total_correct} times out of {total}. But check the \"failure\" cases above since they may be correct anyway.'\n)\n\n\nModel mistralai/Mistral-7B-Instruct-v0.1 failed to return any tool calls for question How do I make pesto?\nQUESTION: What's a good vegan chili recipe?\nEXPECTED Tool Call: {'name': 'get_recipe', 'arguments': {'dish_name': 'vegan chili'}}\nGENERATED Tool Call: {'name': 'solve_math_problem', 'arguments': {'problem': 'What is the square root of 16?'}}\nEXPLANATION: Function Names Do not Match. Expected get_recipe. Predicted: solve_math_problem\n\n\nCorrectly called the proper functions 16 times out of 18. But check the \"failure\" cases above since they may be correct anyway."
  },
  {
    "objectID": "posts/open_hermes_pro/open_hermes.html#mistralaimixtral-8x7b-instruct-v0.1-with-together.ai-function-calling",
    "href": "posts/open_hermes_pro/open_hermes.html#mistralaimixtral-8x7b-instruct-v0.1-with-together.ai-function-calling",
    "title": "Function Calling with Hermes-2-Pro-Mistral-7B",
    "section": "mistralai/Mixtral-8x7B-Instruct-v0.1 with together.ai Function Calling",
    "text": "mistralai/Mixtral-8x7B-Instruct-v0.1 with together.ai Function Calling\n\n\nCode\nmodel = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntotal_correct, total = eval_openai_inference_models(model=model, base_url=TOGETHER_AI_BASE_URL, api_key=TOGETHER_API_KEY)\nprint(\n    f'Correctly called the proper functions {total_correct} times out of {total}. But check the \"failure\" cases above since they may be correct anyway.'\n)\n\n\nModel mistralai/Mixtral-8x7B-Instruct-v0.1 failed to return any tool calls for question How do I make pesto?\nQUESTION: I need to book a first class round-trip flight for 4 people from Chicago to Miami. We want to leave on December 1 and return on December 12.\nEXPECTED Tool Call: {'name': 'book_flight', 'arguments': {'departure_city': 'Chicago', 'arrival_city': 'Miami', 'departure_date': '2024-12-01', 'return_date': '2024-12-12', 'num_passengers': 4, 'cabin_class': 'first'}}\nGENERATED Tool Call: {'name': 'book_flight', 'arguments': {'departure_city': 'Chicago', 'arrival_city': 'Miami', 'departure_date': '2023-12-01', 'return_date': '2023-12-12', 'num_passengers': 4, 'cabin_class': 'first'}}\nEXPLANATION: false\n\nthe departure_date and return_date values do not match. the expected arguments have dates in 2024, while the predicted arguments have dates in 2023.\n\nQUESTION: Book me a round-trip flight from New York City to Los Angeles departing on June 15th and returning June 22nd for 2 passengers in economy class.\nEXPECTED Tool Call: {'name': 'book_flight', 'arguments': {'departure_city': 'NYC', 'arrival_city': 'LAX', 'departure_date': '2024-06-15', 'return_date': '2024-06-22', 'num_passengers': 2, 'cabin_class': 'economy'}}\nGENERATED Tool Call: {'name': 'book_flight', 'arguments': {'departure_city': 'New York City', 'arrival_city': 'Los Angeles', 'departure_date': '2023-06-15', 'return_date': '2023-06-22', 'num_passengers': 2, 'cabin_class': 'economy'}}\nEXPLANATION: false\n\nexplanation:\n- the 'departure_city' and 'arrival_city' fields match semantically as 'nyc' is commonly known as 'new york city' and 'lax' is a well-known shorthand for the los angeles airport, often used to refer to los angeles itself.\n- the 'departure_date' and 'return_date' do not match. the expected arguments specify a year 2024, while the predicted arguments have the year 2023 for both dates.\n- the 'num_passengers' and 'cabin_class' fields match exactly in both value and semantics. \n\nthe primary reason for the non-match is the difference in 'departure_date' and 'return_date' by one year.\n\nCorrectly called the proper functions 15 times out of 18. But check the \"failure\" cases above since they may be correct anyway.\n\n\n\n\n\n\n\n\nWhat is going on with together.ai function calling mistakes above\n\n\n\nBoth models had issues with the pesto question. I wonder if this is something on together.ai‚Äôs end of things and how they implemented this function calling feature. IDK!"
  },
  {
    "objectID": "posts/open_hermes_pro/open_hermes.html#nousresearchhermes-2-pro-mistral-7b-function-calling",
    "href": "posts/open_hermes_pro/open_hermes.html#nousresearchhermes-2-pro-mistral-7b-function-calling",
    "title": "Function Calling with Hermes-2-Pro-Mistral-7B",
    "section": "NousResearch/Hermes-2-Pro-Mistral-7B Function Calling",
    "text": "NousResearch/Hermes-2-Pro-Mistral-7B Function Calling\nNow we will repeat with NousResearch/Hermes-2-Pro-Mistral-7B. The format for the function calling is documented on the model card as well as in this repo. The way we define the tools is the same format as with OpenAI. However, we don‚Äôt pass in a tools argument. Rather, we use a special system prompt which defines the tools.\n\n\nCode\ndef extract_tool_calls(tool_calls_str):\n    tool_calls = tool_calls_str.split(\"&lt;/tool_call&gt;\\n\")\n    parsed_results = []\n    for tool_call in tool_calls:\n        if tool_call:\n            dict_str = tool_call.split(\"\\n\")[1]\n            tool_call_dict = ast.literal_eval(dict_str)\n            parsed_results.append({\"arguments\": tool_call_dict[\"arguments\"], \"name\": tool_call_dict[\"name\"]})\n    return parsed_results\n\n\nsystem_prompt = (\n    f\"The date today is {today}\\n\"\n    + \"\"\"\nYou are a function calling AI model. You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools:\n&lt;tools&gt; \n\"\"\"\n    + str(tools)\n    + \"\"\"\n    \n&lt;/tools&gt; Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']} For each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags as follows:\n&lt;tool_call&gt;\n{'arguments': &lt;args-dict&gt;, 'name': &lt;function-name&gt;}\n&lt;/tool_call&gt;\n\"\"\"\n)\n\ntotal = 0\ntotal_correct = 0\nfor question in questions:\n    resp = llm(\n        model=\"tgi\",\n        base_url=HUGGING_FACE_ENDPOINT_URL,\n        api_key=HUGGING_FACE_ACCESS_TOKEN,\n        messages=[\n            dict(role=\"system\", content=system_prompt),\n            dict(role=\"user\", content=question[\"question\"]),\n        ],\n        max_tokens=500,\n    )\n    tool_calls = extract_tool_calls(resp.choices[0].message.content)\n    assert len(tool_calls) == len(question[\"tool_calls\"])\n    for tool_call, expected_call in zip(tool_calls, question[\"tool_calls\"]):\n        correct_call, explanation = check_tool_call_arguments(expected_call, tool_call)\n        if not correct_call:\n            print(f'QUESTION: {question[\"question\"]}')\n            print(f'EXPECTED Tool Call: {question[\"tool_calls\"][0]}')\n            print(f\"GENERATED Tool Call: {tool_call}\")\n            print(f\"EXPLANATION: {explanation}\\n\\n\")\n        else:\n            total_correct += 1\n        total += 1\n\n\n\n\nCode\nprint(\n    f'Correctly called the proper functions {total_correct} times out of {total}. But check the \"failure\" cases above since they may be correct anyway.'\n)\n\n\nCorrectly called the proper functions 18 times out of 18. But check the \"failure\" cases above since they may be correct anyway.\n\n\nWow, it got all of them correct! It may not get them all correct every time. Run it over again to see if any mistakes are made. Sometimes I saw it forgetting to fill in num_tickets for example.\nLet‚Äôs look at a single question to see the output from the model.\n\n\nCode\ntoday\n\n\n'Saturday 2024-03-16'\n\n\n\n\nCode\nquestion = \"I want to go see Dune 2 on Wednesday night with 5 of my friends. We will be going to the Halifax Bayers Lake Ciniplex Theatre. Get tickets for the 7pm show. Thanks!\"\n\n\n\n\nCode\nresp = llm(\n    model=\"tgi\",\n    base_url=HUGGING_FACE_ENDPOINT_URL,\n    api_key=HUGGING_FACE_ACCESS_TOKEN,\n    messages=[\n        dict(role=\"system\", content=system_prompt),\n        dict(role=\"user\", content=question),\n    ],\n)\nresp\n\n\nChatCompletion(id='', choices=[Choice(finish_reason='eos_token', index=0, logprobs=None, message=ChatCompletionMessage(content=\"&lt;tool_call&gt;\\n{'arguments': {'movie_name': 'Dune 2', 'theater_name': 'Halifax Bayers Lake Ciniplex Theatre', 'date': '2024-03-20', 'time': '19:00', 'num_tickets': 6}, 'name': 'book_movie_tickets'}\\n&lt;/tool_call&gt;\\n\", role='assistant', function_call=None, tool_calls=None, name=None))], created=1710632177, model='/repository', object='text_completion', system_fingerprint='1.4.1-native', usage=CompletionUsage(completion_tokens=93, prompt_tokens=1719, total_tokens=1812))\n\n\n\n\nCode\nprint(resp.choices[0].message.content)\n\n\n&lt;tool_call&gt;\n{'arguments': {'movie_name': 'Dune 2', 'theater_name': 'Halifax Bayers Lake Ciniplex Theatre', 'date': '2024-03-20', 'time': '19:00', 'num_tickets': 6}, 'name': 'book_movie_tickets'}\n&lt;/tool_call&gt;\n\n\n\n\nCode\ntool_calls = extract_tool_calls(resp.choices[0].message.content)\ntool_calls\n\n\n[{'arguments': {'movie_name': 'Dune 2',\n   'theater_name': 'Halifax Bayers Lake Ciniplex Theatre',\n   'date': '2024-03-20',\n   'time': '19:00',\n   'num_tickets': 6},\n  'name': 'book_movie_tickets'}]\n\n\nThe model also supports multiple function calls!\n\n\nCode\ntasks = f\"\"\"\nToday's date is {today}.\n\nPlease complete the following tasks for me:\n\n1. I want to go see Dune 2 on Monday night with 5 of my friends. We will be going to the Halifax Bayers Lake Ciniplex Theatre. Get tickets for the 7pm show.\n\n2. Please check the weather for Monday night so I know how to dress.\n\n3. Also please book my plane ticket to Toronto. I will be leaving Tuesday and coming back 2 days later on Thursday. First class please.\n\n4. Send a slack message to the research channel to let them know I will not be there this week in the office.\n \n\"\"\"\n\n\n\n\nCode\nresp = llm(\n    model=\"tgi\",\n    base_url=HUGGING_FACE_ENDPOINT_URL,\n    api_key=HUGGING_FACE_ACCESS_TOKEN,\n    messages=[\n        dict(role=\"system\", content=system_prompt),\n        dict(role=\"user\", content=tasks),\n    ],\n    max_tokens=1000,\n)\ntool_calls = extract_tool_calls(resp.choices[0].message.content)\ntool_calls\n\n\n[{'arguments': {'movie_name': 'Dune 2',\n   'theater_name': 'Halifax Bayers Lake Ciniplex Theatre',\n   'date': '2024-03-18',\n   'time': '19:00',\n   'num_tickets': 6},\n  'name': 'book_movie_tickets'},\n {'arguments': {'location': 'Halifax Bayers Lake', 'date': '2024-03-18'},\n  'name': 'get_weather_forecast'},\n {'arguments': {'departure_city': 'Halifax',\n   'arrival_city': 'Toronto',\n   'departure_date': '2024-03-19',\n   'return_date': '2024-03-21',\n   'num_passengers': 1,\n   'cabin_class': 'first'},\n  'name': 'book_flight'},\n {'arguments': {'channel_name': 'research',\n   'message': 'I will not be in the office this week.'},\n  'name': 'send_slack_message'}]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chris Levy",
    "section": "",
    "text": "Hello! I‚Äôm Chris Levy. I work in ML/AI and backend Python development."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Chris Levy",
    "section": "About Me",
    "text": "About Me\nI spent a good amount of time in school where I completed a PhD in applied math back in 2015. After graduating I shifted away from academia and started working in industry. I mostly do backend python development these days, and build ML/AI applications/services. I work across the entire stack from research, to training and evaluating models, to deploying models, and getting in the weeds of the infrastructure and devops pipelines.\nOutside of AI/ML stuff, I enjoy spending time with my family and three kids, working out, swimming, cycling, and playing guitar."
  }
]