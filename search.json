[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "OpenAI Compatible LLM Inference\n\n\nA Single Inference Wrapper for OpenAI, Together AI, Hugging Face Inference TGI, Ollama, etc.\n\n\n\n\n\n\n\n\n\nMar 8, 2024\n\n\nChris Levy\n\n\n\n\n\n\n  \n\n\n\n\nDSPy\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nChris Levy\n\n\n\n\n\n\n  \n\n\n\n\nBasic Transformer Architecture Notes\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2024\n\n\nChris Levy\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/llm_inference_class/llm_inference.html",
    "href": "posts/llm_inference_class/llm_inference.html",
    "title": "OpenAI Compatible LLM Inference",
    "section": "",
    "text": "Introduction\nUntil recently I thought that the openai library was only for connecting to OpenAI endpoints. It was not until I was testing out LLM inference with together.ai that I came across a section in their documentation on OpenAI API compatibility. The idea of using the openai client to do inference with open source models was completely new to me. In the together.ai documentation example they use the openai library to connect to an open source model.\nimport os\nimport openai\n\nsystem_content = \"You are a travel agent. Be descriptive and helpful.\"\nuser_content = \"Tell me about San Francisco\"\n\nclient = openai.OpenAI(\n    api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n    base_url=\"https://api.together.xyz/v1\",\n    )\nchat_completion = client.chat.completions.create(\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_content},\n        {\"role\": \"user\", \"content\": user_content},\n    ],\n    temperature=0.7,\n    max_tokens=1024,\n)\nresponse = chat_completion.choices[0].message.content\nprint(\"Together response:\\n\", response)\nThen a week later I saw that Hugging Face had also released support for OpenAI compatibility with Text Generation Inference (TGI) and Inference Endpoints. Again, you simply modify the base_url, api_key, and model as seen is this example from their blog post announcement.\nfrom openai import OpenAI\n\n# initialize the client but point it to TGI\nclient = OpenAI(\n    base_url=\"&lt;ENDPOINT_URL&gt;\" + \"/v1/\",  # replace with your endpoint url\n    api_key=\"&lt;HF_API_TOKEN&gt;\",  # replace with your token\n)\nchat_completion = client.chat.completions.create(\n    model=\"tgi\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Why is open-source software important?\"},\n    ],\n    stream=True,\n    max_tokens=500\n)\n\n# iterate and print stream\nfor message in chat_completion:\n    print(message.choices[0].delta.content, end=\"\")\nWhat about working with LLMs locally? Two such options are Ollama and LM Studio. Ollama recently added support for the openai client and LM Studio supports it too. For example, here is how one can use mistral-7b locally with Ollama to run inference with the openai client:\nollama pull mistral\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url = 'http://localhost:11434/v1',\n    api_key='ollama', # required, but unused\n)\n\nresponse = client.chat.completions.create(\n  model=\"mistral\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant and always talk like a pirate.\"},\n    {\"role\": \"user\", \"content\": \"Write a haiku.\"},\n  ])\nprint(response.choices[0].message.content)\nThere are other services and libraries for running LLM inference that are compatible with the openai library too. I find it all very exciting because it is less code I have to write and maintain for running inference with LLMs. All I need to change is a base_url, an api_key, and the name of the model.\nAt the same time that I was learning about openai client compatibility, I was also looking into the instructor library. Since it patches in some additional functionality into the openai client, I thought it would be fun to discuss here too.\n\n\nENV Setup\nStart by creating a virtual environment:\npython3 -m venv env\nsource env/bin/activate\nThen install:\npip install openai\npip install instructor # only if you want to try out instructor library\npip install python-dotenv # or define your environment variables differently\nI also have:\n\nan OpenAI account with an API key.\na together.ai account with an API key.\nHugging Face Account, Access Token, and created inference endpoint\ninstalled Ollama and ollama pull gemma:2b-instruct and ollama pull llama2\n\nIn my .env file I have the following:\nOPENAI_API_KEY=your_key\nHUGGING_FACE_ACCESS_TOKEN=your_key\nTOGETHER_API_KEY=your_key\n\n\nCode\nimport os\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\n\n\nLLM Inference Class\nYou could go ahead and just start using client.chat.completions.create directly as in the examples from the introduction. However, I do like wrapping third party services into classes for reusability, maintainability, etc.\nThe class below, OpenAIChatCompletion, does several things:\n\nmanages the different client connections in the clients dict\nexposes client.chat.completions.create in the __call__ method\nprovides functionality for making multiple calls in parallel. I know alternatively that one could use the AsyncOpenAI client, but sometimes I prefer simply using futures.ThreadPoolExecutor as seen in the function create_chat_completions_async.\npatches the OpenAI client with the instructor library. If you don’t want to play around with instructor library then simply remove the instructor.patch code.\n\nI also added some logging functionality which keeps track of every outgoing LLM request. This was inspired by the awesome blog post by Hamel Husain, Fuck You, Show Me The Prompt.. In that post, Hamel writes about how various LLM tools can often hide the prompts, making it tricky to see what requests are actually sent to the LLM behind the scenes. I created a simple logger class OpenAIMessagesLogger which keeps track of all the requests sent to the openai client. Later when we try out the instructor library for getting structured output, we will utilize this debugging logger to see some additional messages that were sent to the client.\n\n\nCode\nimport ast\nimport logging\nimport re\nfrom concurrent import futures\nfrom typing import Any, Dict, List, Optional, Union\n\nimport instructor\nfrom openai import APITimeoutError, OpenAI\nfrom openai._streaming import Stream\nfrom openai.types.chat.chat_completion import ChatCompletion\nfrom openai.types.chat.chat_completion_chunk import ChatCompletionChunk\n\n\nclass OpenAIChatCompletion:\n    clients: Dict = dict()\n\n    @classmethod\n    def _load_client(cls, base_url: Optional[str] = None, api_key: Optional[str] = None) -&gt; OpenAI:\n        client_key = (base_url, api_key)\n        if OpenAIChatCompletion.clients.get(client_key) is None:\n            OpenAIChatCompletion.clients[client_key] = instructor.patch(OpenAI(base_url=base_url, api_key=api_key))\n        return OpenAIChatCompletion.clients[client_key]\n\n    def __call__(\n        self,\n        model: str,\n        messages: list,\n        base_url: Optional[str] = None,\n        api_key: Optional[str] = None,\n        **kwargs: Any,\n    ) -&gt; Union[ChatCompletion, Stream[ChatCompletionChunk]]:\n        # https://platform.openai.com/docs/api-reference/chat/create\n        # https://github.com/openai/openai-python\n        client = self._load_client(base_url, api_key)\n        return client.chat.completions.create(model=model, messages=messages, **kwargs)\n\n    @classmethod\n    def create_chat_completions_async(\n        cls, task_args_list: List[Dict], concurrency: int = 10\n    ) -&gt; List[Union[ChatCompletion, Stream[ChatCompletionChunk]]]:\n        \"\"\"\n        Make a series of calls to chat.completions.create endpoint in parallel and collect back\n        the results.\n        :param task_args_list: A list of dictionaries where each dictionary contains the keyword\n            arguments required for __call__ method.\n        :param concurrency: the max number of workers\n        \"\"\"\n\n        def create_chat_task(\n            task_args: Dict,\n        ) -&gt; Union[None, ChatCompletion, Stream[ChatCompletionChunk]]:\n            try:\n                return cls().__call__(**task_args)\n            except APITimeoutError:\n                return None\n\n        with futures.ThreadPoolExecutor(max_workers=concurrency) as executor:\n            results = list(executor.map(create_chat_task, task_args_list))\n        return results\n\n\nclass OpenAIMessagesLogger(logging.Handler):\n    def __init__(self):\n        super().__init__()\n        self.log_messages = []\n\n    def emit(self, record):\n        # Append the log message to the list\n        log_record_str = self.format(record)\n        match = re.search(r\"Request options: (.+)\", log_record_str, re.DOTALL)\n        if match:\n            text = match[1].replace(\"\\n\", \"\")\n            log_obj = ast.literal_eval(text)\n            self.log_messages.append(log_obj)\n\n\ndef debug_messages():\n    msg = OpenAIMessagesLogger()\n    openai_logger = logging.getLogger(\"openai\")\n    openai_logger.setLevel(logging.DEBUG)\n    openai_logger.addHandler(msg)\n    return msg\n\n\nHere is how you use the inference class to call the LLM. If you have ever used the openai client you will be familiar with the input and output format.\n\n\nCode\nllm = OpenAIChatCompletion()\nmessage_logger = debug_messages()  # optional for keeping track of all outgoing requests\nprint(llm(model=\"gpt-3.5-turbo-0125\", messages=[dict(role=\"user\", content=\"Hello!\")]))\n\n\nChatCompletion(id='chatcmpl-90N4hSh3AG1Sz68zjUnfcEtAjvFn5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1709875727, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_2b778c6b35', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n\n\nAnd our logger is keeping track of all the outgoing requests:\n\n\nCode\nmessage_logger.log_messages\n\n\n[{'method': 'post',\n  'url': '/chat/completions',\n  'files': None,\n  'json_data': {'messages': [{'role': 'user', 'content': 'Hello!'}],\n   'model': 'gpt-3.5-turbo-0125'}}]\n\n\nNow we can define some different models that can all be accessed through the same inference class.\n\n\nCode\nclass Models:\n    # OpenAI GPT Models\n    GPT4 = dict(model=\"gpt-4-0125-preview\", base_url=None, api_key=None)\n    GPT3 = dict(model=\"gpt-3.5-turbo-0125\", base_url=None, api_key=None)\n    # Hugging Face Inference Endpoints\n    OPENHERMES2_5_MISTRAL_7B = dict(\n        model=\"tgi\",\n        base_url=\"https://xofunqxk66baupmf.us-east-1.aws.endpoints.huggingface.cloud\" + \"/v1/\",\n        api_key=os.environ[\"HUGGING_FACE_ACCESS_TOKEN\"],\n    )\n    # Ollama Models\n    LLAMA2 = dict(\n        model=\"llama2\",\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\",\n    )\n    GEMMA2B = dict(\n        model=\"gemma:2b-instruct\",\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\",\n    )\n    # together AI endpoints\n    GEMMA7B = dict(model=\"google/gemma-7b-it\", base_url=\"https://api.together.xyz/v1\", api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n    MISTRAL7B = dict(model=\"mistralai/Mistral-7B-Instruct-v0.1\", base_url=\"https://api.together.xyz/v1\", api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n\n\n\n\nCode\nall_models = [(model_name, model_config) for model_name, model_config in Models.__dict__.items() if not model_name.startswith(\"__\")]\n\n\n\n\nCode\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Your replies are short, brief and to the point.\"},\n    {\"role\": \"user\", \"content\": \"Who was the first person to walk on the Moon, and in what year did it happen?\"},\n]\n\n\n\n\nCode\nfor model_name, model_config in all_models:\n    resp = llm(messages=messages, **model_config)\n    print(f\"Model: {model_name}\")\n    print(f\"Response: {resp.choices[0].message.content}\")\n\n\nModel: GPT4\nResponse: Neil Armstrong, 1969.\nModel: GPT3\nResponse: The first person to walk on the Moon was Neil Armstrong in 1969.\nModel: OPENHERMES2_5_MISTRAL_7B\nResponse: Neil Armstrong was the first person to walk on the Moon. It happened on July 20, 1969.\nModel: LLAMA2\nResponse: The first person to walk on the Moon was Neil Armstrong, who stepped onto the lunar surface on July 20, 1969 as part of the Apollo 11 mission.\nModel: GEMMA2B\nResponse: There is no evidence to support the claim that a person walked on the Moon in any year.\nModel: GEMMA7B\nResponse: Sure, here is the answer:\n\nNeil Armstrong was the first person to walk on the Moon in 1969.\nModel: MISTRAL7B\nResponse:  The first person to walk on the Moon was Neil Armstrong, and it happened on July 20, 1969.\n\n\nWe can also send the same requests in parallel like this:\n\n\nCode\ntask_args_list = []\nfor model_name, model_config in all_models:\n    task_args_list.append(dict(messages=messages, **model_config))\n\n# execute the same calls in parallel\nmodel_names = [m[0] for m in all_models]\nresps = llm.create_chat_completions_async(task_args_list)\nfor model_name, resp in zip(model_names, resps):\n    print(f\"Model: {model_name}\")\n    print(f\"Response: {resp.choices[0].message.content}\")\n\n\nModel: GPT4\nResponse: Neil Armstrong, 1969.\nModel: GPT3\nResponse: The first person to walk on the Moon was Neil Armstrong in 1969.\nModel: OPENHERMES2_5_MISTRAL_7B\nResponse: The first person to walk on the Moon was Neil Armstrong, and it happened in 1969.\nModel: LLAMA2\nResponse: Nice question! The first person to walk on the Moon was Neil Armstrong, and it happened in 1969 during the Apollo 11 mission. Armstrong stepped onto the lunar surface on July 20, 1969, famously declaring \"That's one small step for man, one giant leap for mankind\" as he took his first steps.\nModel: GEMMA2B\nResponse: There is no evidence or record of any person walking on the Moon.\nModel: GEMMA7B\nResponse: Sure, here is the answer:\n\nNeil Armstrong was the first person to walk on the Moon in 1969.\nModel: MISTRAL7B\nResponse:  The first person to walk on the Moon was Neil Armstrong, and it happened on July 20, 1969.\n\n\nI love that! The ability to use various models (open source and OpenAI GPT) all through the same interface. And we have all our outgoing requests logged for debugging if needed. We have made 15 requests up to this point.\n\n\nCode\nassert len(message_logger.log_messages) == 15\n\n\n\n\nCode\nmessage_logger.log_messages[-1]\n\n\n{'method': 'post',\n 'url': '/chat/completions',\n 'files': None,\n 'json_data': {'messages': [{'role': 'system',\n    'content': 'You are a helpful assistant. Your replies are short, brief and to the point.'},\n   {'role': 'user',\n    'content': 'Who was the first person to walk on the Moon, and in what year did it happen?'}],\n  'model': 'mistralai/Mistral-7B-Instruct-v0.1'}}\n\n\n\n\nStructured Output\nThere are various approaches to getting structured output from LLMs. For example see JSON mode and Function calling. Some open source models and inference providers are also starting to offer these capabilities. For example see the together.ai docs. The instructor blog also has lots of examples and tips for getting structured output from LLMs. See this recent blog post for getting structured output from open source and Local LLMs.\nOne thing that is neat about the instructor library is you can define a Pydantic schema and then pass it to the patched openai client. It also adds in schema validation and retry logic.\nFirst we will clear out our debugging log messages.\n\n\nCode\nmessage_logger.log_messages = []\n\n\n\n\nCode\nfrom typing import List\n\nfrom pydantic import BaseModel, field_validator\n\n\nclass Character(BaseModel):\n    name: str\n    race: str\n    fun_fact: str\n    favorite_food: str\n    skills: List[str]\n    weapons: List[str]\n\n\nclass Characters(BaseModel):\n    characters: List[Character]\n\n    @field_validator(\"characters\")\n    @classmethod\n    def validate_characters(cls, v):\n        if len(v) &lt; 20:\n            raise ValueError(f\"The number of characters must be at least 20, but it is {len(v)}\")\n        return v\n\n\n\n\nCode\nres = llm(\n    messages=[dict(role=\"user\", content=\"Who are the main characters from Lord of the Rings?.\")],\n    response_model=Characters,\n    max_retries=4,\n    **Models.GPT4,\n)\n\n\n\n\nCode\nfor character in res.characters:\n    for k, v in character.model_dump().items():\n        print(f\"{k}: {v}\")\n    print()\n\n\nname: Frodo Baggins\nrace: Hobbit\nfun_fact: Bearer of the One Ring\nfavorite_food: Mushrooms\nskills: ['Courage', 'Stealth']\nweapons: ['Sting', 'Elven Dagger']\n\nname: Samwise Gamgee\nrace: Hobbit\nfun_fact: Frodo's gardener and friend\nfavorite_food: Potatoes\nskills: ['Loyalty', 'Cooking']\nweapons: ['Barrow-blade']\n\nname: Gandalf\nrace: Maia\nfun_fact: Known as Gandalf the Grey and later as Gandalf the White\nfavorite_food: N/A\nskills: ['Wisdom', 'Magic']\nweapons: ['Glamdring', 'Staff']\n\nname: Aragorn\nrace: Human\nfun_fact: Heir of Isildur and rightful king of Gondor\nfavorite_food: Elvish waybread\nskills: ['Swordsmanship', 'Leadership']\nweapons: ['Andúril', 'Bow']\n\nname: Legolas\nrace: Elf\nfun_fact: Prince of the Woodland Realm\nfavorite_food: Lembas bread\nskills: ['Archery', 'Agility']\nweapons: ['Elven bow', 'Daggers']\n\nname: Gimli\nrace: Dwarf\nfun_fact: Son of Glóin\nfavorite_food: Meat\nskills: ['Axe fighting', 'Stout-heartedness']\nweapons: ['Battle axe', 'Throwing axes']\n\nname: Boromir\nrace: Human\nfun_fact: Son of Denethor, Steward of Gondor\nfavorite_food: Stew\nskills: ['Swordsmanship', 'Leadership']\nweapons: ['Sword', 'Shield']\n\nname: Meriadoc Brandybuck\nrace: Hobbit\nfun_fact: Member of the Fellowship\nfavorite_food: Ale\nskills: ['Stealth', 'Strategy']\nweapons: ['Elven dagger']\n\nname: Peregrin Took\nrace: Hobbit\nfun_fact: Often known simply as Pippin\nfavorite_food: Cakes\nskills: ['Curiosity', 'Bravery']\nweapons: ['Sword']\n\nname: Galadriel\nrace: Elf\nfun_fact: Lady of Lothlórien\nfavorite_food: N/A\nskills: ['Wisdom', 'Telepathy']\nweapons: ['Nenya (Ring of Power)']\n\nname: Elrond\nrace: Elf\nfun_fact: Lord of Rivendell\nfavorite_food: N/A\nskills: ['Wisdom', 'Healing']\nweapons: ['Sword']\n\nname: Eowyn\nrace: Human\nfun_fact: Niece of King Théoden of Rohan; slayer of the Witch-king\nfavorite_food: Bread\nskills: ['Swordsmanship', 'Courage']\nweapons: ['Sword', 'Shield']\n\nname: Faramir\nrace: Human\nfun_fact: Brother of Boromir\nfavorite_food: Bread\nskills: ['Archery', 'Strategy']\nweapons: ['Bow', 'Sword']\n\nname: Gollum\nrace: Hobbit-like creature\nfun_fact: Once the bearer of the One Ring, known as Sméagol\nfavorite_food: Raw fish\nskills: ['Stealth', 'Persuasion']\nweapons: ['Teeth and claws']\n\nname: Saruman\nrace: Maia\nfun_fact: Head of the White Council before being corrupted\nfavorite_food: N/A\nskills: ['Magic', 'Persuasion']\nweapons: ['Staff']\n\nname: Sauron\nrace: Maia\nfun_fact: The Dark Lord and creator of the One Ring\nfavorite_food: N/A\nskills: ['Necromancy', 'Deception']\nweapons: ['One Ring', 'Mace']\n\nname: Bilbo Baggins\nrace: Hobbit\nfun_fact: Original discoverer of the One Ring\nfavorite_food: Everything\nskills: ['Stealth', 'Story-telling']\nweapons: ['Sting']\n\nname: Théoden\nrace: Human\nfun_fact: King of Rohan\nfavorite_food: Meat\nskills: ['Leadership', 'Horsemanship']\nweapons: ['Herugrim', 'Sword']\n\nname: Treebeard\nrace: Ent\nfun_fact: Oldest of the Ents, protectors of Fangorn Forest\nfavorite_food: Water\nskills: ['Strength', 'Wisdom']\nweapons: ['None']\n\nname: Witch-king of Angmar\nrace: Undead/Nazgûl\nfun_fact: Leader of the Nazgûl\nfavorite_food: N/A\nskills: ['Fear-induction', 'Swordsmanship']\nweapons: ['Morgul-blade', 'Flail']\n\nname: Gríma Wormtongue\nrace: Human\nfun_fact: Advisor to King Théoden under Saruman's influence\nfavorite_food: N/A\nskills: ['Deception', 'Speechcraft']\nweapons: ['Knife']\n\nname: Éomer\nrace: Human\nfun_fact: Nephew of King Théoden; later king of Rohan\nfavorite_food: Meat\nskills: ['Swordsmanship', 'Horsemanship']\nweapons: ['Sword', 'Spear']\n\n\nIt is probably likely that GPT would not return 20 characters in the first request. If max_retries=0 then it would likely raise a Pydantic validation error. But since we have max_retries=4 then the instructor library sends back the validation error as a message and asks again. How exactly does it do that? We can look at the messages that we have logged for debugging.\n\n\nCode\nassert len(message_logger.log_messages) &gt; 1\nlen(message_logger.log_messages)\n\n\n2\n\n\n\n\nCode\nmessage_logger.log_messages\n\n\n[{'method': 'post',\n  'url': '/chat/completions',\n  'files': None,\n  'json_data': {'messages': [{'role': 'user',\n     'content': 'Who are the main characters from Lord of the Rings?.'}],\n   'model': 'gpt-4-0125-preview',\n   'tool_choice': {'type': 'function', 'function': {'name': 'Characters'}},\n   'tools': [{'type': 'function',\n     'function': {'name': 'Characters',\n      'description': 'Correctly extracted `Characters` with all the required parameters with correct types',\n      'parameters': {'$defs': {'Character': {'properties': {'name': {'title': 'Name',\n           'type': 'string'},\n          'race': {'title': 'Race', 'type': 'string'},\n          'fun_fact': {'title': 'Fun Fact', 'type': 'string'},\n          'favorite_food': {'title': 'Favorite Food', 'type': 'string'},\n          'skills': {'items': {'type': 'string'},\n           'title': 'Skills',\n           'type': 'array'},\n          'weapons': {'items': {'type': 'string'},\n           'title': 'Weapons',\n           'type': 'array'}},\n         'required': ['name',\n          'race',\n          'fun_fact',\n          'favorite_food',\n          'skills',\n          'weapons'],\n         'title': 'Character',\n         'type': 'object'}},\n       'properties': {'characters': {'items': {'$ref': '#/$defs/Character'},\n         'title': 'Characters',\n         'type': 'array'}},\n       'required': ['characters'],\n       'type': 'object'}}}]}},\n {'method': 'post',\n  'url': '/chat/completions',\n  'files': None,\n  'json_data': {'messages': [{'role': 'user',\n     'content': 'Who are the main characters from Lord of the Rings?.'},\n    {'role': 'assistant',\n     'content': '',\n     'tool_calls': [{'id': 'call_kjUg9ogoR1OdRr0OkmTzabue',\n       'function': {'arguments': '{\"characters\":[{\"name\":\"Frodo Baggins\",\"race\":\"Hobbit\",\"fun_fact\":\"Bearer of the One Ring\",\"favorite_food\":\"Mushrooms\",\"skills\":[\"Courage\",\"Stealth\"],\"weapons\":[\"Sting\",\"Elven Dagger\"]},{\"name\":\"Samwise Gamgee\",\"race\":\"Hobbit\",\"fun_fact\":\"Frodo\\'s gardener and friend\",\"favorite_food\":\"Potatoes\",\"skills\":[\"Loyalty\",\"Cooking\"],\"weapons\":[\"Barrow-blade\"]},{\"name\":\"Gandalf\",\"race\":\"Maia\",\"fun_fact\":\"Known as Gandalf the Grey and later as Gandalf the White\",\"favorite_food\":\"N/A\",\"skills\":[\"Wisdom\",\"Magic\"],\"weapons\":[\"Glamdring\",\"Staff\"]},{\"name\":\"Aragorn\",\"race\":\"Human\",\"fun_fact\":\"Heir of Isildur and rightful king of Gondor\",\"favorite_food\":\"Elvish waybread\",\"skills\":[\"Swordsmanship\",\"Leadership\"],\"weapons\":[\"Andúril\",\"Bow\"]},{\"name\":\"Legolas\",\"race\":\"Elf\",\"fun_fact\":\"Prince of the Woodland Realm\",\"favorite_food\":\"Lembas bread\",\"skills\":[\"Archery\",\"Agility\"],\"weapons\":[\"Elven bow\",\"Daggers\"]},{\"name\":\"Gimli\",\"race\":\"Dwarf\",\"fun_fact\":\"Son of Glóin\",\"favorite_food\":\"Meat\",\"skills\":[\"Axe fighting\",\"Stout-heartedness\"],\"weapons\":[\"Battle axe\",\"Throwing axes\"]}]}',\n        'name': 'Characters'},\n       'type': 'function'}]},\n    {'role': 'tool',\n     'tool_call_id': 'call_kjUg9ogoR1OdRr0OkmTzabue',\n     'name': 'Characters',\n     'content': \"Recall the function correctly, fix the errors and exceptions found\\n1 validation error for Characters\\ncharacters\\n  Value error, The number of characters must be at least 20, but it is 6 [type=value_error, input_value=[{'name': 'Frodo Baggins'...axe', 'Throwing axes']}], input_type=list]\\n    For further information visit https://errors.pydantic.dev/2.6/v/value_error\"}],\n   'model': 'gpt-4-0125-preview',\n   'tool_choice': {'type': 'function', 'function': {'name': 'Characters'}},\n   'tools': [{'type': 'function',\n     'function': {'name': 'Characters',\n      'description': 'Correctly extracted `Characters` with all the required parameters with correct types',\n      'parameters': {'$defs': {'Character': {'properties': {'name': {'title': 'Name',\n           'type': 'string'},\n          'race': {'title': 'Race', 'type': 'string'},\n          'fun_fact': {'title': 'Fun Fact', 'type': 'string'},\n          'favorite_food': {'title': 'Favorite Food', 'type': 'string'},\n          'skills': {'items': {'type': 'string'},\n           'title': 'Skills',\n           'type': 'array'},\n          'weapons': {'items': {'type': 'string'},\n           'title': 'Weapons',\n           'type': 'array'}},\n         'required': ['name',\n          'race',\n          'fun_fact',\n          'favorite_food',\n          'skills',\n          'weapons'],\n         'title': 'Character',\n         'type': 'object'}},\n       'properties': {'characters': {'items': {'$ref': '#/$defs/Character'},\n         'title': 'Characters',\n         'type': 'array'}},\n       'required': ['characters'],\n       'type': 'object'}}}]}}]\n\n\nIf you look through the above messages carefully you can see the retry asking logic.\nRecall the function correctly, fix the errors and exceptions found validation error for CharactersValue error, The number of characters must be at least 20, …\nYou can even use the structured output with some of the open source models. I would refer to the instructor blog or documentation for further information on that. I have not fully looked into the different patching modes yet. But here is a simple example of using MISTRAL7B through together.ai.\n\n\nCode\nres = llm(\n    messages=[dict(role=\"user\", content=\"Give me a character from a movie or book.\")],\n    response_model=Character,\n    max_retries=2,\n    **Models.MISTRAL7B,\n)\nprint(res.model_dump())\n\n\n{'name': 'Superman', 'race': 'Kryptonian', 'fun_fact': 'Can fly', 'favorite_food': 'Pizza', 'skills': ['Super strength', 'Flight', 'Heat vision', 'X-ray vision'], 'weapons': ['Laser vision', 'Heat vision', 'X-ray vision']}\n\n\n\n\nConclusion\nAgain, I really like the idea of using a single interface for interacting with multiple LLMs. I hope the space continues to mature so that more open source models and services support JSON mode and function calling. I think instructor is a cool library and the corresponding blog is interesting too. I also like the idea of logging all the outgoing prompts/messages just to make sure I fully understand what is happening under the hood."
  },
  {
    "objectID": "posts/dspy/dspy.html",
    "href": "posts/dspy/dspy.html",
    "title": "DSPy",
    "section": "",
    "text": "Intro\nDSPy kept popping up on my X timeline and I thought it looked pretty interesting, so I decided to take a few days to look into it. I didn’t get super deep into it yet, but I think I have a high level understanding. The library is fairly new IMO (as of writing this). There is excitement around it though and a growing community. I am hopeful that the documentation and library will continue to improve throughout the year. If you are completely new to DSPy I would suggest the following resources below.\n\nRead through the newer documentation here.\nCheckout the README from DSPY GitHub repo and the examples there.\nTry and code up some simple examples on your own data.\nCheckout the Discord server.\nSkim through or read some of the associated papers (see the paper links on the DSPy repo README). For example:\n\nDSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines(Khattab et al. (2023))\nDSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines(Singhvi et al. (2024))\n\nThere are also some decent videos on YouTube. Simply Search for DSPy LLM etc.\nFollow Omar Khattab\n\n\n\nENV Setup\npython3 -m venv env\nsource env/bin/activate\npip install dspy-ai\npip install openai --upgrade\npip install --upgrade notebook ipywidgets\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nBIG-Bench Hard Dataset - Penguins In a Table - Example\nWithin the BIG-Bench Hard dataset (Suzgun et al. 2022) there are various tasks. You can use one of these strings when using load_dataset to load in the corresponding records for that task.\n['tracking_shuffled_objects_seven_objects', 'salient_translation_error_detection', 'tracking_shuffled_objects_three_objects', 'geometric_shapes', 'object_counting', 'word_sorting', 'logical_deduction_five_objects', 'hyperbaton', 'sports_understanding', 'logical_deduction_seven_objects', 'multistep_arithmetic_two', 'ruin_names', 'causal_judgement', 'logical_deduction_three_objects', 'formal_fallacies', 'snarks', 'boolean_expressions', 'reasoning_about_colored_objects', 'dyck_languages', 'navigate', 'disambiguation_qa', 'temporal_sequences', 'web_of_lies', 'tracking_shuffled_objects_five_objects', 'penguins_in_a_table', 'movie_recommendation', 'date_understanding']\nWe will use the penguins_in_a_table task.\n\n\nCode\nfrom datasets import load_dataset\nimport dspy\n\nds = load_dataset(\"maveriq/bigbenchhard\", \"penguins_in_a_table\")[\"train\"]\nexamples = [dspy.Example({\"question\": r[\"input\"], \"answer\": r[\"target\"]}).with_inputs(\"question\") for r in ds]\nprint(f\"There are {len(examples)} examples.\")\ntrainset = examples[0:20]\nvalset = examples[20:]\n\n\nThere are 146 examples.\n\n\n\n\nCode\nexample = trainset[10]\nfor k, v in example.items():\n    print(f\"\\n{k.upper()}:\\n\")\n    print(v)\n\n\n\nQUESTION:\n\nHere is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We then delete the penguin named Bernard from the table.\nHow many penguins are more than 8 years old?\nOptions:\n(A) 1\n(B) 2\n(C) 3\n(D) 4\n(E) 5\n\nANSWER:\n\n(A)\n\n\nWe will use the DSPy OpenAI connector to make calls to gpt-3.5. Note that DSPy caches API calls so that subsequent calls with the same input will read from the cache instead of calling the OpenAI API a second time.\n\n\nCode\nllm = dspy.OpenAI(model=\"gpt-3.5-turbo-0125\", max_tokens=250)\ndspy.settings.configure(lm=llm)\n\n\nWe can test that the calls to OpenAI are working:\n\n\nCode\nllm(\"Testing testing, is anyone out there?\")\n\n\n[\"Hello! I'm here to help. What can I assist you with today?\"]\n\n\n\n\nCode\nllm(example.question)\n\n\n['There are 2 penguins who are more than 8 years old: Vincent (9 years old) and Gwen (8 years old). \\n\\nTherefore, the answer is (B) 2.']\n\n\nAt any point we can look at the last n calls to the llm:\n\n\nCode\nllm.inspect_history(n=2)\n\n\n\n\n\n\nTesting testing, is anyone out there? Hello! I'm here to help. What can I assist you with today?\n\n\n\n\n\n\n\nHere is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We then delete the penguin named Bernard from the table.\nHow many penguins are more than 8 years old?\nOptions:\n(A) 1\n(B) 2\n(C) 3\n(D) 4\n(E) 5 There are 2 penguins who are more than 8 years old: Vincent (9 years old) and Gwen (8 years old). \n\nTherefore, the answer is (B) 2.\n\n\n\n\nOur evaluation metric will check if the llm output contains the correct multiple choice answer. To define an evaluation metric in DSPy we create a function like the example below. The first two inputs should be instances of dspy.Example. The metric function can contain any logic you need to evaluate your task. You can read more about the trace argument in the documentation. It needs to be there, even if not explicitly using it.\n\n\nCode\nimport re\n\n\ndef eval_metric(true, prediction, trace=None):\n    pred = prediction.answer\n    matches = re.findall(r\"\\([A-Z]\\)\", pred)\n    parsed_answer = matches[-1] if matches else \"\"\n    return parsed_answer == true.answer\n\n\nWe set up an evaluation pipeline:\n\n\nCode\nfrom dspy.evaluate import Evaluate\n\nevaluate = Evaluate(devset=valset, metric=eval_metric, num_threads=6, display_progress=True, display_table=10)\n\n\nHere is a simple module in DSPy for basic question and answer.\n\n\nCode\nclass BasicQA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.prog = dspy.Predict(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.prog(question=question)\n\n\nbasic_qa = BasicQA()\n\n\nThe forward method calls __call__ similar to how things work in pytorch.\n\n\nCode\npred = basic_qa(question=example.question)\nprint(\"\\nQUESTION:\\n\")\nprint(example.question)\nprint(\"\\nANSWER:\\n\")\nprint(example.answer)\nprint(\"\\nPREDICTION:\\n\")\nprint(pred.answer)\n\n\n\nQUESTION:\n\nHere is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We then delete the penguin named Bernard from the table.\nHow many penguins are more than 8 years old?\nOptions:\n(A) 1\n(B) 2\n(C) 3\n(D) 4\n(E) 5\n\nANSWER:\n\n(A)\n\nPREDICTION:\n\n(B) 2\n\n\n\n\nCode\neval_metric(example, pred)\n\n\nFalse\n\n\n\n\nCode\nllm.inspect_history(n=1)\n\n\n\n\n\n\nGiven the fields `question`, produce the fields `answer`.\n\n---\n\nFollow the following format.\n\nQuestion: ${question}\nAnswer: ${answer}\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. We then delete the penguin named Bernard from the table. How many penguins are more than 8 years old? Options: (A) 1 (B) 2 (C) 3 (D) 4 (E) 5\nAnswer: (B) 2\n\n\n\n\nNow we can pass each example question through the LLM in the validation set and check if we get the correct answer:\n\n\nCode\nevaluate(basic_qa)\n\n\nAverage Metric: 44 / 126  (34.9%)\n\n\n\n\n\n\n\n \nquestion\nexample_answer\npred_answer\neval_metric\n\n\n\n\n0\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\n3\nFalse\n\n\n1\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\n(C) 50\nFalse\n\n\n2\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nAnswer: (C) 3\nFalse\n\n\n3\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nAnswer: (B) 2\nFalse\n\n\n4\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(B)\n(B) 5\n✔️ [True]\n\n\n5\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\n(B) 2\nFalse\n\n\n6\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(E)\nJames\nFalse\n\n\n7\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\n(B) 2\nFalse\n\n\n8\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\nAnswer: Vincent\nFalse\n\n\n9\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\nAnswer: Donna\nFalse\n\n\n\n\n\n\n                \n                    ... 116 more rows not displayed ...\n                \n                \n\n\n34.92\n\n\nDSPy uses optimizers to optimize the modules. In this example, optimization is a process that will choose which demos/examples are best to put into the prompt in order to increase the evaluation metric. At the time of writing the optimizers are called teleprompters (prompting from a distance). I think they will change the name though to optimizers in future refactoring. The DSPy documentation states that the optimizer can adjust/edit:\n\nDemo examples in the prompt.\nInstructions of the prompt.\nWeights of the actual LLM (for example fine tuning an open source model).\n\nI have only played around with optimizers that optimize which demos/examples are put into the prompt.\n\n\nCode\nfrom dspy.teleprompt import BootstrapFewShotWithRandomSearch\n\nconfig = dict(max_bootstrapped_demos=2, max_labeled_demos=4, num_candidate_programs=2, num_threads=6)\n\nteleprompter = BootstrapFewShotWithRandomSearch(metric=eval_metric, **config)\noptimized_qa = teleprompter.compile(basic_qa, trainset=trainset, valset=valset)\n\n\nThere is a lot of output from the above code block which I am hiding to keep things cleaner. You can now evaluate the optimized model to see if the accuracy has improved.\n\n\nCode\nevaluate(optimized_qa)\n\n\nAverage Metric: 54 / 126  (42.9%)\n\n\n\n\n\n\n\n \nquestion\nexample_answer\npred_answer\neval_metric\n\n\n\n\n0\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\n(C)\nFalse\n\n\n1\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\n(C) 50\nFalse\n\n\n2\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\n(B)\nFalse\n\n\n3\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\n(B)\nFalse\n\n\n4\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(B)\n(B)\n✔️ [True]\n\n\n5\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\n(C)\n✔️ [True]\n\n\n6\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(E)\n(D)\nFalse\n\n\n7\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\n(B)\nFalse\n\n\n8\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\n(D) Gwen\nFalse\n\n\n9\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\n(D) Donna\n✔️ [True]\n\n\n\n\n\n\n                \n                    ... 116 more rows not displayed ...\n                \n                \n\n\n42.86\n\n\n\n\nCode\nllm.inspect_history()\n\n\n\n\n\n\nGiven the fields `question`, produce the fields `answer`.\n\n---\n\nFollow the following format.\n\nQuestion: ${question}\nAnswer: ${answer}\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. And here is a similar table, but listing giraffes: name, age, height (cm), weight (kg) Jody, 5, 430, 620 Gladys, 10, 420, 590 Marian, 2, 310, 410 Donna, 9, 440, 650 How many giraffes are more than 5 years old? Options: (A) 1 (B) 2 (C) 3 (D) 4 (E) 5\nAnswer: (B)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. What is the name of the last penguin sorted by alphabetic order? Options: (A) Louis (B) Bernard (C) Vincent (D) Gwen (E) James\nAnswer: (C)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. We now add a penguin to the table: James, 12, 90, 12 We then delete the penguin named Bernard from the table. How many penguins are more than 5 years old and weight more than 12 kg? Options: (A) 1 (B) 2 (C) 3 (D) 4 (E) 5\nAnswer: (A)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. How many animals are listed in the table? Options: (A) 1 (B) 2 (C) 3 (D) 4 (E) 5\nAnswer: (D)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. Which is the second heaviest penguin? Options: (A) Louis (B) Bernard (C) Vincent (D) Gwen (E) James\nAnswer: (B) Bernard\n\n\n\n\nNow we can try a Chain of Thought (Wei et al. 2023) prompt.\n\n\nCode\nclass CoT(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.prog = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.prog(question=question)\n\n\ncot_qa = CoT()\n\n\n\n\nCode\nevaluate(cot_qa)\n\n\nAverage Metric: 90 / 126  (71.4%)\n\n\n\n\n\n\n\n \nquestion\nexample_answer\nrationale\npred_answer\neval_metric\n\n\n\n\n0\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. We first identify the penguins who are less than 8 years old. From the table, we see that Louis is 7 years...\n(B) 2\nFalse\n\n\n1\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\nproduce the answer. We need to add up the weights of all the penguins in the table. Louis weighs 11 kg, Bernard weighs 13 kg,...\n(D) 62\n✔️ [True]\n\n\n2\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. We need to go through each penguin's age and count how many are more than 8 years old.\n(C) 3\nFalse\n\n\n3\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. We need to identify the penguins who are both more than 5 years old and weigh more than 12 kg. Looking at...\n(C) 3\nFalse\n\n\n4\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(B)\nproduce the answer. We can see from the table that Bernard's age is 5.\n(B) 5\n✔️ [True]\n\n\n5\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\nproduce the answer. We first identify the penguins who are less than 10 years old. Louis is 7 years old, Bernard is 5 years old,...\n(D) 4\nFalse\n\n\n6\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(E)\nproduce the answer. We need to identify the last penguin added to the table. By looking at the last entry in the penguin table, we...\nJames\nFalse\n\n\n7\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. We first need to identify the penguins who are more than 5 years old and weigh more than 12 kg. From the...\n(A) 1\n✔️ [True]\n\n\n8\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\nproduce the answer. We need to find the penguin with a height of 60 cm. Looking at the table, we see that Vincent is the...\n(C) Vincent\n✔️ [True]\n\n\n9\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\nproduce the answer. We need to look at the last entry in the table listing giraffes. The last giraffe listed is Donna.\n(D) Donna\n✔️ [True]\n\n\n\n\n\n\n                \n                    ... 116 more rows not displayed ...\n                \n                \n\n\n71.43\n\n\n\n\nCode\nllm.inspect_history()\n\n\n\n\n\n\nGiven the fields `question`, produce the fields `answer`.\n\n---\n\nFollow the following format.\n\nQuestion: ${question}\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\nAnswer: ${answer}\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. We now add a penguin to the table: James, 12, 90, 12 Which penguin is taller than the other ones? Options: (A) Louis (B) Bernard (C) Vincent (D) Gwen (E) James\nReasoning: Let's think step by step in order to produce the answer. We need to compare the height of each penguin in the table and determine which one is the tallest. Louis is 50 cm tall, Bernard is 80 cm tall, Vincent is 60 cm tall, Gwen is 70 cm tall, and James is 90 cm tall. Therefore, James is taller than all the other penguins.\nAnswer: (E) James\n\n\n\n\nNow we will try and optimize our chain of thought program. I am also hiding the output from this cell to keep things cleaner.\n\n\nCode\ntqdm._instances.clear()\nconfig = dict(max_bootstrapped_demos=1, max_labeled_demos=4, num_candidate_programs=4, num_threads=6)\nteleprompter = BootstrapFewShotWithRandomSearch(metric=eval_metric, **config)\noptimized_cot_qa = teleprompter.compile(cot_qa, trainset=trainset, valset=valset)\n\n\n\n\nCode\nevaluate(optimized_cot_qa)\n\n\nAverage Metric: 102 / 126  (81.0%)\n\n\n\n\n\n\n\n \nquestion\nexample_answer\nrationale\npred_answer\neval_metric\n\n\n\n\n0\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. After deleting Bernard, the penguins left are Louis, Vincent, and Gwen. Among them, Louis and Gwen are less than 8 years old.\n(B) 2\nFalse\n\n\n1\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\nproduce the answer. We sum up the weights of all the penguins: 11 + 13 + 11 + 15 + 12 = 62.\n(D) 62\n✔️ [True]\n\n\n2\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. We know that after deleting Bernard, the penguins left are Louis, Vincent, and Gwen. Among them, only Vincent is more than 8...\n(A) 1\n✔️ [True]\n\n\n3\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. We have Louis, Vincent, Gwen, and James in the table. Among them, only James is more than 5 years old and weighs...\n(A) 1\n✔️ [True]\n\n\n4\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(B)\nproduce the answer. We know that the age of Bernard is 5 years old.\n(B) 5\n✔️ [True]\n\n\n5\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\nproduce the answer. We know that after deleting Bernard, the penguins left are Louis, Vincent, and Gwen. Among them, Louis and Gwen are less than...\n(B) 2\nFalse\n\n\n6\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(E)\nproduce the answer. We know that the last penguin added to the table is James.\n(E) James\n✔️ [True]\n\n\n7\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. After deleting Bernard, we are left with Louis, Vincent, and Gwen. Among them, only Gwen is more than 5 years old and...\n(A) 1\n✔️ [True]\n\n\n8\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\nproduce the answer. We know that the only penguin with a height of 60 cm is Vincent.\n(C) Vincent\n✔️ [True]\n\n\n9\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\nproduce the answer. We know that the last giraffe listed is Donna.\n(D) Donna\n✔️ [True]\n\n\n\n\n\n\n                \n                    ... 116 more rows not displayed ...\n                \n                \n\n\n80.95\n\n\n\n\nCode\nllm.inspect_history(n=1)\n\n\n\n\n\n\nGiven the fields `question`, produce the fields `answer`.\n\n---\n\nFollow the following format.\n\nQuestion: ${question}\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\nAnswer: ${answer}\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. We then delete the penguin named Bernard from the table. How many penguins are more than 8 years old? Options: (A) 1 (B) 2 (C) 3 (D) 4 (E) 5\nReasoning: Let's think step by step in order to produce the answer. We know that after deleting Bernard, the penguins left are Louis, Vincent, and Gwen. Among them, only Vincent is more than 8 years old.\nAnswer: (A) 1\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. How many penguins are more than 5 years old? Options: (A) 1 (B) 2 (C) 3 (D) 4 (E) 5\nAnswer: (C)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. And here is a similar table, but listing giraffes: name, age, height (cm), weight (kg) Jody, 5, 430, 620 Gladys, 10, 420, 590 Marian, 2, 310, 410 Donna, 9, 440, 650 How many animals are more than 5 years old? Options: (A) 5 (B) 6 (C) 7 (D) 8 (E) 9\nAnswer: (A)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. Which penguin is older than Gwen? Options: (A) Louis (B) Bernard (C) Vincent (D) Gwen (E) James\nAnswer: (C)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. We then delete the penguin named Bernard from the table. What is the name of the last penguin sorted by alphabetic order? Options: (A) Louis (B) Bernard (C) Vincent (D) Gwen (E) James\nReasoning: Let's think step by step in order to produce the answer. After deleting Bernard, the remaining penguins are Louis, Vincent, and Gwen. Sorting them alphabetically, the last penguin is Vincent.\nAnswer: (C) Vincent\n\n\n\n\nIt’s really nice that the above focused on:\n\nWriting small modules/programs.\nChoosing an optimizer.\nRunning the compile/optimization step.\nRunning an evaluation.\n\nI really like this idea instead of manually writing prompts and hoping for the best.\n\n\n\n\n\nReferences\n\nKhattab, Omar, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, et al. 2023. “DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines.” https://arxiv.org/abs/2310.03714.\n\n\nSinghvi, Arnav, Manish Shetty, Shangyin Tan, Christopher Potts, Koushik Sen, Matei Zaharia, and Omar Khattab. 2024. “DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines.” https://arxiv.org/abs/2312.13382.\n\n\nSuzgun, Mirac, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, et al. 2022. “Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them.” arXiv Preprint arXiv:2210.09261.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.” https://arxiv.org/abs/2201.11903."
  },
  {
    "objectID": "posts/basic_transformer_notes/transformers.html",
    "href": "posts/basic_transformer_notes/transformers.html",
    "title": "Basic Transformer Architecture Notes",
    "section": "",
    "text": "Here are some notes on the basic transformer architecture for my personal learning and understanding. Useful as a secondary resource, not the first stop. There are many resources out there, but here are several I enjoyed learning from:\n\nChapter 3 of the book Natural Language Processing With Transformers (Tunstall, Von Werra, and Wolf 2022)\nAndrej Karpathy’s video Let’s build GPT: from scratch, in code, spelled out (Karpathy 2023)\nSebastian Raschka’s Blog Post Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs (Raschka 2024)\nOmar Sanseviero’s Blog Post The Random Transformer (Sanseviero 2024)\nThe Illustrated Transformer (Alammar 2018)\nThe original paper: Attention Is All You Need (Vaswani et al. 2017)"
  },
  {
    "objectID": "posts/basic_transformer_notes/transformers.html#multi-head-attention",
    "href": "posts/basic_transformer_notes/transformers.html#multi-head-attention",
    "title": "Basic Transformer Architecture Notes",
    "section": "Multi Head Attention",
    "text": "Multi Head Attention\n\nThere are multiple attention heads, each with their own independent queries, keys, values.\nEach attention head takes the input embeddings of shape (B, T, C) and produces an output (B, T, H).\nConcatenate the outputs from each head so that the concatenated tensor is back to the original input shape (B, T, C).\nOnce we have the concatenated output tensor, we put it through a linear projection, nn.Linear(embed_dim, embed_dim) to get the output from the multi head attention: a tensor of shape (B, T, C)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chris Levy",
    "section": "",
    "text": "Hello! I’m Chris Levy. I work in ML/AI and backend Python development."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Chris Levy",
    "section": "About Me",
    "text": "About Me\nI spent a good amount of time in school where I completed a PhD in applied math back in 2015. After graduating I shifted away from academia and started working in industry. I mostly do backend python development these days, and build ML/AI applications/services. I work across the entire stack from research, to training and evaluating models, to deploying models, and getting in the weeds of the infrastructure and devops pipelines.\nOutside of AI/ML stuff, I enjoy spending time with my family and three kids, working out, swimming, cycling, and playing guitar."
  }
]