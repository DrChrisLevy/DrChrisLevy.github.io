[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Intro to LLMs\n\n\nLunch and Learn Talk\n\n\n\n\n\n\n\n\n\nMay 1, 2024\n\n\nChris Levy\n\n\n\n\n\n\n  \n\n\n\n\nUsing Modal to Transcribe YouTube Videos with Whisper\n\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2024\n\n\nChris Levy\n\n\n\n\n\n\n  \n\n\n\n\nGetting Started with Axolotl for Fine-Tuning LLMs\n\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2024\n\n\nChris Levy\n\n\n\n\n\n\n  \n\n\n\n\nFunction Calling with Hermes-2-Pro-Mistral-7B\n\n\n\n\n\n\n\n\n\n\n\n\nMar 16, 2024\n\n\nChris Levy\n\n\n\n\n\n\n  \n\n\n\n\nOpenAI Compatible LLM Inference\n\n\nA Single Inference Wrapper for OpenAI, Together AI, Hugging Face Inference TGI, Ollama, etc.\n\n\n\n\n\n\n\n\n\nMar 8, 2024\n\n\nChris Levy\n\n\n\n\n\n\n  \n\n\n\n\nDSPy\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nChris Levy\n\n\n\n\n\n\n  \n\n\n\n\nBasic Transformer Architecture Notes\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2024\n\n\nChris Levy\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#nlp-through-the-years",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#nlp-through-the-years",
    "title": "Intro to LLMs",
    "section": "NLP Through The Years",
    "text": "NLP Through The Years\n\nELIZA (MIT),1964-1967, CS25 V4: Lecture 1 (Spring 2024)"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#word-embeddings",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#word-embeddings",
    "title": "Intro to LLMs",
    "section": "Word Embeddings",
    "text": "Word Embeddings\n\nrepresent each word as an embedding (vector of numbers)\nuseful computations such as distance (cosine/euclidean)\nmapping of words onto a semantic space\nexample: Word2Vec (2013), GloVe, BERT, ELMo"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#attention-and-transformers",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#attention-and-transformers",
    "title": "Intro to LLMs",
    "section": "Attention and Transformers",
    "text": "Attention and Transformers\n\nImage Source: nlp-with-transformers book"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#transformer-multi-head-attention",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#transformer-multi-head-attention",
    "title": "Intro to LLMs",
    "section": "Transformer & Multi-Head Attention",
    "text": "Transformer & Multi-Head Attention\n\nAttention Is All you Need: Paper"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#what-is-a-llm-large-language-model",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#what-is-a-llm-large-language-model",
    "title": "Intro to LLMs",
    "section": "What is a LLM (large language model)?",
    "text": "What is a LLM (large language model)?\n\nLLMs are scaled up versions of the Transformer architecture (millions/billions of parameters)\nMost modern LLMs are decoder only transformers\nTrained on massive amounts of ‚Äúgeneral‚Äù textual data\nTraining objective is typically ‚Äúnext token prediction‚Äù: P(Wt+1|Wt,Wt-1,‚Ä¶,W1)"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#next-token-prediction",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#next-token-prediction",
    "title": "Intro to LLMs",
    "section": "Next Token Prediction",
    "text": "Next Token Prediction\n\nLLMs are next token predictors\n‚ÄúIt is raining today, so I will take my _______.‚Äù"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#tokenization-with-tiktoken-library",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#tokenization-with-tiktoken-library",
    "title": "Intro to LLMs",
    "section": "Tokenization with tiktoken library",
    "text": "Tokenization with tiktoken library\n\nThe first step is to convert the input text into tokens\nEach token has an id in the vocabulary\n\n\nimport tiktoken\n\nenc = tiktoken.encoding_for_model(\"gpt-4-0125\")\nencoded_text = enc.encode(\"tiktoken is great!\")\nencoded_text\n\n[83, 1609, 5963, 374, 2294, 0]\n\n\n\n[enc.decode([token]) for token in encoded_text]\n\n['t', 'ik', 'token', ' is', ' great', '!']\n\n\n\nenc.decode([83, 1609, 5963, 374, 2294, 0])\n\n'tiktoken is great!'"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#tokenization-with-transformers-library",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#tokenization-with-transformers-library",
    "title": "Intro to LLMs",
    "section": "Tokenization with transformers library",
    "text": "Tokenization with transformers library\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ntexts = [\n    \"I love summer\",\n    \"I love tacos\",\n]\ninputs = tokenizer(\n    texts,\n    return_tensors=\"pt\",\n    padding=\"max_length\",\n    max_length=16,\n    truncation=True,\n).input_ids\nprint(inputs)\n\nprint(inputs.shape)  # (B, T)\nprint(tokenizer.vocab_size)\nfor row in inputs:\n    print(tokenizer.convert_ids_to_tokens(row))\n\ntensor([[  101,  1045,  2293,  2621,   102,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [  101,  1045,  2293, 11937, 13186,   102,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0]])\ntorch.Size([2, 16])\n30522\n['[CLS]', 'i', 'love', 'summer', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n['[CLS]', 'i', 'love', 'ta', '##cos', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#tokenization-is-the-first-step",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#tokenization-is-the-first-step",
    "title": "Intro to LLMs",
    "section": "Tokenization is the First Step",
    "text": "Tokenization is the First Step"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#llms-are-not-great-at-math.-why",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#llms-are-not-great-at-math.-why",
    "title": "Intro to LLMs",
    "section": "LLMS are not great at math. Why?",
    "text": "LLMS are not great at math. Why?\n\nbecause of tokenization and next token prediction\n\nWhat is the average of: 2009 1746 4824 8439\n\n\nencoded_text = enc.encode(\"What is the average of:  2009 1746 4824 8439\")\nprint(encoded_text)\n\n[3923, 374, 279, 5578, 315, 25, 220, 220, 1049, 24, 220, 11771, 21, 220, 21984, 19, 220, 23996, 24]\n\n\n\nprint([enc.decode([token]) for token in encoded_text])\n\n['What', ' is', ' the', ' average', ' of', ':', ' ', ' ', '200', '9', ' ', '174', '6', ' ', '482', '4', ' ', '843', '9']"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#basic-transformer-architecture---futher-reading",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#basic-transformer-architecture---futher-reading",
    "title": "Intro to LLMs",
    "section": "Basic Transformer Architecture - Futher Reading",
    "text": "Basic Transformer Architecture - Futher Reading\n\nLots of resources online\nSome of the ones I enjoyed while learning:\n\nChapter 3 of the book Natural Language Processing With Transformers\nAndrej Karpathy‚Äôs video Let‚Äôs build GPT: from scratch, in code, spelled out\nSebastian Raschka‚Äôs Blog Post Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs\nOmar Sanseviero‚Äôs Blog Post The Random Transformer\nThe Illustrated Transformer\nThe original paper: Attention Is All You Need"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#base-models-vs-instruct-models",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#base-models-vs-instruct-models",
    "title": "Intro to LLMs",
    "section": "Base Models VS Instruct Models",
    "text": "Base Models VS Instruct Models\n\nmeta-llama/Meta-Llama-3-8B (base model)"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#base-models-vs-instruct-models-1",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#base-models-vs-instruct-models-1",
    "title": "Intro to LLMs",
    "section": "Base Models VS Instruct Models",
    "text": "Base Models VS Instruct Models\n\nmeta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#popular-instruction-fine-tuned-llms",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#popular-instruction-fine-tuned-llms",
    "title": "Intro to LLMs",
    "section": "Popular Instruction Fine-Tuned LLMs",
    "text": "Popular Instruction Fine-Tuned LLMs\n\nclosed\n\nOpen AI: gpt-4-turbo-2024-04-09, gpt-3.5-turbo-0125, etc.\nAnthropic: opus, sonnet, haiku\nGoogle, Gemini 1.5\n\nopen\n\nMeta: Llama-3-8B-Instruct, Llama-3-70B-Instruct\nMistral: Mistral 7B, Mixtral 8x7B, Mixtral 8x22B\nQwen: Qwen-1.8B, Qwen-7B, Qwen-14B, Qwen-72B\nHuggingFace: Zephyr-ORPO-141b-A35b-v0.1\nDatabricks: DBRX-Instruct-Preview\nNousResearch: Hermes-2-Pro-Mistral-7B,\nCohere: Command R+"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#the-gap-is-closing",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#the-gap-is-closing",
    "title": "Intro to LLMs",
    "section": "The Gap is closing",
    "text": "The Gap is closing\n\nimage source - Maxime Labonne, üèÜ LMSYS Chatbot Arena\nanother fun animation"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#aligning-language-models",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#aligning-language-models",
    "title": "Intro to LLMs",
    "section": "Aligning language models",
    "text": "Aligning language models\n\nThere is so much theory/research behind creating instruction models\nNot going to cover that here\nCheckout this recent talk, Aligning open language models, from Nathan Lambert\nState of GPT Keynote By Andrej Karpathy\nLarge Language Model Course by Maxime Labonne"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#openai-compatible-llm-inference-1",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#openai-compatible-llm-inference-1",
    "title": "Intro to LLMs",
    "section": "OpenAI Compatible LLM Inference",
    "text": "OpenAI Compatible LLM Inference\n\nimport openai\n\nclient = openai.OpenAI()\nchat_completion = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-0125\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Who are the main characters from Lord of the Rings?.\"},\n    ],\n)\nresponse = chat_completion.choices[0].message.content\nprint(response)\n\nThe main characters from Lord of the Rings are Frodo Baggins, Samwise Gamgee, Aragorn, Legolas, Gimli, Gandalf, Boromir, Merry and Pippin, and Gollum."
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#openai-compatible-llm-inference-2",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#openai-compatible-llm-inference-2",
    "title": "Intro to LLMs",
    "section": "OpenAI Compatible LLM Inference",
    "text": "OpenAI Compatible LLM Inference\n\ntogether.ai\n\nimport openai\n\nclient = openai.OpenAI(api_key=os.environ.get(\"TOGETHER_API_KEY\"), base_url=\"https://api.together.xyz/v1\")\nchat_completion = client.chat.completions.create(\n    model=\"META-LLAMA/LLAMA-3-70B-CHAT-HF\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Who are the main characters from Lord of the Rings?.\"},\n    ],\n)\nresponse = chat_completion.choices[0].message.content\nprint(response)"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#openai-compatible-llm-inference-3",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#openai-compatible-llm-inference-3",
    "title": "Intro to LLMs",
    "section": "OpenAI Compatible LLM Inference",
    "text": "OpenAI Compatible LLM Inference\n\ntogether.ai\n\n\nimport openai\n\nclient = openai.OpenAI(api_key=os.environ.get(\"TOGETHER_API_KEY\"), base_url=\"https://api.together.xyz/v1\")\nchat_completion = client.chat.completions.create(\n    model=\"META-LLAMA/LLAMA-3-70B-CHAT-HF\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Who are the main characters from Lord of the Rings?.\"},\n    ],\n)\nresponse = chat_completion.choices[0].message.content\nprint(response)\n\nThe main characters from J.R.R. Tolkien's epic fantasy novel \"The Lord of the Rings\" are:\n\n1. **Frodo Baggins**: The hobbit who inherits the One Ring from Bilbo Baggins and undertakes the perilous journey to destroy it in the fires of Mount Doom.\n2. **Samwise Gamgee** (Sam): Frodo's loyal hobbit servant and friend, who accompanies him on his quest.\n3. **Aragorn (Strider)**: A human warrior who becomes the leader of the Fellowship of the Ring and helps guide Frodo on his journey. He is the rightful King of Gondor.\n4. **Legolas**: An elf archer who joins the Fellowship and provides skilled marksmanship and agility.\n5. **Gimli**: A dwarf warrior who joins the Fellowship and provides strength and combat skills.\n6. **Gandalf the Grey**: A powerful wizard who helps guide Frodo on his quest and provides wisdom and magical assistance.\n7. **Boromir**: A human warrior from the land of Gondor, who joins the Fellowship but ultimately tries to take the Ring from Frodo.\n8. **Merry Brandybuck** and **Pippin Took**: Frodo's hobbit cousins, who join the Fellowship and provide comic relief and bravery in the face of danger.\n9. **Sauron**: The primary antagonist, a dark lord who created the One Ring and seeks to conquer Middle-earth.\n10. **Saruman**: A wizard who betrays Gandalf and allies himself with Sauron, seeking to gain power and control over Middle-earth.\n\nThese characters form the core of the story, and their interactions and relationships drive the plot of \"The Lord of the Rings\"."
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#openai-compatible-llm-inference-4",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#openai-compatible-llm-inference-4",
    "title": "Intro to LLMs",
    "section": "OpenAI Compatible LLM Inference",
    "text": "OpenAI Compatible LLM Inference\n\nlocal inference with ollama\n\n\nimport openai\n\nclient = openai.OpenAI(api_key=\"ollama\", base_url=\"http://localhost:11434/v1\")\nchat_completion = client.chat.completions.create(\n    model=\"llama3\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Who are the main characters from Lord of the Rings?.\"},\n    ],\n)\nresponse = chat_completion.choices[0].message.content\nprint(response)\n\nThe main characters in J.R.R. Tolkien's \"Lord of the Rings\" trilogy, which includes \"The Fellowship of the Ring\", \"The Two Towers\", and \"The Return of the King\", are:\n\n1. Frodo Baggins: The hobbit who inherits the One Ring from Bilbo and sets out on a quest to destroy it in the fires of Mount Doom.\n2. Samwise Gamgee (Sam): Frodo's loyal hobbit servant and friend, who accompanies him on his journey to Mordor.\n3. Aragorn (Strider): A human warrior who leads the Fellowship and helps them navigate the perilous lands of Middle-earth.\n4. Legolas: An elf archer who joins the Fellowship and fights alongside them against Sauron's armies.\n5. Gimli: A dwarf warrior who also joins the Fellowship, seeking to avenge his father's death at the hands of orcs.\n6. Boromir: The human son of the Steward of Gondor, who tries to take the One Ring from Frodo for the benefit of his own people.\n7. Meriadoc Brandybuck (Merry) and Peregrin Took (Pippin): Two hobbit friends of Frodo's who accompany him on his journey and become embroiled in the quest to destroy the Ring.\n\nThese characters, along with Gandalf the Grey, a powerful wizard, and other supporting characters, drive the story and its themes of friendship, sacrifice, and the struggle against evil."
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#chat-templates",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#chat-templates",
    "title": "Intro to LLMs",
    "section": "Chat Templates",
    "text": "Chat Templates\n\nfrom transformers import AutoTokenizer\n\ncheckpoint = \"meta-llama/Meta-Llama-3-8B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n\nEach model has its own expected input format. For Llama3 it‚Äôs this:\n\n\"\"\"\n&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\nYou are a friendly chatbot who always responds in the style of a pirate&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\nHow many helicopters can a human eat in one sitting?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\"\"\"\n\nWith chat templates we can use this familiar standard:\n\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n    },\n    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n]\ntokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\nprint(tokenizer.decode(tokenized_chat[0]))\n\n&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\nYou are a friendly chatbot who always responds in the style of a pirate&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\nHow many helicopters can a human eat in one sitting?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#structured-output-1",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#structured-output-1",
    "title": "Intro to LLMs",
    "section": "Structured Output",
    "text": "Structured Output\n\nimport openai\n\nclient = openai.OpenAI()\nchat_completion = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-0125\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Who are the main characters from Lord of the Rings?. \"\n            \"For each character give the name, race, \"\n            \"favorite food, skills, weapons, and a fun fact.\",\n        },\n    ],\n)\nresponse = chat_completion.choices[0].message.content\nprint(response)\n\n1. Frodo Baggins\n- Race: Hobbit\n- Favorite food: Mushrooms\n- Skills: Determination, stealth, resilience\n- Weapons: Sting (his sword)\n- Fun fact: Frodo is the only character to have directly interacted with the One Ring and survived its corrupting influence.\n\n2. Aragorn (also known as Strider)\n- Race: Human (Dunedain)\n- Favorite food: Lembas bread\n- Skills: Swordsmanship, tracking, leadership\n- Weapons: Anduril (his sword), bow and arrows\n- Fun fact: Aragorn is the heir to the throne of Gondor and the rightful King of Arnor.\n\n3. Gandalf\n- Race: Maia (wizard)\n- Favorite food: Pipe-weed\n- Skills: Magic, wisdom, leadership\n- Weapons: Glamdring (his sword), staff\n- Fun fact: Gandalf is actually one of the Maiar, a group of powerful beings who serve the Valar (gods) in the world of Middle-earth.\n\n4. Legolas\n- Race: Elf\n- Favorite food: Waybread (Lembas)\n- Skills: Archery, agility, keen eyesight\n- Weapons: Bow and arrows, knives\n- Fun fact: Legolas is the son of Thranduil, the Elven King of the Woodland Realm in Mirkwood.\n\n5. Gimli\n- Race: Dwarf\n- Favorite food: Roast meats\n- Skills: Axe-fighting, mining, loyalty\n- Weapons: Axe, throwing axes\n- Fun fact: Gimli is a member of the Fellowship representing the Dwarves, who are known for their craftsmanship and love of gold and jewels."
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#structured-output-2",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#structured-output-2",
    "title": "Intro to LLMs",
    "section": "Structured Output",
    "text": "Structured Output\n\nJSON mode and Function Calling give us structured output\ninstructor - library - ‚ÄúPydantic is all you need‚Äù\n\n\nimport openai\nimport instructor\nfrom pydantic import BaseModel\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\n# Define your desired output structure\nclass UserInfo(BaseModel):\n    name: str\n    age: int\n\n\n# Extract structured data from natural language\nuser_info = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-0125\",\n    response_model=UserInfo,\n    messages=[{\"role\": \"user\", \"content\": \"Chris is 38 years old.\"}],\n)\nprint(user_info.model_dump())\nprint(user_info.name)\nprint(user_info.age)\n\n{'name': 'Chris', 'age': 38}\nChris\n38"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#structured-output-3",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#structured-output-3",
    "title": "Intro to LLMs",
    "section": "Structured Output",
    "text": "Structured Output\n\nimport openai\nimport instructor\nfrom typing import List\n\nfrom pydantic import BaseModel, field_validator\n\nclient = instructor.from_openai(openai.OpenAI())\n\n\nclass Character(BaseModel):\n    name: str\n    race: str\n    fun_fact: str\n    favorite_food: str\n    skills: List[str]\n    weapons: List[str]\n\n\nclass Characters(BaseModel):\n    characters: List[Character]\n\n    @field_validator(\"characters\")\n    @classmethod\n    def validate_characters(cls, v):\n        if len(v) &lt; 10:\n            raise ValueError(f\"The number of characters must be at least 10, but it is {len(v)}\")\n        return v\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-0125\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Who are the main characters from Lord of the Rings?. \"\n            \"For each character give the name, race, \"\n            \"favorite food, skills, weapons, and a fun fact. Give me at least 10 different characters.\",\n        },\n    ],\n    response_model=Characters,\n    max_retries=4,\n)\n\nfrom pprint import pprint\n\npprint(response.model_dump())\n\n{'characters': [{'favorite_food': 'Mushrooms',\n                 'fun_fact': 'Frodo is the nephew of Bilbo Baggins.',\n                 'name': 'Frodo Baggins',\n                 'race': 'Hobbit',\n                 'skills': ['Ringbearer', 'Stealth', 'Courage'],\n                 'weapons': ['Sting', 'Phial of Galadriel']},\n                {'favorite_food': 'Lembas bread',\n                 'fun_fact': 'Aragorn is the rightful heir to the throne of '\n                             'Gondor.',\n                 'name': 'Aragorn',\n                 'race': 'Man',\n                 'skills': ['Swordsmanship', 'Leadership', 'Tracking'],\n                 'weapons': ['Anduril', 'Bow and Arrow']},\n                {'favorite_food': 'Roast Pork',\n                 'fun_fact': 'Gimli is the son of Gloin, one of the Dwarves in '\n                             \"'The Hobbit'.\",\n                 'name': 'Gimli',\n                 'race': 'Dwarf',\n                 'skills': ['Axe throwing', 'Smithing', 'Courage'],\n                 'weapons': ['Axe', 'Throwing Axe']},\n                {'favorite_food': 'Lembas bread',\n                 'fun_fact': 'Legolas has keen eyesight and can spot enemies '\n                             'from great distances.',\n                 'name': 'Legolas',\n                 'race': 'Elf',\n                 'skills': ['Archery', 'Agility', 'Sight'],\n                 'weapons': ['Bow', 'Arrow']},\n                {'favorite_food': 'Pipe-weed',\n                 'fun_fact': 'Gandalf is also known as Mithrandir in Elvish.',\n                 'name': 'Gandalf',\n                 'race': 'Maia',\n                 'skills': ['Wizardry', 'Wisdom', 'Combat'],\n                 'weapons': ['Glamdring', 'Staff']},\n                {'favorite_food': 'Venison',\n                 'fun_fact': 'Boromir hails from the realm of Gondor.',\n                 'name': 'Boromir',\n                 'race': 'Man',\n                 'skills': ['Swordsmanship', 'Leadership', 'Athletics'],\n                 'weapons': ['Sword', 'Shield']},\n                {'favorite_food': 'Potatoes',\n                 'fun_fact': 'Sam is known for his unwavering loyalty to '\n                             'Frodo.',\n                 'name': 'Samwise Gamgee',\n                 'race': 'Hobbit',\n                 'skills': ['Gardening', 'Loyalty', 'Cooking'],\n                 'weapons': ['Cooking pot', 'Gardening tools']},\n                {'favorite_food': 'Berry tarts',\n                 'fun_fact': 'Arwen is the daughter of Elrond, Lord of '\n                             'Rivendell.',\n                 'name': 'Arwen',\n                 'race': 'Half-Elf',\n                 'skills': ['Horseback riding', 'Healing', 'Sword fighting'],\n                 'weapons': ['Sword']},\n                {'favorite_food': 'Apple pie',\n                 'fun_fact': \"Merry is one of Frodo's close friends and part \"\n                             'of the Fellowship of the Ring.',\n                 'name': 'Merry Brandybuck',\n                 'race': 'Hobbit',\n                 'skills': ['Stealth', 'Swordsmanship', 'Cooking'],\n                 'weapons': ['Dagger', 'Sword']},\n                {'favorite_food': 'Mushrooms',\n                 'fun_fact': 'Pippin becomes a Knight of Gondor for his '\n                             'bravery in battle.',\n                 'name': 'Pippin Took',\n                 'race': 'Hobbit',\n                 'skills': ['Loyalty', 'Entertainment', 'Courage'],\n                 'weapons': ['Dagger', 'Sword']}]}"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#function-calling",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#function-calling",
    "title": "Intro to LLMs",
    "section": "Function Calling",
    "text": "Function Calling\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather_forecast\",\n            \"description\": \"Provides a weather forecast for a given location and date.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\"location\": {\"type\": \"string\"}, \"date\": {\"type\": \"string\"}},\n                \"required\": [\"location\", \"date\"],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"book_flight\",\n            \"description\": \"Book a flight.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"departure_city\": {\"type\": \"string\"},\n                    \"arrival_city\": {\"type\": \"string\"},\n                    \"departure_date\": {\"type\": \"string\"},\n                    \"return_date\": {\"type\": \"string\"},\n                    \"num_passengers\": {\"type\": \"integer\"},\n                    \"cabin_class\": {\"type\": \"string\"},\n                },\n                \"required\": [\n                    \"departure_city\",\n                    \"arrival_city\",\n                    \"departure_date\",\n                    \"return_date\",\n                    \"num_passengers\",\n                    \"cabin_class\",\n                ],\n            },\n        },\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"send_slack_message\",\n            \"description\": \"Send a slack message to specific channel.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\"channel_name\": {\"type\": \"string\"}, \"message\": {\"type\": \"string\"}},\n                \"required\": [\"channel_name\", \"message\"],\n            },\n        },\n    },\n]\n\nimport openai\nfrom datetime import date\nimport json\n\nclient = openai.OpenAI()\nchat_completion = client.chat.completions.create(\n    model=\"gpt-4-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": f\"Today's date is {date.today()}\"},\n        {\n            \"role\": \"user\",\n            \"content\": \"\"\"This coming Friday I need to book a flight from Halifax, NS to Austin, Texas. \n                                    It will be me and my friend and we need first class seats. \n                                    We will come back on Sunday. Let me know what I should pack for clothes \n                                    according to the weather there each day. Also please remind my team on \n                                    the DEV slack channel that I will be out of office on Friday. \n                                    1. Book the flight. \n                                    2. Let me know the weather. \n                                    3. Send the slack message.\"\"\",\n        },\n    ],\n    tools=tools,\n)\n\nfor tool in chat_completion.choices[0].message.tool_calls:\n    print(f\"function name: {tool.function.name}\")\n    print(f\"function arguments: {json.loads(tool.function.arguments)}\")\n    print()\n\nfunction name: book_flight\nfunction arguments: {'departure_city': 'Halifax', 'arrival_city': 'Austin', 'departure_date': '2024-05-03', 'return_date': '2024-05-05', 'num_passengers': 2, 'cabin_class': 'First'}\n\nfunction name: get_weather_forecast\nfunction arguments: {'location': 'Austin, Texas', 'date': '2024-05-03'}\n\nfunction name: get_weather_forecast\nfunction arguments: {'location': 'Austin, Texas', 'date': '2024-05-04'}\n\nfunction name: get_weather_forecast\nfunction arguments: {'location': 'Austin, Texas', 'date': '2024-05-05'}\n\nfunction name: send_slack_message\nfunction arguments: {'channel_name': 'DEV', 'message': 'I will be out of office this Friday, May 3, 2024. Please reach out via email if urgent.'}"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#rag-step-1---index-your-documents",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#rag-step-1---index-your-documents",
    "title": "Intro to LLMs",
    "section": "RAG: Step 1 - Index your Documents",
    "text": "RAG: Step 1 - Index your Documents\n\nRAG is a technique for augmenting LLM knowledge with additional data.\nimage source: langchain docs"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#rag-step-2---query-and-prompt-llm",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#rag-step-2---query-and-prompt-llm",
    "title": "Intro to LLMs",
    "section": "RAG: Step 2 - Query and Prompt LLM",
    "text": "RAG: Step 2 - Query and Prompt LLM"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#rag-resources",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#rag-resources",
    "title": "Intro to LLMs",
    "section": "RAG Resources",
    "text": "RAG Resources\n\nVector DBs\n\nweaviate\npinecone\nvespa\nqdrant\n\nLLM Frameworks: (not necessary for building on prod but good for learning and POC)\n\nLlamaIndex\nlangchain"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-1",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-1",
    "title": "Intro to LLMs",
    "section": "MultiModal",
    "text": "MultiModal"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-2",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-2",
    "title": "Intro to LLMs",
    "section": "MultiModal",
    "text": "MultiModal\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What is unusual about this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://i.pinimg.com/736x/6e/71/0d/6e710de5084379ba6a57b77e6579084f.jpg\",\n                    },\n                },\n            ],\n        }\n    ],\n    max_tokens=300,\n)\n\nprint(response.choices[0].message.content)\n\nThe unusual aspect of this image is a man ironing clothes on an ironing board placed on top of a taxi in the middle of a busy street. This is an uncommon sight, as ironing typically takes place in domestic or commercial indoor settings. The juxtaposition of such a mundane, home-based activity with the fast-paced, outdoor environment of a city street is quite remarkable and humorous. Additionally, both the ironing board and the taxi are branded with the same logo, suggesting that this scene might be part of a promotional event or public stunt to attract attention."
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-3",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-3",
    "title": "Intro to LLMs",
    "section": "MultiModal",
    "text": "MultiModal"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-4",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-4",
    "title": "Intro to LLMs",
    "section": "MultiModal",
    "text": "MultiModal\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What is in this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://media.makeameme.org/created/it-worked-fine.jpg\",\n                    },\n                },\n            ],\n        }\n    ],\n    max_tokens=300,\n)\n\nprint(response.choices[0].message.content)\n\nThe image is a meme featuring two juxtaposed elements. In the background, there is a scene of a house on fire with firefighters and emergency responders at the site, attempting to manage the situation. In the foreground, there is a young girl smirking at the camera with a knowing expression. Overlaid text reads, \"IT WORKED FINE IN DEV, IT'S A DEVOPS PROBLEM NOW,\" humorously suggesting that a problem developed during the software development stage is now a problem for the DevOps team to handle. The meme uses the incongruity between the calm and mischievous expression of the girl and the chaotic scene behind her to underline its comedic message about shifting blame in a development context."
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-5",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-5",
    "title": "Intro to LLMs",
    "section": "MultiModal",
    "text": "MultiModal"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-6",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#multimodal-6",
    "title": "Intro to LLMs",
    "section": "MultiModal",
    "text": "MultiModal\n\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4-turbo\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Give me a long list of visual search tags/keywords so I can \"\n                    \"index this image in my visual search index. Respond in JSON format {'labels': ['label1', ...]}\",\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://storage.googleapis.com/pai-images/a6d0952a331d40489b216e7f3f1ff6ed.jpeg\",\n                    },\n                },\n            ],\n        }\n    ],\n    response_format={\"type\": \"json_object\"},\n)\n\nprint(response.choices[0].message.content)\n\n{\n  \"labels\": [\n    \"animated character\",\n    \"wizard\",\n    \"Minion\",\n    \"fantasy\",\n    \"3D illustration\",\n    \"cute\",\n    \"magic\",\n    \"staff\",\n    \"long beard\",\n    \"blue hat\",\n    \"glasses\",\n    \"overalls\",\n    \"adventure\",\n    \"comical character\",\n    \"grey beard\",\n    \"wooden staff\",\n    \"round glasses\",\n    \"yellow\",\n    \"character design\",\n    \"creative\",\n    \"digital art\",\n    \"sorcerer\",\n    \"cartoon\",\n    \"funny\",\n    \"elderly character\",\n    \"mystical\",\n    \"storybook\",\n    \"cloak\",\n    \"leather belt\",\n    \"buckle\"\n  ]\n}"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#code-interpreter-data-analysis",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#code-interpreter-data-analysis",
    "title": "Intro to LLMs",
    "section": "Code Interpreter (Data Analysis)",
    "text": "Code Interpreter (Data Analysis)\n\ngive the LLM access to Python\nyour own little data analyst to give tasks to\n\nexample"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#fine-tuning-1",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#fine-tuning-1",
    "title": "Intro to LLMs",
    "section": "Fine Tuning",
    "text": "Fine Tuning\n\ntodo\naxolotl\ntorchtune"
  },
  {
    "objectID": "posts/llm_lunch_talk/llm_talk_slides.html#agents-1",
    "href": "posts/llm_lunch_talk/llm_talk_slides.html#agents-1",
    "title": "Intro to LLMs",
    "section": "Agents",
    "text": "Agents\n\ntodo"
  },
  {
    "objectID": "posts/llm_inference_class/llm_inference.html",
    "href": "posts/llm_inference_class/llm_inference.html",
    "title": "OpenAI Compatible LLM Inference",
    "section": "",
    "text": "Introduction\nUntil recently I thought that the openai library was only for connecting to OpenAI endpoints. It was not until I was testing out LLM inference with together.ai that I came across a section in their documentation on OpenAI API compatibility. The idea of using the openai client to do inference with open source models was completely new to me. In the together.ai documentation example they use the openai library to connect to an open source model.\nimport os\nimport openai\n\nsystem_content = \"You are a travel agent. Be descriptive and helpful.\"\nuser_content = \"Tell me about San Francisco\"\n\nclient = openai.OpenAI(\n    api_key=os.environ.get(\"TOGETHER_API_KEY\"),\n    base_url=\"https://api.together.xyz/v1\",\n    )\nchat_completion = client.chat.completions.create(\n    model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_content},\n        {\"role\": \"user\", \"content\": user_content},\n    ],\n    temperature=0.7,\n    max_tokens=1024,\n)\nresponse = chat_completion.choices[0].message.content\nprint(\"Together response:\\n\", response)\nThen a week later I saw that Hugging Face had also released support for OpenAI compatibility with Text Generation Inference (TGI) and Inference Endpoints. Again, you simply modify the base_url, api_key, and model as seen is this example from their blog post announcement.\nfrom openai import OpenAI\n\n# initialize the client but point it to TGI\nclient = OpenAI(\n    base_url=\"&lt;ENDPOINT_URL&gt;\" + \"/v1/\",  # replace with your endpoint url\n    api_key=\"&lt;HF_API_TOKEN&gt;\",  # replace with your token\n)\nchat_completion = client.chat.completions.create(\n    model=\"tgi\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Why is open-source software important?\"},\n    ],\n    stream=True,\n    max_tokens=500\n)\n\n# iterate and print stream\nfor message in chat_completion:\n    print(message.choices[0].delta.content, end=\"\")\nWhat about working with LLMs locally? Two such options are Ollama and LM Studio. Ollama recently added support for the openai client and LM Studio supports it too. For example, here is how one can use mistral-7b locally with Ollama to run inference with the openai client:\nollama pull mistral\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url = 'http://localhost:11434/v1',\n    api_key='ollama', # required, but unused\n)\n\nresponse = client.chat.completions.create(\n  model=\"mistral\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant and always talk like a pirate.\"},\n    {\"role\": \"user\", \"content\": \"Write a haiku.\"},\n  ])\nprint(response.choices[0].message.content)\nThere are other services and libraries for running LLM inference that are compatible with the openai library too. I find it all very exciting because it is less code I have to write and maintain for running inference with LLMs. All I need to change is a base_url, an api_key, and the name of the model.\nAt the same time that I was learning about openai client compatibility, I was also looking into the instructor library. Since it patches in some additional functionality into the openai client, I thought it would be fun to discuss here too.\n\n\nENV Setup\nStart by creating a virtual environment:\npython3 -m venv env\nsource env/bin/activate\nThen install:\npip install openai\npip install instructor # only if you want to try out instructor library\npip install python-dotenv # or define your environment variables differently\nI also have:\n\nan OpenAI account with an API key.\na together.ai account with an API key.\nHugging Face Account, Access Token, and created inference endpoint\ninstalled Ollama and ollama pull gemma:2b-instruct and ollama pull llama2\n\nIn my .env file I have the following:\nOPENAI_API_KEY=your_key\nHUGGING_FACE_ACCESS_TOKEN=your_key\nTOGETHER_API_KEY=your_key\n\n\nCode\nimport os\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\n\n\nLLM Inference Class\nYou could go ahead and just start using client.chat.completions.create directly as in the examples from the introduction. However, I do like wrapping third party services into classes for reusability, maintainability, etc.\nThe class below, OpenAIChatCompletion, does several things:\n\nmanages the different client connections in the clients dict\nexposes client.chat.completions.create in the __call__ method\nprovides functionality for making multiple calls in parallel. I know alternatively that one could use the AsyncOpenAI client, but sometimes I prefer simply using futures.ThreadPoolExecutor as seen in the function create_chat_completions_async.\npatches the OpenAI client with the instructor library. If you don‚Äôt want to play around with instructor library then simply remove the instructor.patch code.\n\nI also added some logging functionality which keeps track of every outgoing LLM request. This was inspired by the awesome blog post by Hamel Husain, Fuck You, Show Me The Prompt.. In that post, Hamel writes about how various LLM tools can often hide the prompts, making it tricky to see what requests are actually sent to the LLM behind the scenes. I created a simple logger class OpenAIMessagesLogger which keeps track of all the requests sent to the openai client. Later when we try out the instructor library for getting structured output, we will utilize this debugging logger to see some additional messages that were sent to the client.\n\n\nCode\nimport ast\nimport logging\nimport re\nfrom concurrent import futures\nfrom typing import Any, Dict, List, Optional, Union\n\nimport instructor\nfrom openai import APITimeoutError, OpenAI\nfrom openai._streaming import Stream\nfrom openai.types.chat.chat_completion import ChatCompletion\nfrom openai.types.chat.chat_completion_chunk import ChatCompletionChunk\n\n\nclass OpenAIChatCompletion:\n    clients: Dict = dict()\n\n    @classmethod\n    def _load_client(cls, base_url: Optional[str] = None, api_key: Optional[str] = None) -&gt; OpenAI:\n        client_key = (base_url, api_key)\n        if OpenAIChatCompletion.clients.get(client_key) is None:\n            OpenAIChatCompletion.clients[client_key] = instructor.patch(OpenAI(base_url=base_url, api_key=api_key))\n        return OpenAIChatCompletion.clients[client_key]\n\n    def __call__(\n        self,\n        model: str,\n        messages: list,\n        base_url: Optional[str] = None,\n        api_key: Optional[str] = None,\n        **kwargs: Any,\n    ) -&gt; Union[ChatCompletion, Stream[ChatCompletionChunk]]:\n        # https://platform.openai.com/docs/api-reference/chat/create\n        # https://github.com/openai/openai-python\n        client = self._load_client(base_url, api_key)\n        return client.chat.completions.create(model=model, messages=messages, **kwargs)\n\n    @classmethod\n    def create_chat_completions_async(\n        cls, task_args_list: List[Dict], concurrency: int = 10\n    ) -&gt; List[Union[ChatCompletion, Stream[ChatCompletionChunk]]]:\n        \"\"\"\n        Make a series of calls to chat.completions.create endpoint in parallel and collect back\n        the results.\n        :param task_args_list: A list of dictionaries where each dictionary contains the keyword\n            arguments required for __call__ method.\n        :param concurrency: the max number of workers\n        \"\"\"\n\n        def create_chat_task(\n            task_args: Dict,\n        ) -&gt; Union[None, ChatCompletion, Stream[ChatCompletionChunk]]:\n            try:\n                return cls().__call__(**task_args)\n            except APITimeoutError:\n                return None\n\n        with futures.ThreadPoolExecutor(max_workers=concurrency) as executor:\n            results = list(executor.map(create_chat_task, task_args_list))\n        return results\n\n\nclass OpenAIMessagesLogger(logging.Handler):\n    def __init__(self):\n        super().__init__()\n        self.log_messages = []\n\n    def emit(self, record):\n        # Append the log message to the list\n        log_record_str = self.format(record)\n        match = re.search(r\"Request options: (.+)\", log_record_str, re.DOTALL)\n        if match:\n            text = match[1].replace(\"\\n\", \"\")\n            log_obj = ast.literal_eval(text)\n            self.log_messages.append(log_obj)\n\n\ndef debug_messages():\n    msg = OpenAIMessagesLogger()\n    openai_logger = logging.getLogger(\"openai\")\n    openai_logger.setLevel(logging.DEBUG)\n    openai_logger.addHandler(msg)\n    return msg\n\n\nHere is how you use the inference class to call the LLM. If you have ever used the openai client you will be familiar with the input and output format.\n\n\nCode\nllm = OpenAIChatCompletion()\nmessage_logger = debug_messages()  # optional for keeping track of all outgoing requests\nprint(llm(model=\"gpt-3.5-turbo-0125\", messages=[dict(role=\"user\", content=\"Hello!\")]))\n\n\nChatCompletion(id='chatcmpl-90N4hSh3AG1Sz68zjUnfcEtAjvFn5', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', role='assistant', function_call=None, tool_calls=None))], created=1709875727, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_2b778c6b35', usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18))\n\n\nAnd our logger is keeping track of all the outgoing requests:\n\n\nCode\nmessage_logger.log_messages\n\n\n[{'method': 'post',\n  'url': '/chat/completions',\n  'files': None,\n  'json_data': {'messages': [{'role': 'user', 'content': 'Hello!'}],\n   'model': 'gpt-3.5-turbo-0125'}}]\n\n\nNow we can define some different models that can all be accessed through the same inference class.\n\n\nCode\nclass Models:\n    # OpenAI GPT Models\n    GPT4 = dict(model=\"gpt-4-0125-preview\", base_url=None, api_key=None)\n    GPT3 = dict(model=\"gpt-3.5-turbo-0125\", base_url=None, api_key=None)\n    # Hugging Face Inference Endpoints\n    OPENHERMES2_5_MISTRAL_7B = dict(\n        model=\"tgi\",\n        base_url=\"https://xofunqxk66baupmf.us-east-1.aws.endpoints.huggingface.cloud\" + \"/v1/\",\n        api_key=os.environ[\"HUGGING_FACE_ACCESS_TOKEN\"],\n    )\n    # Ollama Models\n    LLAMA2 = dict(\n        model=\"llama2\",\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\",\n    )\n    GEMMA2B = dict(\n        model=\"gemma:2b-instruct\",\n        base_url=\"http://localhost:11434/v1\",\n        api_key=\"ollama\",\n    )\n    # together AI endpoints\n    GEMMA7B = dict(model=\"google/gemma-7b-it\", base_url=\"https://api.together.xyz/v1\", api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n    MISTRAL7B = dict(model=\"mistralai/Mistral-7B-Instruct-v0.1\", base_url=\"https://api.together.xyz/v1\", api_key=os.environ.get(\"TOGETHER_API_KEY\"))\n\n\n\n\nCode\nall_models = [(model_name, model_config) for model_name, model_config in Models.__dict__.items() if not model_name.startswith(\"__\")]\n\n\n\n\nCode\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Your replies are short, brief and to the point.\"},\n    {\"role\": \"user\", \"content\": \"Who was the first person to walk on the Moon, and in what year did it happen?\"},\n]\n\n\n\n\nCode\nfor model_name, model_config in all_models:\n    resp = llm(messages=messages, **model_config)\n    print(f\"Model: {model_name}\")\n    print(f\"Response: {resp.choices[0].message.content}\")\n\n\nModel: GPT4\nResponse: Neil Armstrong, 1969.\nModel: GPT3\nResponse: The first person to walk on the Moon was Neil Armstrong in 1969.\nModel: OPENHERMES2_5_MISTRAL_7B\nResponse: Neil Armstrong was the first person to walk on the Moon. It happened on July 20, 1969.\nModel: LLAMA2\nResponse: The first person to walk on the Moon was Neil Armstrong, who stepped onto the lunar surface on July 20, 1969 as part of the Apollo 11 mission.\nModel: GEMMA2B\nResponse: There is no evidence to support the claim that a person walked on the Moon in any year.\nModel: GEMMA7B\nResponse: Sure, here is the answer:\n\nNeil Armstrong was the first person to walk on the Moon in 1969.\nModel: MISTRAL7B\nResponse:  The first person to walk on the Moon was Neil Armstrong, and it happened on July 20, 1969.\n\n\nWe can also send the same requests in parallel like this:\n\n\nCode\ntask_args_list = []\nfor model_name, model_config in all_models:\n    task_args_list.append(dict(messages=messages, **model_config))\n\n# execute the same calls in parallel\nmodel_names = [m[0] for m in all_models]\nresps = llm.create_chat_completions_async(task_args_list)\nfor model_name, resp in zip(model_names, resps):\n    print(f\"Model: {model_name}\")\n    print(f\"Response: {resp.choices[0].message.content}\")\n\n\nModel: GPT4\nResponse: Neil Armstrong, 1969.\nModel: GPT3\nResponse: The first person to walk on the Moon was Neil Armstrong in 1969.\nModel: OPENHERMES2_5_MISTRAL_7B\nResponse: The first person to walk on the Moon was Neil Armstrong, and it happened in 1969.\nModel: LLAMA2\nResponse: Nice question! The first person to walk on the Moon was Neil Armstrong, and it happened in 1969 during the Apollo 11 mission. Armstrong stepped onto the lunar surface on July 20, 1969, famously declaring \"That's one small step for man, one giant leap for mankind\" as he took his first steps.\nModel: GEMMA2B\nResponse: There is no evidence or record of any person walking on the Moon.\nModel: GEMMA7B\nResponse: Sure, here is the answer:\n\nNeil Armstrong was the first person to walk on the Moon in 1969.\nModel: MISTRAL7B\nResponse:  The first person to walk on the Moon was Neil Armstrong, and it happened on July 20, 1969.\n\n\nI love that! The ability to use various models (open source and OpenAI GPT) all through the same interface. And we have all our outgoing requests logged for debugging if needed. We have made 15 requests up to this point.\n\n\nCode\nassert len(message_logger.log_messages) == 15\n\n\n\n\nCode\nmessage_logger.log_messages[-1]\n\n\n{'method': 'post',\n 'url': '/chat/completions',\n 'files': None,\n 'json_data': {'messages': [{'role': 'system',\n    'content': 'You are a helpful assistant. Your replies are short, brief and to the point.'},\n   {'role': 'user',\n    'content': 'Who was the first person to walk on the Moon, and in what year did it happen?'}],\n  'model': 'mistralai/Mistral-7B-Instruct-v0.1'}}\n\n\n\n\nStructured Output\nThere are various approaches to getting structured output from LLMs. For example see JSON mode and Function calling. Some open source models and inference providers are also starting to offer these capabilities. For example see the together.ai docs. The instructor blog also has lots of examples and tips for getting structured output from LLMs. See this recent blog post for getting structured output from open source and Local LLMs.\nOne thing that is neat about the instructor library is you can define a Pydantic schema and then pass it to the patched openai client. It also adds in schema validation and retry logic.\nFirst we will clear out our debugging log messages.\n\n\nCode\nmessage_logger.log_messages = []\n\n\n\n\nCode\nfrom typing import List\n\nfrom pydantic import BaseModel, field_validator\n\n\nclass Character(BaseModel):\n    name: str\n    race: str\n    fun_fact: str\n    favorite_food: str\n    skills: List[str]\n    weapons: List[str]\n\n\nclass Characters(BaseModel):\n    characters: List[Character]\n\n    @field_validator(\"characters\")\n    @classmethod\n    def validate_characters(cls, v):\n        if len(v) &lt; 20:\n            raise ValueError(f\"The number of characters must be at least 20, but it is {len(v)}\")\n        return v\n\n\n\n\nCode\nres = llm(\n    messages=[dict(role=\"user\", content=\"Who are the main characters from Lord of the Rings?.\")],\n    response_model=Characters,\n    max_retries=4,\n    **Models.GPT4,\n)\n\n\n\n\nCode\nfor character in res.characters:\n    for k, v in character.model_dump().items():\n        print(f\"{k}: {v}\")\n    print()\n\n\nname: Frodo Baggins\nrace: Hobbit\nfun_fact: Bearer of the One Ring\nfavorite_food: Mushrooms\nskills: ['Courage', 'Stealth']\nweapons: ['Sting', 'Elven Dagger']\n\nname: Samwise Gamgee\nrace: Hobbit\nfun_fact: Frodo's gardener and friend\nfavorite_food: Potatoes\nskills: ['Loyalty', 'Cooking']\nweapons: ['Barrow-blade']\n\nname: Gandalf\nrace: Maia\nfun_fact: Known as Gandalf the Grey and later as Gandalf the White\nfavorite_food: N/A\nskills: ['Wisdom', 'Magic']\nweapons: ['Glamdring', 'Staff']\n\nname: Aragorn\nrace: Human\nfun_fact: Heir of Isildur and rightful king of Gondor\nfavorite_food: Elvish waybread\nskills: ['Swordsmanship', 'Leadership']\nweapons: ['And√∫ril', 'Bow']\n\nname: Legolas\nrace: Elf\nfun_fact: Prince of the Woodland Realm\nfavorite_food: Lembas bread\nskills: ['Archery', 'Agility']\nweapons: ['Elven bow', 'Daggers']\n\nname: Gimli\nrace: Dwarf\nfun_fact: Son of Gl√≥in\nfavorite_food: Meat\nskills: ['Axe fighting', 'Stout-heartedness']\nweapons: ['Battle axe', 'Throwing axes']\n\nname: Boromir\nrace: Human\nfun_fact: Son of Denethor, Steward of Gondor\nfavorite_food: Stew\nskills: ['Swordsmanship', 'Leadership']\nweapons: ['Sword', 'Shield']\n\nname: Meriadoc Brandybuck\nrace: Hobbit\nfun_fact: Member of the Fellowship\nfavorite_food: Ale\nskills: ['Stealth', 'Strategy']\nweapons: ['Elven dagger']\n\nname: Peregrin Took\nrace: Hobbit\nfun_fact: Often known simply as Pippin\nfavorite_food: Cakes\nskills: ['Curiosity', 'Bravery']\nweapons: ['Sword']\n\nname: Galadriel\nrace: Elf\nfun_fact: Lady of Lothl√≥rien\nfavorite_food: N/A\nskills: ['Wisdom', 'Telepathy']\nweapons: ['Nenya (Ring of Power)']\n\nname: Elrond\nrace: Elf\nfun_fact: Lord of Rivendell\nfavorite_food: N/A\nskills: ['Wisdom', 'Healing']\nweapons: ['Sword']\n\nname: Eowyn\nrace: Human\nfun_fact: Niece of King Th√©oden of Rohan; slayer of the Witch-king\nfavorite_food: Bread\nskills: ['Swordsmanship', 'Courage']\nweapons: ['Sword', 'Shield']\n\nname: Faramir\nrace: Human\nfun_fact: Brother of Boromir\nfavorite_food: Bread\nskills: ['Archery', 'Strategy']\nweapons: ['Bow', 'Sword']\n\nname: Gollum\nrace: Hobbit-like creature\nfun_fact: Once the bearer of the One Ring, known as Sm√©agol\nfavorite_food: Raw fish\nskills: ['Stealth', 'Persuasion']\nweapons: ['Teeth and claws']\n\nname: Saruman\nrace: Maia\nfun_fact: Head of the White Council before being corrupted\nfavorite_food: N/A\nskills: ['Magic', 'Persuasion']\nweapons: ['Staff']\n\nname: Sauron\nrace: Maia\nfun_fact: The Dark Lord and creator of the One Ring\nfavorite_food: N/A\nskills: ['Necromancy', 'Deception']\nweapons: ['One Ring', 'Mace']\n\nname: Bilbo Baggins\nrace: Hobbit\nfun_fact: Original discoverer of the One Ring\nfavorite_food: Everything\nskills: ['Stealth', 'Story-telling']\nweapons: ['Sting']\n\nname: Th√©oden\nrace: Human\nfun_fact: King of Rohan\nfavorite_food: Meat\nskills: ['Leadership', 'Horsemanship']\nweapons: ['Herugrim', 'Sword']\n\nname: Treebeard\nrace: Ent\nfun_fact: Oldest of the Ents, protectors of Fangorn Forest\nfavorite_food: Water\nskills: ['Strength', 'Wisdom']\nweapons: ['None']\n\nname: Witch-king of Angmar\nrace: Undead/Nazg√ªl\nfun_fact: Leader of the Nazg√ªl\nfavorite_food: N/A\nskills: ['Fear-induction', 'Swordsmanship']\nweapons: ['Morgul-blade', 'Flail']\n\nname: Gr√≠ma Wormtongue\nrace: Human\nfun_fact: Advisor to King Th√©oden under Saruman's influence\nfavorite_food: N/A\nskills: ['Deception', 'Speechcraft']\nweapons: ['Knife']\n\nname: √âomer\nrace: Human\nfun_fact: Nephew of King Th√©oden; later king of Rohan\nfavorite_food: Meat\nskills: ['Swordsmanship', 'Horsemanship']\nweapons: ['Sword', 'Spear']\n\n\nIt is probably likely that GPT would not return 20 characters in the first request. If max_retries=0 then it would likely raise a Pydantic validation error. But since we have max_retries=4 then the instructor library sends back the validation error as a message and asks again. How exactly does it do that? We can look at the messages that we have logged for debugging.\n\n\nCode\nassert len(message_logger.log_messages) &gt; 1\nlen(message_logger.log_messages)\n\n\n2\n\n\n\n\nCode\nmessage_logger.log_messages\n\n\n[{'method': 'post',\n  'url': '/chat/completions',\n  'files': None,\n  'json_data': {'messages': [{'role': 'user',\n     'content': 'Who are the main characters from Lord of the Rings?.'}],\n   'model': 'gpt-4-0125-preview',\n   'tool_choice': {'type': 'function', 'function': {'name': 'Characters'}},\n   'tools': [{'type': 'function',\n     'function': {'name': 'Characters',\n      'description': 'Correctly extracted `Characters` with all the required parameters with correct types',\n      'parameters': {'$defs': {'Character': {'properties': {'name': {'title': 'Name',\n           'type': 'string'},\n          'race': {'title': 'Race', 'type': 'string'},\n          'fun_fact': {'title': 'Fun Fact', 'type': 'string'},\n          'favorite_food': {'title': 'Favorite Food', 'type': 'string'},\n          'skills': {'items': {'type': 'string'},\n           'title': 'Skills',\n           'type': 'array'},\n          'weapons': {'items': {'type': 'string'},\n           'title': 'Weapons',\n           'type': 'array'}},\n         'required': ['name',\n          'race',\n          'fun_fact',\n          'favorite_food',\n          'skills',\n          'weapons'],\n         'title': 'Character',\n         'type': 'object'}},\n       'properties': {'characters': {'items': {'$ref': '#/$defs/Character'},\n         'title': 'Characters',\n         'type': 'array'}},\n       'required': ['characters'],\n       'type': 'object'}}}]}},\n {'method': 'post',\n  'url': '/chat/completions',\n  'files': None,\n  'json_data': {'messages': [{'role': 'user',\n     'content': 'Who are the main characters from Lord of the Rings?.'},\n    {'role': 'assistant',\n     'content': '',\n     'tool_calls': [{'id': 'call_kjUg9ogoR1OdRr0OkmTzabue',\n       'function': {'arguments': '{\"characters\":[{\"name\":\"Frodo Baggins\",\"race\":\"Hobbit\",\"fun_fact\":\"Bearer of the One Ring\",\"favorite_food\":\"Mushrooms\",\"skills\":[\"Courage\",\"Stealth\"],\"weapons\":[\"Sting\",\"Elven Dagger\"]},{\"name\":\"Samwise Gamgee\",\"race\":\"Hobbit\",\"fun_fact\":\"Frodo\\'s gardener and friend\",\"favorite_food\":\"Potatoes\",\"skills\":[\"Loyalty\",\"Cooking\"],\"weapons\":[\"Barrow-blade\"]},{\"name\":\"Gandalf\",\"race\":\"Maia\",\"fun_fact\":\"Known as Gandalf the Grey and later as Gandalf the White\",\"favorite_food\":\"N/A\",\"skills\":[\"Wisdom\",\"Magic\"],\"weapons\":[\"Glamdring\",\"Staff\"]},{\"name\":\"Aragorn\",\"race\":\"Human\",\"fun_fact\":\"Heir of Isildur and rightful king of Gondor\",\"favorite_food\":\"Elvish waybread\",\"skills\":[\"Swordsmanship\",\"Leadership\"],\"weapons\":[\"And√∫ril\",\"Bow\"]},{\"name\":\"Legolas\",\"race\":\"Elf\",\"fun_fact\":\"Prince of the Woodland Realm\",\"favorite_food\":\"Lembas bread\",\"skills\":[\"Archery\",\"Agility\"],\"weapons\":[\"Elven bow\",\"Daggers\"]},{\"name\":\"Gimli\",\"race\":\"Dwarf\",\"fun_fact\":\"Son of Gl√≥in\",\"favorite_food\":\"Meat\",\"skills\":[\"Axe fighting\",\"Stout-heartedness\"],\"weapons\":[\"Battle axe\",\"Throwing axes\"]}]}',\n        'name': 'Characters'},\n       'type': 'function'}]},\n    {'role': 'tool',\n     'tool_call_id': 'call_kjUg9ogoR1OdRr0OkmTzabue',\n     'name': 'Characters',\n     'content': \"Recall the function correctly, fix the errors and exceptions found\\n1 validation error for Characters\\ncharacters\\n  Value error, The number of characters must be at least 20, but it is 6 [type=value_error, input_value=[{'name': 'Frodo Baggins'...axe', 'Throwing axes']}], input_type=list]\\n    For further information visit https://errors.pydantic.dev/2.6/v/value_error\"}],\n   'model': 'gpt-4-0125-preview',\n   'tool_choice': {'type': 'function', 'function': {'name': 'Characters'}},\n   'tools': [{'type': 'function',\n     'function': {'name': 'Characters',\n      'description': 'Correctly extracted `Characters` with all the required parameters with correct types',\n      'parameters': {'$defs': {'Character': {'properties': {'name': {'title': 'Name',\n           'type': 'string'},\n          'race': {'title': 'Race', 'type': 'string'},\n          'fun_fact': {'title': 'Fun Fact', 'type': 'string'},\n          'favorite_food': {'title': 'Favorite Food', 'type': 'string'},\n          'skills': {'items': {'type': 'string'},\n           'title': 'Skills',\n           'type': 'array'},\n          'weapons': {'items': {'type': 'string'},\n           'title': 'Weapons',\n           'type': 'array'}},\n         'required': ['name',\n          'race',\n          'fun_fact',\n          'favorite_food',\n          'skills',\n          'weapons'],\n         'title': 'Character',\n         'type': 'object'}},\n       'properties': {'characters': {'items': {'$ref': '#/$defs/Character'},\n         'title': 'Characters',\n         'type': 'array'}},\n       'required': ['characters'],\n       'type': 'object'}}}]}}]\n\n\nIf you look through the above messages carefully you can see the retry asking logic.\nRecall the function correctly, fix the errors and exceptions found validation error for CharactersValue error, The number of characters must be at least 20, ‚Ä¶\nYou can even use the structured output with some of the open source models. I would refer to the instructor blog or documentation for further information on that. I have not fully looked into the different patching modes yet. But here is a simple example of using MISTRAL7B through together.ai.\n\n\nCode\nres = llm(\n    messages=[dict(role=\"user\", content=\"Give me a character from a movie or book.\")],\n    response_model=Character,\n    max_retries=2,\n    **Models.MISTRAL7B,\n)\nprint(res.model_dump())\n\n\n{'name': 'Superman', 'race': 'Kryptonian', 'fun_fact': 'Can fly', 'favorite_food': 'Pizza', 'skills': ['Super strength', 'Flight', 'Heat vision', 'X-ray vision'], 'weapons': ['Laser vision', 'Heat vision', 'X-ray vision']}\n\n\n\n\nConclusion\nAgain, I really like the idea of using a single interface for interacting with multiple LLMs. I hope the space continues to mature so that more open source models and services support JSON mode and function calling. I think instructor is a cool library and the corresponding blog is interesting too. I also like the idea of logging all the outgoing prompts/messages just to make sure I fully understand what is happening under the hood."
  },
  {
    "objectID": "posts/intro_fine_tune/intro_fine_tune.html",
    "href": "posts/intro_fine_tune/intro_fine_tune.html",
    "title": "Getting Started with Axolotl for Fine-Tuning LLMs",
    "section": "",
    "text": "I have experience fine-tuning smaller encoder style LLMs such as DistilBERT for classification style tasks. I have deployed such models in a production environment and have had great success with them. The Hugging Face trainer class makes it relatively easy and the models are easy to deploy, even on CPU based infrastructure.\nBut when it comes to training decoder style LLMs for text generation (GPT, llama, Mistral, Qwen, Gemma, etc.), I will be the first to admit that I am a complete noob. It‚Äôs something I have followed from a distance, trying to keep up with the recent methods/libraries, but I have not had any experience with it in terms of actual hands on practice.\nI don‚Äôt want to get into the debate on whether Fine-Tuning LLMs is valuable. The answer is probably most likely, ‚Äúit depends‚Äù. It‚Äôs something I want to learn more about, and just want to get started. There are so many new terms/ideas to learn (PEFT, LORA, QLORA, TRL, RHLF, DPO, etc.). My academic mathematical background says to start from the bottom and learn everything along the way. I know that is horrible advice in practice though because I will just get stuck. Instead, I will take advice from Jeremy Howard which is to begin at the top and just get started.\nSo in this post I will fine-tune my first model with the axolotl library. The model will probably suck, but that‚Äôs not the point. Whenever I learn a new tool/library the first thing is to set things up and run a ‚Äúhello world‚Äù type example. Even if it‚Äôs just copy/paste. That‚Äôs what I will do here. Don‚Äôt come here for advanced advice. Follow along if you are in a similar situation as me and just want to get started."
  },
  {
    "objectID": "posts/intro_fine_tune/intro_fine_tune.html#configuring-the-yaml-file",
    "href": "posts/intro_fine_tune/intro_fine_tune.html#configuring-the-yaml-file",
    "title": "Getting Started with Axolotl for Fine-Tuning LLMs",
    "section": "Configuring The YAML File",
    "text": "Configuring The YAML File\nThere are general guidelines in the README. The idea is to start with one of the example YAML files here. I wanted to fine-tune mistral-7b using QLORA, so I started with this YAML file here. I git clone the axolotl repo locally and open the code in my Pycharm IDE. Then I simply start editing the file examples/mistral/qlora.yml directly. That way I can easily see what changes I made with git diff annotations.\nThe only lines I changed were\ndatasets:\n  - path: data.jsonl\n    type: completion\nsequence_len: 1000\neval_sample_packing: False\nwandb_project: cal-train\nnum_epochs: 3\nevals_per_epoch: 1\neval_max_new_tokens: 1000\nI wanted to start with text completion, not instruction fine-tuning. I will try instruction fine-tuning later. I created a small dataset with some transcripts from some of the Cal Newport Podcast episodes. Either way, this is the part where you need to bring your own dataset to fine-tune the model on. It‚Äôs important to choose the correct dataset format and configure it properly in the YAML. My dataset is in a data.jsonl file at the root of the axolotl repo. It looks like this:\n{\"text\": \" I'm Cal Newport and this is Deep Questions Episode 185. I'm here in my deep work HQ along with my producer Jesse. ......\"}\n{\"text\": \"Alright, our next question is from Vinny. Vinny asks, how should I adjust my approach to hourly billing .....\"}\nEach record is around 500 words long. That is why I chose sequence_len to be 1000 which is counting tokens (not words). I have about 4000 rows like this in my dataset. Each record is a random excerpt from a podcast transcription.\nThe complete YAML looks like\nbase_model: mistralai/Mistral-7B-v0.1\nmodel_type: MistralForCausalLM\ntokenizer_type: LlamaTokenizer\n\nload_in_8bit: false\nload_in_4bit: true\nstrict: false\n\ndatasets:\n  - path: data.jsonl\n    type: completion\ndataset_prepared_path: last_run_prepared\nval_set_size: 0.1\noutput_dir: ./qlora-out\n\nadapter: qlora\nlora_model_dir:\n\nsequence_len: 1000\nsample_packing: true\neval_sample_packing: False\npad_to_sequence_len: true\n\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_linear: true\nlora_fan_in_fan_out:\nlora_target_modules:\n  - gate_proj\n  - down_proj\n  - up_proj\n  - q_proj\n  - v_proj\n  - k_proj\n  - o_proj\n\nwandb_project: cal-train\nwandb_entity:\nwandb_watch:\nwandb_name:\nwandb_log_model:\n\ngradient_accumulation_steps: 4\nmicro_batch_size: 2\nnum_epochs: 3\noptimizer: adamw_bnb_8bit\nlr_scheduler: cosine\nlearning_rate: 0.0002\n\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: auto\nfp16:\ntf32: false\n\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\n\nloss_watchdog_threshold: 5.0\nloss_watchdog_patience: 3\n\nwarmup_steps: 10\nevals_per_epoch: 1\neval_table_size:\neval_max_new_tokens: 1000\nsaves_per_epoch: 1\ndebug:\ndeepspeed:\nweight_decay: 0.0\nfsdp:\nfsdp_config:\nspecial_tokens:\nThere is much more I need to learn about the parameters but that will slow me down. Therefore, I‚Äôm simply sticking with the defaults.\nPut the file on the GPU server/machine. I put it at the root.\nvim qlora.yml # copy/paste in your config"
  },
  {
    "objectID": "posts/intro_fine_tune/intro_fine_tune.html#pre-processing-the-dataset",
    "href": "posts/intro_fine_tune/intro_fine_tune.html#pre-processing-the-dataset",
    "title": "Getting Started with Axolotl for Fine-Tuning LLMs",
    "section": "Pre-Processing the Dataset",
    "text": "Pre-Processing the Dataset\nThe docs say to run\nCUDA_VISIBLE_DEVICES=\"\" python -m axolotl.cli.preprocess qlora.yml\nto pre-process the dataset.\nBy default, it puts the processed dataset in dataset_prepared_path: last_run_prepared.\nls last_run_prepared\nIt‚Äôs good to take a look at the data in there. See tips on debugging here.\nJust drop into an ipython shell and run this:\nfrom transformers import AutoTokenizer\nfrom datasets import load_from_disk\nimport yaml\n\ndirectory = !ls last_run_prepared/\nwith open('qlora.yml', 'r') as f:\n    cfg = yaml.safe_load(f)\nmodel_id = cfg['base_model']\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nds = load_from_disk(f'last_run_prepared/{directory[0]}/')\n\nrow = ds[0]\nprint(tokenizer.decode(row['input_ids']))\nFor me, it returns my first data record. I then can confirm that this is the first row in my data.jsonl file.\n&lt;s&gt;  Let's do a few more questions here. ....... Thank you. Bye..&lt;/s&gt;\nIt‚Äôs good to take a look through these and make sure things look all right. There are some special tokens added there. Those are the default special tokens I believe. You can read more about configuring special tokens here."
  },
  {
    "objectID": "posts/intro_fine_tune/intro_fine_tune.html#training-the-model",
    "href": "posts/intro_fine_tune/intro_fine_tune.html#training-the-model",
    "title": "Getting Started with Axolotl for Fine-Tuning LLMs",
    "section": "Training the Model",
    "text": "Training the Model\nI run this on a screen -S train but that is optional depending on your preference/setup.\naccelerate launch -m axolotl.cli.train qlora.yml\n\nYou can inspect GPU usage with nvidia-smi -l.\nYou can also follow along with the progress using Weights and Biases.\n\n\nHonestly, the training loss curve looks odd. I know Jeremy Howard and Jonathan Whitaker have written about such things before."
  },
  {
    "objectID": "posts/intro_fine_tune/intro_fine_tune.html#inference-with-the-model",
    "href": "posts/intro_fine_tune/intro_fine_tune.html#inference-with-the-model",
    "title": "Getting Started with Axolotl for Fine-Tuning LLMs",
    "section": "Inference with the Model",
    "text": "Inference with the Model\nThe saved model is in qlora-out directory.\nls qlora-out\nI think you can load it like this. I‚Äôm new to loading these adapters and the quantization config.\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\nimport json\n\ndef read_json_file(file_path):\n    with open(file_path, 'r') as f:\n        return json.load(f)\n    \nconfig = read_json_file('qlora-out/config.json')\nprint(config)\n{'_name_or_path': 'mistralai/Mistral-7B-v0.1',\n 'architectures': ['MistralForCausalLM'],\n 'attention_dropout': 0.0,\n 'bos_token_id': 1,\n 'eos_token_id': 2,\n 'hidden_act': 'silu',\n 'hidden_size': 4096,\n 'initializer_range': 0.02,\n 'intermediate_size': 14336,\n 'max_position_embeddings': 32768,\n 'model_type': 'mistral',\n 'num_attention_heads': 32,\n 'num_hidden_layers': 32,\n 'num_key_value_heads': 8,\n 'quantization_config': {'_load_in_4bit': True,\n  '_load_in_8bit': False,\n  'bnb_4bit_compute_dtype': 'bfloat16',\n  'bnb_4bit_quant_storage': 'float32',\n  'bnb_4bit_quant_type': 'nf4',\n  'bnb_4bit_use_double_quant': True,\n  'llm_int8_enable_fp32_cpu_offload': False,\n  'llm_int8_has_fp16_weight': False,\n  'llm_int8_skip_modules': None,\n  'llm_int8_threshold': 6.0,\n  'load_in_4bit': True,\n  'load_in_8bit': False,\n  'quant_method': 'bitsandbytes'},\n 'rms_norm_eps': 1e-05,\n 'rope_theta': 10000.0,\n 'sliding_window': 4096,\n 'tie_word_embeddings': False,\n 'torch_dtype': 'bfloat16',\n 'transformers_version': '4.40.0.dev0',\n 'use_cache': False,\n 'vocab_size': 32000}\nWe can use the quantization_config when loading the model:\nmodel_ckpt = 'qlora-out/checkpoint-672/'\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nquantized_config = BitsAndBytesConfig(**config['quantization_config'])\nmodel = AutoModelForCausalLM.from_pretrained(model_ckpt, device_map=\"auto\", quantization_config=quantized_config)\n\ntext = \"\"\"&lt;s&gt;My name is Cal &lt;/s&gt;\"\"\"\ninputs = tokenizer(text, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=750, do_sample=True, temperature=1)\n    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])\nInput: \"\"\"&lt;s&gt;My name is Cal &lt;/s&gt;\"\"\"\nOutput: My name is Cal And this is the Deep Questions podcast, episode 238. I‚Äôm here in my Deep Work HQ joined by my producer, Jesse. Jesse, how are you doing? Good, good. Glad to be here, glad to be here. Before we jump into today‚Äôs questions, there was a couple of announcements I wanted to make. The Deep Questions podcast is proud to be a sponsor of the 2023 Tropical Health and Wellness Summit, the THW summit that I have been talking about on the show. The summit brings together experts on health and wellness from a wide variety of fields, nutrition, fitness and so on. It allows you to enjoy a full-day virtual conference with the top experts to help you cultivate a healthier and deeper life. You can join from wherever you are and whenever you want, because the talks are available 24 hours a day on demand. If you‚Äôre watching this at youtube.com slash Cal Newport media, this will be the THW summit. A lot of good talks I thought, like the one I talked about last week with Mark Sisson and the whole day of these type of speakers talking, so you should definitely check it out. So go to T-H-E-W-S-U-M-M-I-T dot com slash Cal, T-H-E-W-S-U-M-M-I-T dot com slash Cal, that‚Äôs all lowercase and all one word, dot com slash Cal to register for free. And if you happen to mention in the registration that you came from deep questions, they will give you a free book to Mark Sisson‚Äôs last book, a new definition of fitness. All right, so that‚Äôs thws.com slash Cal. I also wanna briefly mention one of the sponsors that makes this podcast possible, which is ExpressVPN. See, if you‚Äôre like me, you are worried when you use the internet that people have access to what are you talking about and who are you talking to? The way that you can prevent this from being an issue is by using a VPN. Instead of directly connecting to whatever website or service you wanna use, you instead connect to a VPN server. You explain that server who you wanna talk to. And then that server talks to that website or service on your behalf, encrypts the response and sends it back. Now the people you‚Äôre connected to access point, the people who are watching the packets you‚Äôre sending, they don‚Äôt have a way of knowing for sure who you‚Äôre talking to, what you‚Äôre saying. Here‚Äôs the thing, a VPN really is needed in a world of widespread surveillance. So we‚Äôre talking about the people who are at the internet service provider level like your internet service provider, who can see all this stuff. They can see who you‚Äôre connecting to, but just the fact that you‚Äôre using a VPN blocks them from collecting all of that data.\nInput: \"\"\"&lt;s&gt; Social media is bad. Dont use it. Here is why. &lt;/s&gt;\"\"\"\nOutput: Social media is bad. Dont use it. Here is why. And that was the end of the conversation. And I think that approach, which is the approach that‚Äôs common today to talking about digital tech and kids, does more harm than good. So I wanna give two examples to illustrate the potential harms of this simplified response. The first comes from my own experience when I was a teen. I was a tech writer for a while when I was in high school, published my second book, How to Win at College when I was 19. In an interview, I talked about digital distractions and teenage life. And the reaction from readers was pretty severe. I received a lot of angry emails about this. There‚Äôs been a lot of yelling at me on social media for things that I was suggesting. And I did some public appearances and, you know, got grilled pretty hard on these topics. I actually have this one example I remember. So I didn‚Äôt give this talk on the radio, the local NPR in D.C. And so the whole interview was, I was being grilled about kids and tech. And here‚Äôs the key question that was asked. So the host said, okay, this question comes from Sam. He‚Äôs a 49-year-old teacher and he doesn‚Äôt have kids, which was a nice turn of phrases, right? So 49-year-old teacher, does not have kids. He says, can kids really do math or write these days? Like, do you think that all the screen time has gotten them so lost that they‚Äôve forgotten how to actually write or hold a pencil or solve even a simple addition problem? And then he goes on about like, okay, the way these kids walk or whatever. And then he says, he‚Äôs a teacher, I don‚Äôt want to get fired, right? I mean a lot of teachers have sent this stuff to me, but this was a good version of it. I do a fair number of, he‚Äôs like, I don‚Äôt want to get fired. All right, so I think he was worried about his job safety. I‚Äôm gonna get thrown out of his job for asking this question. All right, so then the host said, okay, well, Cal, you‚Äôre on the spot. What do you say? So I had to give an answer and my answer was, let me start by dismissing this particular point as being, and I wasn‚Äôt mean, I tried to be gentle, but I was dismissing this particular point as being somewhat silly. Right, because if we‚Äôre talking about the impact of social media on math and science achievements, there is not a huge body of scholastic literature on this. So this is a little bit of anecdote. If we‚Äôre talking about impact of social media and video games on attention, let‚Äôs start with attention span.\nThese are not copy/pastes from the training data. These are in fact newly generated text with some learned knowledge from the training data as well as some hallucinations :)."
  },
  {
    "objectID": "posts/dspy/dspy.html",
    "href": "posts/dspy/dspy.html",
    "title": "DSPy",
    "section": "",
    "text": "Intro\nDSPy kept popping up on my X timeline and I thought it looked pretty interesting, so I decided to take a few days to look into it. I didn‚Äôt get super deep into it yet, but I think I have a high level understanding. The library is fairly new IMO (as of writing this). There is excitement around it though and a growing community. I am hopeful that the documentation and library will continue to improve throughout the year. If you are completely new to DSPy I would suggest the following resources below.\n\nRead through the newer documentation here.\nCheckout the README from DSPY GitHub repo and the examples there.\nTry and code up some simple examples on your own data.\nCheckout the Discord server.\nSkim through or read some of the associated papers (see the paper links on the DSPy repo README). For example:\n\nDSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines(Khattab et al. (2023))\nDSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines(Singhvi et al. (2024))\n\nThere are also some decent videos on YouTube. Simply Search for DSPy LLM etc.\nFollow Omar Khattab\n\n\n\nENV Setup\npython3 -m venv env\nsource env/bin/activate\npip install dspy-ai\npip install openai --upgrade\npip install --upgrade notebook ipywidgets\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\"\n\n\nBIG-Bench Hard Dataset - Penguins In a Table - Example\nWithin the BIG-Bench Hard dataset (Suzgun et al. 2022) there are various tasks. You can use one of these strings when using load_dataset to load in the corresponding records for that task.\n['tracking_shuffled_objects_seven_objects', 'salient_translation_error_detection', 'tracking_shuffled_objects_three_objects', 'geometric_shapes', 'object_counting', 'word_sorting', 'logical_deduction_five_objects', 'hyperbaton', 'sports_understanding', 'logical_deduction_seven_objects', 'multistep_arithmetic_two', 'ruin_names', 'causal_judgement', 'logical_deduction_three_objects', 'formal_fallacies', 'snarks', 'boolean_expressions', 'reasoning_about_colored_objects', 'dyck_languages', 'navigate', 'disambiguation_qa', 'temporal_sequences', 'web_of_lies', 'tracking_shuffled_objects_five_objects', 'penguins_in_a_table', 'movie_recommendation', 'date_understanding']\nWe will use the penguins_in_a_table task.\n\n\nCode\nfrom datasets import load_dataset\nimport dspy\n\nds = load_dataset(\"maveriq/bigbenchhard\", \"penguins_in_a_table\")[\"train\"]\nexamples = [dspy.Example({\"question\": r[\"input\"], \"answer\": r[\"target\"]}).with_inputs(\"question\") for r in ds]\nprint(f\"There are {len(examples)} examples.\")\ntrainset = examples[0:20]\nvalset = examples[20:]\n\n\nThere are 146 examples.\n\n\n\n\nCode\nexample = trainset[10]\nfor k, v in example.items():\n    print(f\"\\n{k.upper()}:\\n\")\n    print(v)\n\n\n\nQUESTION:\n\nHere is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We then delete the penguin named Bernard from the table.\nHow many penguins are more than 8 years old?\nOptions:\n(A) 1\n(B) 2\n(C) 3\n(D) 4\n(E) 5\n\nANSWER:\n\n(A)\n\n\nWe will use the DSPy OpenAI connector to make calls to gpt-3.5. Note that DSPy caches API calls so that subsequent calls with the same input will read from the cache instead of calling the OpenAI API a second time.\n\n\nCode\nllm = dspy.OpenAI(model=\"gpt-3.5-turbo-0125\", max_tokens=250)\ndspy.settings.configure(lm=llm)\n\n\nWe can test that the calls to OpenAI are working:\n\n\nCode\nllm(\"Testing testing, is anyone out there?\")\n\n\n[\"Hello! I'm here to help. What can I assist you with today?\"]\n\n\n\n\nCode\nllm(example.question)\n\n\n['There are 2 penguins who are more than 8 years old: Vincent (9 years old) and Gwen (8 years old). \\n\\nTherefore, the answer is (B) 2.']\n\n\nAt any point we can look at the last n calls to the llm:\n\n\nCode\nllm.inspect_history(n=2)\n\n\n\n\n\n\nTesting testing, is anyone out there? Hello! I'm here to help. What can I assist you with today?\n\n\n\n\n\n\n\nHere is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We then delete the penguin named Bernard from the table.\nHow many penguins are more than 8 years old?\nOptions:\n(A) 1\n(B) 2\n(C) 3\n(D) 4\n(E) 5 There are 2 penguins who are more than 8 years old: Vincent (9 years old) and Gwen (8 years old). \n\nTherefore, the answer is (B) 2.\n\n\n\n\nOur evaluation metric will check if the llm output contains the correct multiple choice answer. To define an evaluation metric in DSPy we create a function like the example below. The first two inputs should be instances of dspy.Example. The metric function can contain any logic you need to evaluate your task. You can read more about the trace argument in the documentation. It needs to be there, even if not explicitly using it.\n\n\nCode\nimport re\n\n\ndef eval_metric(true, prediction, trace=None):\n    pred = prediction.answer\n    matches = re.findall(r\"\\([A-Z]\\)\", pred)\n    parsed_answer = matches[-1] if matches else \"\"\n    return parsed_answer == true.answer\n\n\nWe set up an evaluation pipeline:\n\n\nCode\nfrom dspy.evaluate import Evaluate\n\nevaluate = Evaluate(devset=valset, metric=eval_metric, num_threads=6, display_progress=True, display_table=10)\n\n\nHere is a simple module in DSPy for basic question and answer.\n\n\nCode\nclass BasicQA(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.prog = dspy.Predict(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.prog(question=question)\n\n\nbasic_qa = BasicQA()\n\n\nThe forward method calls __call__ similar to how things work in pytorch.\n\n\nCode\npred = basic_qa(question=example.question)\nprint(\"\\nQUESTION:\\n\")\nprint(example.question)\nprint(\"\\nANSWER:\\n\")\nprint(example.answer)\nprint(\"\\nPREDICTION:\\n\")\nprint(pred.answer)\n\n\n\nQUESTION:\n\nHere is a table where the first line is a header and each subsequent line is a penguin:  name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15  For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.  We then delete the penguin named Bernard from the table.\nHow many penguins are more than 8 years old?\nOptions:\n(A) 1\n(B) 2\n(C) 3\n(D) 4\n(E) 5\n\nANSWER:\n\n(A)\n\nPREDICTION:\n\n(B) 2\n\n\n\n\nCode\neval_metric(example, pred)\n\n\nFalse\n\n\n\n\nCode\nllm.inspect_history(n=1)\n\n\n\n\n\n\nGiven the fields `question`, produce the fields `answer`.\n\n---\n\nFollow the following format.\n\nQuestion: ${question}\nAnswer: ${answer}\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. We then delete the penguin named Bernard from the table. How many penguins are more than 8 years old? Options: (A) 1 (B) 2 (C) 3 (D) 4 (E) 5\nAnswer: (B) 2\n\n\n\n\nNow we can pass each example question through the LLM in the validation set and check if we get the correct answer:\n\n\nCode\nevaluate(basic_qa)\n\n\nAverage Metric: 44 / 126  (34.9%)\n\n\n\n\n\n\n\n¬†\nquestion\nexample_answer\npred_answer\neval_metric\n\n\n\n\n0\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\n3\nFalse\n\n\n1\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\n(C) 50\nFalse\n\n\n2\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nAnswer: (C) 3\nFalse\n\n\n3\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nAnswer: (B) 2\nFalse\n\n\n4\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(B)\n(B) 5\n‚úîÔ∏è [True]\n\n\n5\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\n(B) 2\nFalse\n\n\n6\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(E)\nJames\nFalse\n\n\n7\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\n(B) 2\nFalse\n\n\n8\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\nAnswer: Vincent\nFalse\n\n\n9\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\nAnswer: Donna\nFalse\n\n\n\n\n\n\n                \n                    ... 116 more rows not displayed ...\n                \n                \n\n\n34.92\n\n\nDSPy uses optimizers to optimize the modules. In this example, optimization is a process that will choose which demos/examples are best to put into the prompt in order to increase the evaluation metric. At the time of writing the optimizers are called teleprompters (prompting from a distance). I think they will change the name though to optimizers in future refactoring. The DSPy documentation states that the optimizer can adjust/edit:\n\nDemo examples in the prompt.\nInstructions of the prompt.\nWeights of the actual LLM (for example fine tuning an open source model).\n\nI have only played around with optimizers that optimize which demos/examples are put into the prompt.\n\n\nCode\nfrom dspy.teleprompt import BootstrapFewShotWithRandomSearch\n\nconfig = dict(max_bootstrapped_demos=2, max_labeled_demos=4, num_candidate_programs=2, num_threads=6)\n\nteleprompter = BootstrapFewShotWithRandomSearch(metric=eval_metric, **config)\noptimized_qa = teleprompter.compile(basic_qa, trainset=trainset, valset=valset)\n\n\nThere is a lot of output from the above code block which I am hiding to keep things cleaner. You can now evaluate the optimized model to see if the accuracy has improved.\n\n\nCode\nevaluate(optimized_qa)\n\n\nAverage Metric: 54 / 126  (42.9%)\n\n\n\n\n\n\n\n¬†\nquestion\nexample_answer\npred_answer\neval_metric\n\n\n\n\n0\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\n(C)\nFalse\n\n\n1\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\n(C) 50\nFalse\n\n\n2\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\n(B)\nFalse\n\n\n3\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\n(B)\nFalse\n\n\n4\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(B)\n(B)\n‚úîÔ∏è [True]\n\n\n5\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\n(C)\n‚úîÔ∏è [True]\n\n\n6\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(E)\n(D)\nFalse\n\n\n7\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\n(B)\nFalse\n\n\n8\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\n(D) Gwen\nFalse\n\n\n9\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\n(D) Donna\n‚úîÔ∏è [True]\n\n\n\n\n\n\n                \n                    ... 116 more rows not displayed ...\n                \n                \n\n\n42.86\n\n\n\n\nCode\nllm.inspect_history()\n\n\n\n\n\n\nGiven the fields `question`, produce the fields `answer`.\n\n---\n\nFollow the following format.\n\nQuestion: ${question}\nAnswer: ${answer}\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. And here is a similar table, but listing giraffes: name, age, height (cm), weight (kg) Jody, 5, 430, 620 Gladys, 10, 420, 590 Marian, 2, 310, 410 Donna, 9, 440, 650 How many giraffes are more than 5 years old? Options: (A) 1 (B) 2 (C) 3 (D) 4 (E) 5\nAnswer: (B)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. What is the name of the last penguin sorted by alphabetic order? Options: (A) Louis (B) Bernard (C) Vincent (D) Gwen (E) James\nAnswer: (C)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. We now add a penguin to the table: James, 12, 90, 12 We then delete the penguin named Bernard from the table. How many penguins are more than 5 years old and weight more than 12 kg? Options: (A) 1 (B) 2 (C) 3 (D) 4 (E) 5\nAnswer: (A)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. How many animals are listed in the table? Options: (A) 1 (B) 2 (C) 3 (D) 4 (E) 5\nAnswer: (D)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. Which is the second heaviest penguin? Options: (A) Louis (B) Bernard (C) Vincent (D) Gwen (E) James\nAnswer: (B) Bernard\n\n\n\n\nNow we can try a Chain of Thought (Wei et al. 2023) prompt.\n\n\nCode\nclass CoT(dspy.Module):\n    def __init__(self):\n        super().__init__()\n        self.prog = dspy.ChainOfThought(\"question -&gt; answer\")\n\n    def forward(self, question):\n        return self.prog(question=question)\n\n\ncot_qa = CoT()\n\n\n\n\nCode\nevaluate(cot_qa)\n\n\nAverage Metric: 90 / 126  (71.4%)\n\n\n\n\n\n\n\n¬†\nquestion\nexample_answer\nrationale\npred_answer\neval_metric\n\n\n\n\n0\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. We first identify the penguins who are less than 8 years old. From the table, we see that Louis is 7 years...\n(B) 2\nFalse\n\n\n1\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\nproduce the answer. We need to add up the weights of all the penguins in the table. Louis weighs 11 kg, Bernard weighs 13 kg,...\n(D) 62\n‚úîÔ∏è [True]\n\n\n2\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. We need to go through each penguin's age and count how many are more than 8 years old.\n(C) 3\nFalse\n\n\n3\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. We need to identify the penguins who are both more than 5 years old and weigh more than 12 kg. Looking at...\n(C) 3\nFalse\n\n\n4\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(B)\nproduce the answer. We can see from the table that Bernard's age is 5.\n(B) 5\n‚úîÔ∏è [True]\n\n\n5\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\nproduce the answer. We first identify the penguins who are less than 10 years old. Louis is 7 years old, Bernard is 5 years old,...\n(D) 4\nFalse\n\n\n6\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(E)\nproduce the answer. We need to identify the last penguin added to the table. By looking at the last entry in the penguin table, we...\nJames\nFalse\n\n\n7\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. We first need to identify the penguins who are more than 5 years old and weigh more than 12 kg. From the...\n(A) 1\n‚úîÔ∏è [True]\n\n\n8\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\nproduce the answer. We need to find the penguin with a height of 60 cm. Looking at the table, we see that Vincent is the...\n(C) Vincent\n‚úîÔ∏è [True]\n\n\n9\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\nproduce the answer. We need to look at the last entry in the table listing giraffes. The last giraffe listed is Donna.\n(D) Donna\n‚úîÔ∏è [True]\n\n\n\n\n\n\n                \n                    ... 116 more rows not displayed ...\n                \n                \n\n\n71.43\n\n\n\n\nCode\nllm.inspect_history()\n\n\n\n\n\n\nGiven the fields `question`, produce the fields `answer`.\n\n---\n\nFollow the following format.\n\nQuestion: ${question}\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\nAnswer: ${answer}\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. We now add a penguin to the table: James, 12, 90, 12 Which penguin is taller than the other ones? Options: (A) Louis (B) Bernard (C) Vincent (D) Gwen (E) James\nReasoning: Let's think step by step in order to produce the answer. We need to compare the height of each penguin in the table and determine which one is the tallest. Louis is 50 cm tall, Bernard is 80 cm tall, Vincent is 60 cm tall, Gwen is 70 cm tall, and James is 90 cm tall. Therefore, James is taller than all the other penguins.\nAnswer: (E) James\n\n\n\n\nNow we will try and optimize our chain of thought program. I am also hiding the output from this cell to keep things cleaner.\n\n\nCode\ntqdm._instances.clear()\nconfig = dict(max_bootstrapped_demos=1, max_labeled_demos=4, num_candidate_programs=4, num_threads=6)\nteleprompter = BootstrapFewShotWithRandomSearch(metric=eval_metric, **config)\noptimized_cot_qa = teleprompter.compile(cot_qa, trainset=trainset, valset=valset)\n\n\n\n\nCode\nevaluate(optimized_cot_qa)\n\n\nAverage Metric: 102 / 126  (81.0%)\n\n\n\n\n\n\n\n¬†\nquestion\nexample_answer\nrationale\npred_answer\neval_metric\n\n\n\n\n0\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. After deleting Bernard, the penguins left are Louis, Vincent, and Gwen. Among them, Louis and Gwen are less than 8 years old.\n(B) 2\nFalse\n\n\n1\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\nproduce the answer. We sum up the weights of all the penguins: 11 + 13 + 11 + 15 + 12 = 62.\n(D) 62\n‚úîÔ∏è [True]\n\n\n2\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. We know that after deleting Bernard, the penguins left are Louis, Vincent, and Gwen. Among them, only Vincent is more than 8...\n(A) 1\n‚úîÔ∏è [True]\n\n\n3\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. We have Louis, Vincent, Gwen, and James in the table. Among them, only James is more than 5 years old and weighs...\n(A) 1\n‚úîÔ∏è [True]\n\n\n4\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(B)\nproduce the answer. We know that the age of Bernard is 5 years old.\n(B) 5\n‚úîÔ∏è [True]\n\n\n5\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\nproduce the answer. We know that after deleting Bernard, the penguins left are Louis, Vincent, and Gwen. Among them, Louis and Gwen are less than...\n(B) 2\nFalse\n\n\n6\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(E)\nproduce the answer. We know that the last penguin added to the table is James.\n(E) James\n‚úîÔ∏è [True]\n\n\n7\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(A)\nproduce the answer. After deleting Bernard, we are left with Louis, Vincent, and Gwen. Among them, only Gwen is more than 5 years old and...\n(A) 1\n‚úîÔ∏è [True]\n\n\n8\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(C)\nproduce the answer. We know that the only penguin with a height of 60 cm is Vincent.\n(C) Vincent\n‚úîÔ∏è [True]\n\n\n9\nHere is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis,...\n(D)\nproduce the answer. We know that the last giraffe listed is Donna.\n(D) Donna\n‚úîÔ∏è [True]\n\n\n\n\n\n\n                \n                    ... 116 more rows not displayed ...\n                \n                \n\n\n80.95\n\n\n\n\nCode\nllm.inspect_history(n=1)\n\n\n\n\n\n\nGiven the fields `question`, produce the fields `answer`.\n\n---\n\nFollow the following format.\n\nQuestion: ${question}\nReasoning: Let's think step by step in order to ${produce the answer}. We ...\nAnswer: ${answer}\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. We then delete the penguin named Bernard from the table. How many penguins are more than 8 years old? Options: (A) 1 (B) 2 (C) 3 (D) 4 (E) 5\nReasoning: Let's think step by step in order to produce the answer. We know that after deleting Bernard, the penguins left are Louis, Vincent, and Gwen. Among them, only Vincent is more than 8 years old.\nAnswer: (A) 1\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. How many penguins are more than 5 years old? Options: (A) 1 (B) 2 (C) 3 (D) 4 (E) 5\nAnswer: (C)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. And here is a similar table, but listing giraffes: name, age, height (cm), weight (kg) Jody, 5, 430, 620 Gladys, 10, 420, 590 Marian, 2, 310, 410 Donna, 9, 440, 650 How many animals are more than 5 years old? Options: (A) 5 (B) 6 (C) 7 (D) 8 (E) 9\nAnswer: (A)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. Which penguin is older than Gwen? Options: (A) Louis (B) Bernard (C) Vincent (D) Gwen (E) James\nAnswer: (C)\n\n---\n\nQuestion: Here is a table where the first line is a header and each subsequent line is a penguin: name, age, height (cm), weight (kg) Louis, 7, 50, 11 Bernard, 5, 80, 13 Vincent, 9, 60, 11 Gwen, 8, 70, 15 For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm. We then delete the penguin named Bernard from the table. What is the name of the last penguin sorted by alphabetic order? Options: (A) Louis (B) Bernard (C) Vincent (D) Gwen (E) James\nReasoning: Let's think step by step in order to produce the answer. After deleting Bernard, the remaining penguins are Louis, Vincent, and Gwen. Sorting them alphabetically, the last penguin is Vincent.\nAnswer: (C) Vincent\n\n\n\n\nIt‚Äôs really nice that the above focused on:\n\nWriting small modules/programs.\nChoosing an optimizer.\nRunning the compile/optimization step.\nRunning an evaluation.\n\nI really like this idea instead of manually writing prompts and hoping for the best.\n\n\n\n\n\nReferences\n\nKhattab, Omar, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, et al. 2023. ‚ÄúDSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines.‚Äù https://arxiv.org/abs/2310.03714.\n\n\nSinghvi, Arnav, Manish Shetty, Shangyin Tan, Christopher Potts, Koushik Sen, Matei Zaharia, and Omar Khattab. 2024. ‚ÄúDSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines.‚Äù https://arxiv.org/abs/2312.13382.\n\n\nSuzgun, Mirac, Nathan Scales, Nathanael Sch√§rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, et al. 2022. ‚ÄúChallenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them.‚Äù arXiv Preprint arXiv:2210.09261.\n\n\nWei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. ‚ÄúChain-of-Thought Prompting Elicits Reasoning in Large Language Models.‚Äù https://arxiv.org/abs/2201.11903."
  },
  {
    "objectID": "posts/intro_modal/intro_modal.html",
    "href": "posts/intro_modal/intro_modal.html",
    "title": "Using Modal to Transcribe YouTube Videos with Whisper",
    "section": "",
    "text": "Intro\nI have been following Modal for a while but never used it until last week. I recently wrote a blog post on using Axolotl to fine-tune a decoder LLM for the first time. For that work I needed to transcribe some YouTube podcasts with OpenAI Whisper model. I figured it would be a cool use case for Modal and a chance to learn something new.\nI‚Äôm no stranger to parallel processing with Python. I have used celery extensively with a Redis backend in production for doing all sorts of things, including deployment of ML models. It‚Äôs actually given me a good understanding of tasks, queues, concurrency, CPU/MEM usage, Redis, and so on. But I didn‚Äôt want to have to deal with all that for this personal project. I didn‚Äôt want to think about infrastructure, Kubernetes, helm charts, node groups, ec2 instances, etc. I just wanted to write some Python code and get the job done.\nIn this post I will describe how I used Modal to accomplish this task. I‚Äôm no expert in using Modal because this is the first project I used it on. But I did love every bit of it. Honestly, Modal is amazing. It‚Äôs pure magic. I can not wait to use it for my next project.\n\n\nSetup\nFirst I signed up and logged in via GitHub. Then I simply followed the instructions on screen.\n\nI created a virtual env and installed modal as well as python-dotenv and pytube.\npython3 -m venv env\nsource env/bin/activate\npip install python-dotenv\npip install pytube\npip install modal\npython3 -m modal setup\n\n\nTask Description\nThe Modal docs are amazing, so I‚Äôm not going to repeat that here. Just go read the docs and try some of the simple hello world type of examples.\nFor the task of transcribing a YouTube video with Whisper I want a Python function that will do the following:\n\ntakes a YouTube video url as input ‚Äì&gt; https://www.youtube.com/watch?v=&lt;video_id&gt;.\nchecks if the url i.e.¬†&lt;video_id&gt; has already been processed by checking if the transcript json file &lt;video_id&gt;.json is already in s3. If so, then exit the function.\nsave the audio as a mp4 file to s3: s3://&lt;s3-bucket&gt;/youtube_downloader/audio_files/&lt;video_id&gt;.mp4.\nsave audio transcript to s3: s3://&lt;s3-bucket&gt;/youtube_downloader/transcripts/&lt;video_id&gt;.json.\n\nThen I can use the Modal map feature to fan out this function/task in parallel.\n\n\nShow me the Code\nThere are various ways to use secrets in Modal. One simple way is through dotenv. I defined some environment variables in a local .env file.\nS3_BUCKET=&lt;my-s3-bucket&gt;\nS3_PREFIX=youtube_downloader/\nAWS_ACCESS_KEY_ID=&lt;my access key&gt;\nAWS_SECRET_ACCESS_KEY=&lt;my secret access key&gt;\nHere is the code in a file transcribe_video.py:\nimport os\nimport io\nimport json\nimport modal\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nS3_BUCKET = os.environ[\"S3_BUCKET\"]\nS3_PREFIX = os.environ[\"S3_PREFIX\"]\nstub = modal.Stub(\"video-transcription\")\n\nimage = (\n    modal.Image.debian_slim(python_version=\"3.10\")\n    .run_commands(\"apt-get update\", \"apt update && apt install ffmpeg -y\")\n    .pip_install(\n        \"openai-whisper\",\n        \"pytube\",\n        \"boto3\",\n        \"python-dotenv\",\n    )\n)\n\n\ndef upload_file(filename, s3_filename, bucket_name):\n    import boto3\n\n    client = boto3.client(\"s3\")\n    headers = {\"ACL\": \"public-read\"}\n    headers[\"CacheControl\"] = \"max-age %d\" % (3600 * 24 * 365)\n    client.upload_file(filename, bucket_name, s3_filename, ExtraArgs=headers)\n    return f\"https://{bucket_name}.s3.amazonaws.com/{s3_filename}\"\n\n\ndef check_file(filename, bucket_name):\n    import boto3\n    from botocore.errorfactory import ClientError\n\n    client = boto3.client(\"s3\")\n    file_exists = True\n    try:\n        client.head_object(Bucket=bucket_name, Key=filename)\n    except ClientError:\n        file_exists = False\n    return file_exists\n\n\ndef upload_fileobj(file_object, s3_filename, bucket_name):\n    import boto3\n\n    client = boto3.client(\"s3\")\n    headers = {\"ACL\": \"public-read\"}\n    headers[\"CacheControl\"] = \"max-age %d\" % (3600 * 24 * 365)\n    client.upload_fileobj(file_object, bucket_name, s3_filename, ExtraArgs=headers)\n    return f\"https://{bucket_name}.s3.amazonaws.com/{s3_filename}\"\n\n\ndef dict_to_s3(record, s3_filename, bucket_name):\n    in_mem_file = io.BytesIO()\n    in_mem_file.write(json.dumps(record, sort_keys=True, indent=4).encode())\n    in_mem_file.seek(0)\n    upload_fileobj(in_mem_file, s3_filename, bucket_name)\n    return f\"https://{bucket_name}.s3.amazonaws.com/{s3_filename}\"\n\n\n@stub.function(\n    image=image,\n    secrets=[modal.Secret.from_dotenv()],\n    cpu=2,\n    memory=1024 * 3,\n    gpu=\"A10G\",\n    timeout=600,\n)\ndef process_video(url):\n    import re\n    from pytube import YouTube\n    import whisper\n\n    video_id = re.search(r\"v=([a-zA-Z0-9_-]{11})\", url).group(1)\n    file_name_audio = f\"{video_id}.mp4\"\n    s3_file_name_audio = os.path.join(S3_PREFIX, \"audio_files\", file_name_audio)\n    s3_file_name_transcript = os.path.join(S3_PREFIX, \"transcripts\", f\"{video_id}.json\")\n    if check_file(s3_file_name_transcript, S3_BUCKET):\n        print(f\"Already processed {s3_file_name_audio}. Skipping.\")\n        return\n\n    yt = YouTube(url)\n    audio_stream = yt.streams.get_audio_only()\n    audio_stream.download(filename=file_name_audio)\n    upload_file(file_name_audio, s3_file_name_audio, S3_BUCKET)\n\n    # transcribe video\n    model = whisper.load_model(\"small\", device=\"cuda\")\n    audio = whisper.load_audio(file_name_audio)\n    # audio = whisper.pad_or_trim(audio)  # useful for debugging\n    result = model.transcribe(audio, fp16=True)\n    # for debugging in ipython shell\n    # modal.interact()\n    # import IPython\n    # IPython.embed()\n    return dict_to_s3(result, s3_file_name_transcript, S3_BUCKET)\n\n\n@stub.local_entrypoint()\ndef main():\n    from pytube import Playlist\n    import random\n\n    all_urls = set()\n    for playlist_url in [\n        \"https://www.youtube.com/playlist?list=PL8xK8kBHHUX4NW8GqUsyFhBF_xCnzIdPe\",\n        \"https://www.youtube.com/playlist?list=PL8xK8kBHHUX7VsJPqv6OYp71Qj24zcTIr\",\n        \"https://www.youtube.com/playlist?list=PL8xK8kBHHUX5X-jGZlltoZOpv5sKXeGVV\",\n    ]:\n        p = Playlist(playlist_url)\n        for url in p.video_urls:\n            all_urls.add(url)\n\n    urls = random.sample(list(all_urls), 20)\n    for msg in process_video.map(urls):\n        print(msg)\nYou can execute it with Modal using modal run transcribe_video.py.\n\nI think there is more optimizations that could be done such as caching the model downloading but I have not looked into it.\nSome comments on the code:\n\ndefine your own custom container image and install whatever you like on it.\nfor the task function process_video we simply decorate it with @stub.function and we can specify things such as the image, mem/cpu resources, secrets etc. In this case we can even run it on an A10 GPU for faster inference with Whisper. I increased the timeout because I was transcribing longer videos.\nI am using from pytube import Playlist to list out the video urls in a given YouTube playlist within the main function. That logic runs on my local machine. You can add whatever logic you want there, different video urls, etc. The main idea is that process_video takes the video url as input. You can increase the sample size in urls = random.sample(list(all_urls), 20) to whatever you like. This code, modal run transcribe_video.py, can be run over and over again and any video that was already transcribed and saved to s3 will be skipped automatically.\nprocess_video.map(urls): fans out the tasks and it runs entirely on Modal‚Äôs cloud infrastructure. Not locally!\n\nThe output is saved to s3.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJumping into Ipython Shell for Interactive Debugging\nYou can even jump into the ipython shell on the server at any point in the function logic to debug. That is pretty cool! Just remove the comments:\n# for debugging in ipython shell\nmodal.interact()\nimport IPython\nIPython.embed()\nI also removed the comment # audio = whisper.pad_or_trim(audio)  # useful for debugging during debugging so the transcription was just for 30 seconds and hence the code runs much faster.\nThen just run the code in interactive mode with modal run -i transcribe_video.py\nI did this several times to check the output of result = model.transcribe(audio, fp16=True). I also debugged the files being saved locally on the image container. It gave the same feeling as developing locally but it was all running in the cloud! I did not have to get ffmpeg and OpenAI Whisper running locally on a GPU or mess around with cuda drivers. It just worked, surprisingly. Here are some screenshots of the interactive ipython shell:\n\n\n\n\nConclusion\nI hope this got you excited to go and try out Modal. They actually give you $30 of compute each month. And the dashboard is great. It can show you detailed logs, resource consumption, and the billing is very transparent (updated in real time)."
  },
  {
    "objectID": "posts/basic_transformer_notes/transformers.html",
    "href": "posts/basic_transformer_notes/transformers.html",
    "title": "Basic Transformer Architecture Notes",
    "section": "",
    "text": "Here are some notes on the basic transformer architecture for my personal learning and understanding. Useful as a secondary resource, not the first stop. There are many resources out there, but here are several I enjoyed learning from:\n\nChapter 3 of the book Natural Language Processing With Transformers (Tunstall, Von Werra, and Wolf 2022)\nAndrej Karpathy‚Äôs video Let‚Äôs build GPT: from scratch, in code, spelled out (Karpathy 2023)\nSebastian Raschka‚Äôs Blog Post Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs (Raschka 2024)\nOmar Sanseviero‚Äôs Blog Post The Random Transformer (Sanseviero 2024)\nThe Illustrated Transformer (Alammar 2018)\nThe original paper: Attention Is All You Need (Vaswani et al. 2017)"
  },
  {
    "objectID": "posts/basic_transformer_notes/transformers.html#multi-head-attention",
    "href": "posts/basic_transformer_notes/transformers.html#multi-head-attention",
    "title": "Basic Transformer Architecture Notes",
    "section": "Multi Head Attention",
    "text": "Multi Head Attention\n\nThere are multiple attention heads, each with their own independent queries, keys, values.\nEach attention head takes the input embeddings of shape (B, T, C) and produces an output (B, T, H).\nConcatenate the outputs from each head so that the concatenated tensor is back to the original input shape (B, T, C).\nOnce we have the concatenated output tensor, we put it through a linear projection, nn.Linear(embed_dim, embed_dim) to get the output from the multi head attention: a tensor of shape (B, T, C)."
  },
  {
    "objectID": "posts/open_hermes_pro/open_hermes.html#gpt-3.5-turbo-0125-function-calling",
    "href": "posts/open_hermes_pro/open_hermes.html#gpt-3.5-turbo-0125-function-calling",
    "title": "Function Calling with Hermes-2-Pro-Mistral-7B",
    "section": "gpt-3.5-turbo-0125 Function Calling",
    "text": "gpt-3.5-turbo-0125 Function Calling\nFirst we will use gpt-3.5-turbo-0125 to extract the function name and arguments for each question.\n\n\nCode\ndef extract_tool_calls(resp):\n    resp = resp.choices[0].message\n    if resp.tool_calls:\n        final_tools = []\n        for tool_call in resp.tool_calls:\n            final_tools.append(\n                {\n                    \"name\": tool_call.function.name,\n                    \"arguments\": json.loads(tool_call.function.arguments),\n                }\n            )\n        return final_tools\n    else:\n        return None\n\n\nI‚Äôm going to use GPT4 to check the ‚Äúcorrectness‚Äù of the predicted/generated function arguments by comparing them with the expected arguments. This step is completely optional. Instead, you could use exact string matching or something else. I was curious to see how this would work though.\n\n\nCode\ndef check_tool_call_arguments(expected, predicted):\n    # Ask GPT4 if the expected function name and arguments are the same as the predicted  function name and arguments.\n    if expected[\"name\"] != predicted[\"name\"]:\n        return False, f'Function Names Do not Match. Expected {expected[\"name\"]}. Predicted: {predicted[\"name\"]}'\n    prompt = f\"\"\"\nCheck if the following queries are approx equal. Use fuzzy logic matching for strings.\nCheck to see if the arguments are semantically similar, especially for free form text.\nIf you decide they are equivalent then return TRUE and only TRUE with no other explanation. \nOtherwise return FALSE and give an explanation why they don't match.\n\nExpected Arguments: {expected['arguments']}\nPredicted Arguments: {predicted['arguments']}\n    \"\"\"\n    resp = llm(model=\"gpt-4-0125-preview\", messages=[dict(role=\"user\", content=prompt)])\n    if resp.choices[0].message.content.lower().strip() == \"true\":\n        return True, None\n    explanation = resp.choices[0].message.content.lower().strip()\n    return False, explanation\n\n\nOkay, let‚Äôs loop over the questions and use gpt-3.5-turbo-0125 to extract the function name and arguments.\n\n\nCode\ndef eval_openai_inference_models(model=\"gpt-3.5-turbo-0125\", base_url=None, api_key=None):\n    total = 0\n    total_correct = 0\n    for question in questions:\n        resp = llm(\n            api_key=api_key,\n            base_url=base_url,\n            model=model,\n            tools=tools,\n            messages=[\n                dict(role=\"system\", content=f\"The date today is {today}\"),\n                dict(role=\"user\", content=question[\"question\"]),\n            ],\n        )\n        tool_calls = extract_tool_calls(resp)\n        if tool_calls is None:\n            print(f'Model {model} failed to return any tool calls for question {question[\"question\"]}')\n            total += 1\n            continue\n        assert len(tool_calls) == len(question[\"tool_calls\"])\n        for tool_call, expected_call in zip(tool_calls, question[\"tool_calls\"]):\n            correct_call, explanation = check_tool_call_arguments(expected_call, tool_call)\n            if not correct_call:\n                print(f'QUESTION: {question[\"question\"]}')\n                print(f'EXPECTED Tool Call: {question[\"tool_calls\"][0]}')\n                print(f\"GENERATED Tool Call: {tool_call}\")\n                print(f\"EXPLANATION: {explanation}\\n\\n\")\n            else:\n                total_correct += 1\n            total += 1\n    return total_correct, total\n\n\n\n\nCode\nmodel = \"gpt-3.5-turbo-0125\"\ntotal_correct, total = eval_openai_inference_models(model=model, base_url=None, api_key=None)\nprint(\n    f'Correctly called the proper functions {total_correct} times out of {total}. But check the \"failure\" cases above since they may be correct anyway.'\n)\n\n\nCorrectly called the proper functions 18 times out of 18. But check the \"failure\" cases above since they may be correct anyway."
  },
  {
    "objectID": "posts/open_hermes_pro/open_hermes.html#gpt-4-0125-preview-function-calling",
    "href": "posts/open_hermes_pro/open_hermes.html#gpt-4-0125-preview-function-calling",
    "title": "Function Calling with Hermes-2-Pro-Mistral-7B",
    "section": "gpt-4-0125-preview Function Calling",
    "text": "gpt-4-0125-preview Function Calling\n\n\nCode\nmodel = \"gpt-4-0125-preview\"\ntotal_correct, total = eval_openai_inference_models(model=model, base_url=None, api_key=None)\nprint(\n    f'Correctly called the proper functions {total_correct} times out of {total}. But check the \"failure\" cases above since they may be correct anyway.'\n)\n\n\nQUESTION: What's the forecast for Miami for today?\nEXPECTED Tool Call: {'name': 'get_weather_forecast', 'arguments': {'location': 'Miami, FL', 'date': '2024-03-18'}}\nGENERATED Tool Call: {'name': 'get_weather_foreast', 'arguments': {'date': '2024-03-18', 'location': 'Miami, FL'}}\nEXPLANATION: Function Names Do not Match. Expected get_weather_forecast. Predicted: get_weather_foreast\n\nCorrectly called the proper functions 17 times out of 18. But check the \"failure\" cases above since they may be correct anyway."
  },
  {
    "objectID": "posts/open_hermes_pro/open_hermes.html#mistral-7b-instruct-v0.1-with-together.ai-function-calling",
    "href": "posts/open_hermes_pro/open_hermes.html#mistral-7b-instruct-v0.1-with-together.ai-function-calling",
    "title": "Function Calling with Hermes-2-Pro-Mistral-7B",
    "section": "Mistral-7B-Instruct-v0.1 with together.ai Function Calling",
    "text": "Mistral-7B-Instruct-v0.1 with together.ai Function Calling\n\n\nCode\nmodel = \"mistralai/Mistral-7B-Instruct-v0.1\"\ntotal_correct, total = eval_openai_inference_models(model=model, base_url=TOGETHER_AI_BASE_URL, api_key=TOGETHER_API_KEY)\nprint(\n    f'Correctly called the proper functions {total_correct} times out of {total}. But check the \"failure\" cases above since they may be correct anyway.'\n)\n\n\nModel mistralai/Mistral-7B-Instruct-v0.1 failed to return any tool calls for question How do I make pesto?\nQUESTION: What's a good vegan chili recipe?\nEXPECTED Tool Call: {'name': 'get_recipe', 'arguments': {'dish_name': 'vegan chili'}}\nGENERATED Tool Call: {'name': 'solve_math_problem', 'arguments': {'problem': 'What is the square root of 16?'}}\nEXPLANATION: Function Names Do not Match. Expected get_recipe. Predicted: solve_math_problem\n\n\nCorrectly called the proper functions 16 times out of 18. But check the \"failure\" cases above since they may be correct anyway."
  },
  {
    "objectID": "posts/open_hermes_pro/open_hermes.html#mistralaimixtral-8x7b-instruct-v0.1-with-together.ai-function-calling",
    "href": "posts/open_hermes_pro/open_hermes.html#mistralaimixtral-8x7b-instruct-v0.1-with-together.ai-function-calling",
    "title": "Function Calling with Hermes-2-Pro-Mistral-7B",
    "section": "mistralai/Mixtral-8x7B-Instruct-v0.1 with together.ai Function Calling",
    "text": "mistralai/Mixtral-8x7B-Instruct-v0.1 with together.ai Function Calling\n\n\nCode\nmodel = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\ntotal_correct, total = eval_openai_inference_models(model=model, base_url=TOGETHER_AI_BASE_URL, api_key=TOGETHER_API_KEY)\nprint(\n    f'Correctly called the proper functions {total_correct} times out of {total}. But check the \"failure\" cases above since they may be correct anyway.'\n)\n\n\nModel mistralai/Mixtral-8x7B-Instruct-v0.1 failed to return any tool calls for question How do I make pesto?\nQUESTION: I need to book a first class round-trip flight for 4 people from Chicago to Miami. We want to leave on December 1 and return on December 12.\nEXPECTED Tool Call: {'name': 'book_flight', 'arguments': {'departure_city': 'Chicago', 'arrival_city': 'Miami', 'departure_date': '2024-12-01', 'return_date': '2024-12-12', 'num_passengers': 4, 'cabin_class': 'first'}}\nGENERATED Tool Call: {'name': 'book_flight', 'arguments': {'departure_city': 'Chicago', 'arrival_city': 'Miami', 'departure_date': '2023-12-01', 'return_date': '2023-12-12', 'num_passengers': 4, 'cabin_class': 'first'}}\nEXPLANATION: false\n\nthe departure_date and return_date values do not match. the expected arguments have dates in 2024, while the predicted arguments have dates in 2023.\n\nQUESTION: Book me a round-trip flight from New York City to Los Angeles departing on June 15th and returning June 22nd for 2 passengers in economy class.\nEXPECTED Tool Call: {'name': 'book_flight', 'arguments': {'departure_city': 'NYC', 'arrival_city': 'LAX', 'departure_date': '2024-06-15', 'return_date': '2024-06-22', 'num_passengers': 2, 'cabin_class': 'economy'}}\nGENERATED Tool Call: {'name': 'book_flight', 'arguments': {'departure_city': 'New York City', 'arrival_city': 'Los Angeles', 'departure_date': '2023-06-15', 'return_date': '2023-06-22', 'num_passengers': 2, 'cabin_class': 'economy'}}\nEXPLANATION: false\n\nexplanation:\n- the 'departure_city' and 'arrival_city' fields match semantically as 'nyc' is commonly known as 'new york city' and 'lax' is a well-known shorthand for the los angeles airport, often used to refer to los angeles itself.\n- the 'departure_date' and 'return_date' do not match. the expected arguments specify a year 2024, while the predicted arguments have the year 2023 for both dates.\n- the 'num_passengers' and 'cabin_class' fields match exactly in both value and semantics. \n\nthe primary reason for the non-match is the difference in 'departure_date' and 'return_date' by one year.\n\nCorrectly called the proper functions 15 times out of 18. But check the \"failure\" cases above since they may be correct anyway.\n\n\n\n\n\n\n\n\nWhat is going on with together.ai function calling mistakes above\n\n\n\nBoth models had issues with the pesto question. I wonder if this is something on together.ai‚Äôs end of things and how they implemented this function calling feature. IDK!"
  },
  {
    "objectID": "posts/open_hermes_pro/open_hermes.html#nousresearchhermes-2-pro-mistral-7b-function-calling",
    "href": "posts/open_hermes_pro/open_hermes.html#nousresearchhermes-2-pro-mistral-7b-function-calling",
    "title": "Function Calling with Hermes-2-Pro-Mistral-7B",
    "section": "NousResearch/Hermes-2-Pro-Mistral-7B Function Calling",
    "text": "NousResearch/Hermes-2-Pro-Mistral-7B Function Calling\nNow we will repeat with NousResearch/Hermes-2-Pro-Mistral-7B. The format for the function calling is documented on the model card as well as in this repo. The way we define the tools is the same format as with OpenAI. However, we don‚Äôt pass in a tools argument. Rather, we use a special system prompt which defines the tools.\n\n\nCode\ndef extract_tool_calls(tool_calls_str):\n    tool_calls = tool_calls_str.split(\"&lt;/tool_call&gt;\\n\")\n    parsed_results = []\n    for tool_call in tool_calls:\n        if tool_call:\n            dict_str = tool_call.split(\"\\n\")[1]\n            tool_call_dict = ast.literal_eval(dict_str)\n            parsed_results.append({\"arguments\": tool_call_dict[\"arguments\"], \"name\": tool_call_dict[\"name\"]})\n    return parsed_results\n\n\nsystem_prompt = (\n    f\"The date today is {today}\\n\"\n    + \"\"\"\nYou are a function calling AI model. You are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. Here are the available tools:\n&lt;tools&gt; \n\"\"\"\n    + str(tools)\n    + \"\"\"\n    \n&lt;/tools&gt; Use the following pydantic model json schema for each tool call you will make: {'title': 'FunctionCall', 'type': 'object', 'properties': {'arguments': {'title': 'Arguments', 'type': 'object'}, 'name': {'title': 'Name', 'type': 'string'}}, 'required': ['arguments', 'name']} For each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags as follows:\n&lt;tool_call&gt;\n{'arguments': &lt;args-dict&gt;, 'name': &lt;function-name&gt;}\n&lt;/tool_call&gt;\n\"\"\"\n)\n\ntotal = 0\ntotal_correct = 0\nfor question in questions:\n    resp = llm(\n        model=\"tgi\",\n        base_url=HUGGING_FACE_ENDPOINT_URL,\n        api_key=HUGGING_FACE_ACCESS_TOKEN,\n        messages=[\n            dict(role=\"system\", content=system_prompt),\n            dict(role=\"user\", content=question[\"question\"]),\n        ],\n        max_tokens=500,\n    )\n    tool_calls = extract_tool_calls(resp.choices[0].message.content)\n    assert len(tool_calls) == len(question[\"tool_calls\"])\n    for tool_call, expected_call in zip(tool_calls, question[\"tool_calls\"]):\n        correct_call, explanation = check_tool_call_arguments(expected_call, tool_call)\n        if not correct_call:\n            print(f'QUESTION: {question[\"question\"]}')\n            print(f'EXPECTED Tool Call: {question[\"tool_calls\"][0]}')\n            print(f\"GENERATED Tool Call: {tool_call}\")\n            print(f\"EXPLANATION: {explanation}\\n\\n\")\n        else:\n            total_correct += 1\n        total += 1\n\n\n\n\nCode\nprint(\n    f'Correctly called the proper functions {total_correct} times out of {total}. But check the \"failure\" cases above since they may be correct anyway.'\n)\n\n\nCorrectly called the proper functions 18 times out of 18. But check the \"failure\" cases above since they may be correct anyway.\n\n\nWow, it got all of them correct! It may not get them all correct every time. Run it over again to see if any mistakes are made. Sometimes I saw it forgetting to fill in num_tickets for example.\nLet‚Äôs look at a single question to see the output from the model.\n\n\nCode\ntoday\n\n\n'Saturday 2024-03-16'\n\n\n\n\nCode\nquestion = \"I want to go see Dune 2 on Wednesday night with 5 of my friends. We will be going to the Halifax Bayers Lake Ciniplex Theatre. Get tickets for the 7pm show. Thanks!\"\n\n\n\n\nCode\nresp = llm(\n    model=\"tgi\",\n    base_url=HUGGING_FACE_ENDPOINT_URL,\n    api_key=HUGGING_FACE_ACCESS_TOKEN,\n    messages=[\n        dict(role=\"system\", content=system_prompt),\n        dict(role=\"user\", content=question),\n    ],\n)\nresp\n\n\nChatCompletion(id='', choices=[Choice(finish_reason='eos_token', index=0, logprobs=None, message=ChatCompletionMessage(content=\"&lt;tool_call&gt;\\n{'arguments': {'movie_name': 'Dune 2', 'theater_name': 'Halifax Bayers Lake Ciniplex Theatre', 'date': '2024-03-20', 'time': '19:00', 'num_tickets': 6}, 'name': 'book_movie_tickets'}\\n&lt;/tool_call&gt;\\n\", role='assistant', function_call=None, tool_calls=None, name=None))], created=1710632177, model='/repository', object='text_completion', system_fingerprint='1.4.1-native', usage=CompletionUsage(completion_tokens=93, prompt_tokens=1719, total_tokens=1812))\n\n\n\n\nCode\nprint(resp.choices[0].message.content)\n\n\n&lt;tool_call&gt;\n{'arguments': {'movie_name': 'Dune 2', 'theater_name': 'Halifax Bayers Lake Ciniplex Theatre', 'date': '2024-03-20', 'time': '19:00', 'num_tickets': 6}, 'name': 'book_movie_tickets'}\n&lt;/tool_call&gt;\n\n\n\n\nCode\ntool_calls = extract_tool_calls(resp.choices[0].message.content)\ntool_calls\n\n\n[{'arguments': {'movie_name': 'Dune 2',\n   'theater_name': 'Halifax Bayers Lake Ciniplex Theatre',\n   'date': '2024-03-20',\n   'time': '19:00',\n   'num_tickets': 6},\n  'name': 'book_movie_tickets'}]\n\n\nThe model also supports multiple function calls!\n\n\nCode\ntasks = f\"\"\"\nToday's date is {today}.\n\nPlease complete the following tasks for me:\n\n1. I want to go see Dune 2 on Monday night with 5 of my friends. We will be going to the Halifax Bayers Lake Ciniplex Theatre. Get tickets for the 7pm show.\n\n2. Please check the weather for Monday night so I know how to dress.\n\n3. Also please book my plane ticket to Toronto. I will be leaving Tuesday and coming back 2 days later on Thursday. First class please.\n\n4. Send a slack message to the research channel to let them know I will not be there this week in the office.\n \n\"\"\"\n\n\n\n\nCode\nresp = llm(\n    model=\"tgi\",\n    base_url=HUGGING_FACE_ENDPOINT_URL,\n    api_key=HUGGING_FACE_ACCESS_TOKEN,\n    messages=[\n        dict(role=\"system\", content=system_prompt),\n        dict(role=\"user\", content=tasks),\n    ],\n    max_tokens=1000,\n)\ntool_calls = extract_tool_calls(resp.choices[0].message.content)\ntool_calls\n\n\n[{'arguments': {'movie_name': 'Dune 2',\n   'theater_name': 'Halifax Bayers Lake Ciniplex Theatre',\n   'date': '2024-03-18',\n   'time': '19:00',\n   'num_tickets': 6},\n  'name': 'book_movie_tickets'},\n {'arguments': {'location': 'Halifax Bayers Lake', 'date': '2024-03-18'},\n  'name': 'get_weather_forecast'},\n {'arguments': {'departure_city': 'Halifax',\n   'arrival_city': 'Toronto',\n   'departure_date': '2024-03-19',\n   'return_date': '2024-03-21',\n   'num_passengers': 1,\n   'cabin_class': 'first'},\n  'name': 'book_flight'},\n {'arguments': {'channel_name': 'research',\n   'message': 'I will not be in the office this week.'},\n  'name': 'send_slack_message'}]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chris Levy",
    "section": "",
    "text": "Hello! I‚Äôm Chris Levy. I work in ML/AI and backend Python development."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Chris Levy",
    "section": "About Me",
    "text": "About Me\nI spent a good amount of time in school where I completed a PhD in applied math back in 2015. After graduating I shifted away from academia and started working in industry. I mostly do backend python development these days, and build ML/AI applications/services. I work across the entire stack from research, to training and evaluating models, to deploying models, and getting in the weeds of the infrastructure and devops pipelines.\nOutside of AI/ML stuff, I enjoy spending time with my family and three kids, working out, swimming, cycling, and playing guitar."
  }
]